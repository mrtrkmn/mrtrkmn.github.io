[{"content":"In this post, I will create a simple command to run a workflow on Github. This will be done with help of AWS Gateway. AWS Gateway provides 1 million API calls per month for 12 months at the time of writing this post.\nSetup Github On Github side, there is not much to setup other than adding a line to a workflow. For demonstration purposes, I will go use one of the repository from merkez. Let\u0026rsquo;s choose insthat repository for this occurence. When we check its existing workflow file, it looks like as follows:\nname: Test installation script  on: push: paths: - \u0026#39;install-tools.sh\u0026#39; schedule: # 15:05 UTC \u0026gt; 17:05 CEST  - cron: \u0026#39;5 15 * * *\u0026#39; workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Test all functions  run: |sudo chmod +x ./install-tools.sh sudo bash install-tools.sh --random It is, may be the simplest workflow file that you can see. After workflow_dispatch, we need to add repository_dispatch. For more about repository_dispatch checkout here: https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#repository_dispatch\nLet\u0026rsquo;s add following lines after workflow_dispatch.\nrepository_dispatch: types: on-demand-run Final workflow file: https://github.com/merkez/insthat/blob/main/.github/workflows/test-script.yml\nGenerate PAT (Personal Access Token) with required permission; at least Repo option needs to be selected.\nMore information about how to generate PAT can be found here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\nThis is the only things to be done on Github side, nothing else.\nSetup AWS Gateway API Navigate to AWS Console then API Gateway and click build Rest API option from list of options you see. Afterwards select Rest and New API options as shown below.\nFill out API name and description (optional) and click Create.\nIn opened window, click \u0026ldquo;Create Resource\u0026rdquo; and fill out input fields according to your preference and create it.\nUnder resource, create a POST method from Actions button again. It will open a window as given below, here select HTTP as Integration Type and in order to call workflow on Github, we need to construct the link. The template for the Github link that we will use is https://api.github.com/repos/{owner}/{repository}/dispatches, when we insert our values it is: https://api.github.com/repos/merkez/insthat/dispatches\nWhen we applied save, we should be able to see following page:\nFrom there, navigate to Integration Request box to setup authentication keys and request body to Github.\nFor authentication, following headers are required to be set.\n   Key Value     Accept \u0026lsquo;application/vnd.github.v3+json\u0026rsquo;   Authorization \u0026lsquo;Bearer {PERSONAL_ACCESS_TOKEN}\u0026rsquo;        Adjust \u0026ldquo;Mapping Templates\u0026rdquo; section with Slack\u0026rsquo;s content type, which is application/x-www-form-urlencoded.\nThe body, {\u0026quot;event_type\u0026quot;:\u0026quot;on-demand-run\u0026quot;} should match what you had in repository_dispath types.\nLastly, deploy API on AWS Gateway. https://t76xrsn8z6.execute-api.us-east-1.amazonaws.com/execute-workflow\nSet stage input fields.\nAfter deployment, navigate to Stages, and check full \u0026ldquo;Invoke URL\u0026rdquo; as shown below:\nFor our case Invoke URL is: https://t76xrsn8z6.execute-api.us-east-1.amazonaws.com/execute-workflow/runner-api\nThis URL will be used when creating the slack command.\nWe can now setup Slack command and give a try through Slack.\nSetup Slack App In order to create slash commands, we need to create a slack application on the workspace that we have permissions. For this demonstration, I will use https://mrkzi.slack.com workspace.\nAfterwards, go to slash commands option in application page of Slack and click \u0026ldquo;Create New Command\u0026rdquo;, following information will be asked. Feel free to choose any command that you want to use. The most important section in this fields is Request URL, retrieve it from AWS Gateway (Invoke URL), as explained in setting up AWS Gateway step.\nOnce this is done, install the app you created to the workspace from basic information section of the app.\nWhen it is done, you can go to Slack desktop application or on web, open workspace, you should be able to see autobot-runner or whatever you call it under apps of the workspace.\nThen, type the command you generated, for this demonstration it is, /run-insthat, when it is typed following option will appear. Execute it and let Github execute the workflow :)\nOnce the command /run-insthat is executed on Slack, workflow will automatically run.\nNow, we can start the workflow through Slack whether through Phone, Tablet or PC, does not matter, as long as you have access to Slack app. Its flexibility boosts level of confident.\n[Automate the boring stuff ü§åüèª ]\n","permalink":"https://mrturkmen.com/posts/automate-ci-cd-with-slack-command/","summary":"In this post, I will create a simple command to run a workflow on Github. This will be done with help of AWS Gateway. AWS Gateway provides 1 million API calls per month for 12 months at the time of writing this post.\nSetup Github On Github side, there is not much to setup other than adding a line to a workflow. For demonstration purposes, I will go use one of the repository from merkez.","title":"automate: run github ci/cd through slack slash command"},{"content":"Intro git cherry-pick is a powerful subcommand of git extensive information can be found here: https://www.atlassian.com/git/tutorials/cherry-pick\nIn this mini blog post, I will demonstrate how I just used it to remove specific commits from git history and preserve rest of the information as it is. (e.g metadata, timestamps, author, committer information)\ncherry-pick is generally used to remove some of the commits from git history, or re-order commit history. In this example, I will go through simple use case of it with combination of git filter-branch.\nThe story I created a private repository months ago, and consistenly filled it up with solutions of some exercises for a course at the university. However, I pushed official slide documents which I should not, luckily the repository was private and no one has an access to it. In time, I kept pushing more stuff to the repo, then we came to end of the semester. I thought that, it is better to make it public without leaking official solutions. Therefore, I had to remove the commits which are included earlier to the git history. The screenshot indicates which commits needs to be removed. It is indeed easy process to complete if it is done correctly.\nSteps to complete   Need to create a new branch before the document is committed to main/master branch\n  on main/master:\ngit checkout -b new-main     All commits except highlighted ones needs to be cheery-picked.\n  cherry-pick commits, except highlighted ones :\ngit cherry-pick \u0026lt;commit-id\u0026gt;   cherry-pick command will pick the commits that you select and append them to head of new branch. However, the problem is that it will NOT preserve timestamp information. It means that even though you made some commits months ago or a week ago, it will be replaced with the time that you cheerry-picked the commits. This behaviour breaks git history of a repository unexpectedly. The ideal scenario would be to have same information ( -timestamp-) as before. Therefore, there is one more additional step.\n    Set original timestamp information on new branch for all commits.\n  set git committer date to author date:\ngit filter-branch --env-filter \u0026#39;export GIT_COMMITTER_DATE=\u0026#34;$GIT_AUTHOR_DATE\u0026#34;\u0026#39;     Finally, you can push new-main branch to an upstream.\nEnd result \u0026ndash;\u0026gt; https://github.com/mrtrkmn/cloud-computing/commits/main\n","permalink":"https://mrturkmen.com/posts/cherry-pick/","summary":"Intro git cherry-pick is a powerful subcommand of git extensive information can be found here: https://www.atlassian.com/git/tutorials/cherry-pick\nIn this mini blog post, I will demonstrate how I just used it to remove specific commits from git history and preserve rest of the information as it is. (e.g metadata, timestamps, author, committer information)\ncherry-pick is generally used to remove some of the commits from git history, or re-order commit history. In this example, I will go through simple use case of it with combination of git filter-branch.","title":"cherry-pick: re-build git history"},{"content":"Gitbook is simple, easy and free way of keeping notes under your own domain. However, it is NOT possible to set password protection when you do not want anyone access to your resources except you or your team for free. (More information: https://docs.gitbook.com/features/visitor-authentication) Nice news is that it is possible to set password protection even though you are on free tier with some tricks.\nFirst of all, for this example, your custom domain should be configured to Cloudflare, since I will use Cloudflare Workers to setup Basic Authentication in between you and Gitbook origin.\nI assumed that your custom domain is already configured with Cloudflare, if not, configure it, if you want to use Cloudflare Workers feature.\nCloudflare worker has some request limitation in free tier (Accounts using the Workers Free plan are subject to a daily request limit of 100,000 requests.). However, it is not a problem for us since we won\u0026rsquo;t run e-commerce website or something. It is just private notes which is accesible from anywhere when you have the credentials.\nHere is an example code block for Basic Authentication\n(Taken and modified from: https://developers.cloudflare.com/workers/examples/basic-auth/ )\n/** * Shows how to restrict access using the HTTP Basic schema. * @see https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication * @see https://tools.ietf.org/html/rfc7617 * * A user-id containing a colon (\u0026#34;:\u0026#34;) character is invalid, as the * first colon in a user-pass string separates user and password. */ const BASIC_USER = \u0026#39;randomusername\u0026#39;; const BASIC_PASS = \u0026#39;randompass\u0026#39;; addEventListener(\u0026#39;fetch\u0026#39;, event =\u0026gt; { event.respondWith( handleRequest(event.request).catch(err =\u0026gt; { const message = err.reason || err.stack || \u0026#39;Unknown Error\u0026#39;; return new Response(message, { status: err.status || 500, statusText: err.statusText || null, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;text/plain;charset=UTF-8\u0026#39;, // Disables caching by default.  \u0026#39;Cache-Control\u0026#39;: \u0026#39;no-store\u0026#39;, // Returns the \u0026#34;Content-Length\u0026#34; header for HTTP HEAD requests.  \u0026#39;Content-Length\u0026#39;: message.length, }, }); }) ); }); /** * Receives a HTTP request and replies with a response. * @param {Request} request * @returns {Promise\u0026lt;Response\u0026gt;} */ async function handleRequest(request) { const { protocol, pathname } = new URL(request.url); // In the case of a Basic authentication, the exchange  // MUST happen over an HTTPS (TLS) connection to be secure.  if (\u0026#39;https:\u0026#39; !== protocol || \u0026#39;https\u0026#39; !== request.headers.get(\u0026#39;x-forwarded-proto\u0026#39;)) { throw new BadRequestException(\u0026#39;Please use a HTTPS connection.\u0026#39;); } switch (pathname) { // case \u0026#39;/\u0026#39;:  // return new Response(\u0026#39;Anyone can access the homepage.\u0026#39;);  // case \u0026#39;/logout\u0026#39;:  // // Invalidate the \u0026#34;Authorization\u0026#34; header by returning a HTTP 401.  // // We do not send a \u0026#34;WWW-Authenticate\u0026#34; header, as this would trigger  // // a popup in the browser, immediately asking for credentials again.  // return new Response(\u0026#39;Logged out.\u0026#39;, { status: 401 });  default: { // The \u0026#34;Authorization\u0026#34; header is sent when authenticated.  if (request.headers.has(\u0026#39;Authorization\u0026#39;)) { // Throws exception when authorization fails.  const { user, pass } = basicAuthentication(request); if (verifyCredentials(user, pass)) { return await fetch(request) } } // Not authenticated.  return new Response(\u0026#39;You need to login.\u0026#39;, { status: 401, headers: { // Prompts the user for credentials.  \u0026#39;WWW-Authenticate\u0026#39;: \u0026#39;Basic realm=\u0026#34;Private Area\u0026#34;, charset=\u0026#34;UTF-8\u0026#34;\u0026#39;, }, }); } case \u0026#39;/favicon.ico\u0026#39;: case \u0026#39;/robots.txt\u0026#39;: return new Response(null, { status: 204 }); } return new Response(\u0026#39;Not Found.\u0026#39;, { status: 404 }); } function verifyCredentials(user, pass) { if (BASIC_USER !== user || BASIC_PASS !== pass) { return false } return true } /** * Parse HTTP Basic Authorization value. * @param {Request} request * @throws {BadRequestException} * @returns {{ user: string, pass: string }} */ function basicAuthentication(request) { const Authorization = request.headers.get(\u0026#39;Authorization\u0026#39;); const [scheme, encoded] = Authorization.split(\u0026#39; \u0026#39;); // The Authorization header must start with Basic, followed by a space.  if (!encoded || scheme !== \u0026#39;Basic\u0026#39;) { throw new BadRequestException(\u0026#39;Malformed authorization header.\u0026#39;); } // Decodes the base64 value and performs unicode normalization.  // @see https://datatracker.ietf.org/doc/html/rfc7613#section-3.3.2 (and #section-4.2.2)  // @see https://dev.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String/normalize  const buffer = Uint8Array.from(atob(encoded), character =\u0026gt; character.charCodeAt(0)); const decoded = new TextDecoder().decode(buffer).normalize(); // The username \u0026amp; password are split by the first colon.  //=\u0026gt; example: \u0026#34;username:password\u0026#34;  const index = decoded.indexOf(\u0026#39;:\u0026#39;); // The user \u0026amp; password are split by the first colon and MUST NOT contain control characters.  // @see https://tools.ietf.org/html/rfc5234#appendix-B.1 (=\u0026gt; \u0026#34;CTL = %x00-1F / %x7F\u0026#34;)  if (index === -1 || /[\\0-\\x1F\\x7F]/.test(decoded)) { throw new BadRequestException(\u0026#39;Invalid authorization value.\u0026#39;); } return { user: decoded.substring(0, index), pass: decoded.substring(index + 1), }; } function UnauthorizedException(reason) { this.status = 401; this.statusText = \u0026#39;Unauthorized\u0026#39;; this.reason = reason; } function BadRequestException(reason) { this.status = 400; this.statusText = \u0026#39;Bad Request\u0026#39;; this.reason = reason; } This is the domain to be protected:\nhttps://worker.mrturkmen.com\nThis subdomain contains an example Gitbook, which is synced with Github repo.\nhttps://github.com/merkez/worker-gitbook\nYou can follow the steps given below from web dashboard of Cloudflare:\nCreate worker service Step 1: Create worker service\nCreate trigger Step 2: Create trigger, add route to custom domain created for Gitbook\nEdit Service Step 3: Edit Service, click QUICK EDIT button\nModify Worker Code Step 4: Paste Authentication code to editor in left side and deploy\nVisit website The website is only accesible with provided username and password in the example code above.\nCloudflare workers has a lot of capabilites not limited to this simple approach. More examples and information can be found in provided links below:\n https://workers.cloudflare.com https://developers.cloudflare.com/workers/examples/ https://developers.cloudflare.com/workers/examples/basic-auth/  ","permalink":"https://mrturkmen.com/posts/cloudflare-workers/","summary":"Gitbook is simple, easy and free way of keeping notes under your own domain. However, it is NOT possible to set password protection when you do not want anyone access to your resources except you or your team for free. (More information: https://docs.gitbook.com/features/visitor-authentication) Nice news is that it is possible to set password protection even though you are on free tier with some tricks.\nFirst of all, for this example, your custom domain should be configured to Cloudflare, since I will use Cloudflare Workers to setup Basic Authentication in between you and Gitbook origin.","title":"cloudflare workers: add auth to free Gitbook space"},{"content":"Would like to see it in action right away ? Go to demonstration video here:\n\n I am always amazed with Cloudflare\u0026rsquo;s products, blog posts and tools that they are offering. In my spare time, I am trying to read through blog posts over here Cloudflare Blog. Some of them are really easy to consume, some are not, you might need to have an idea about network related terms, or need to follow Cloudflare closely to catch on the blog posts.\nRecently, when I was surfing on Cloudflare website, I came crossed with a tool that I have seen for the first time, which is called cloudflared\nIt is a client of Argo Tunnel which is another amazing product that Cloudflare provide, including for free tiers. I will not describe or explain it in detail, since Cloudflare Blog and documentation here: Argo Tunnel explains it very well. Furthermore, over there, you can see and experience other cool products as well.\nInstead, in this blog post, I am going to use cloudflared tool, to serve a website which is locally running on my PC to the world thanks to Cloudflare Tunnel. Since its guidelines and instructions are crystal clear, some steps might be seen as repetitive of what Cloudflare explained. Neverthless, I would like to do it in an unofficial style anyway.\nI assume that you have hosted your custom domain at Cloudflare. Otherwise, do it as described here: Register a new domain. After installing the tool as described on official page of Cloudflared tool, you can authenticate it with the domain that you will work on. Once authentication is done, following steps can be followed.\nHere is some commands which can easily be grasped at first glance.\n# complete steps and download certificates to authenticate user and domain to use $ cloudflared tunnel login # create a tunnel with name mrtrkmn-tunnel $ cloudflared tunnel create mrtrkmn-tunnel # serve locally running web application on local.mrturkmen.com globally ## this step is not required when you already have  ## a config file as described below $ cloudflared tunnel --hostname local.mrturkmen.com --url http://localhost:8000 # create a route to the tunnel over local.mrturkmen.com # it creates a CNAME record on Cloudflare. $ cloudflared tunnel route dns mrtrkmn-tunnel local.mrturkmen.com # run the tunnel  $ cloudflared tunnel run Cloudflared tool has a configuration option under your home directory, as shown here:\nI setup localhost:8000 to be served through mrtrkmn-tunnel (ea6cf5bf-13b4-41c5-9000-14705384d83a).\nDifferent websites can be served through different tunnels by specifying newly created tunnel IDs and corresponding URLs. For demonstration purposes, I will only consider one example.\nTotal number of commands after authentication of cloudflared tool is five including running your local deployment.\nCreate tunnel  $ cloudflared create tunnel mrtrkmn-tunnel Tunnel credentials written to \u0026lt;your-home-dir\u0026gt;/.cloudflared/961486d1-c624-46b3-8eb5-f74ba8ab2a91.json. cloudflared chose this file based on where your origin certificate was found. Keep this file secret. To revoke these credentials, delete the tunnel. Create tunnel config  url: \u0026#34;http://localhost:8000\u0026#34; tunnel: mrtrkmn-tunnel credentials-file: \u0026lt;your-home-dir\u0026gt;/.cloudflared/ea6cf5bf-13b4-41c5-9000-14705384d83a.json. Route tunnel traffic to a subdomain  $ cloudflared tunnel route dns mrtrkmn-tunnel local.mrturkmen.com Run HTTP server or a website on given port in config file  Just for proof of concept, run a dummy HTTP server to serve globally.\n $ python3 -m http.server --bind 0.0.0.0 Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ... Run tunnel $ cloudflared tunnel run mrtrkmn-tunnel Now, you should be able to see, your local deployment or HTTP(s) server is available on given subdomain in routing step above.\n There are other cool stuff that Cloudflare is providing even though you have free tier account. Since I am quite interested on Cloudflare tech stack and its products, I will try to test out their products and create simple blog posts. It is now much easy to test out your local deployments with your teammates, friends and other communities.\n References  Cloudflare Tunnel, https://www.cloudflare.com/products/tunnel/ Cloudflare Zero Trust - Get Started, https://developers.cloudflare.com/cloudflare-one/setup/ Cloudflare Zero Trust - Add web applications, https://developers.cloudflare.com/cloudflare-one/applications/configure-apps/  ","permalink":"https://mrturkmen.com/posts/cloudflare-tunneling/","summary":"Would like to see it in action right away ? Go to demonstration video here:\n\n I am always amazed with Cloudflare\u0026rsquo;s products, blog posts and tools that they are offering. In my spare time, I am trying to read through blog posts over here Cloudflare Blog. Some of them are really easy to consume, some are not, you might need to have an idea about network related terms, or need to follow Cloudflare closely to catch on the blog posts.","title":"cloudflare tunneling: serve local like not local"},{"content":"  I am alive !!!\n  Right, I did not post anything for long time and could not imagine that this will be my next blog post after long time, but it is indeed.\n  This is a story about a company who hold the product (- sent for warranty -) for five months and did not respond any emails.\nLet\u0026rsquo;s start from the beginning of the story, the product itself is not important, instead how they (- company -) approached to situation is important.\nI have sent the product to Karaca (- krc.com.tr -) in November 2021 by hoping that it will be fixed and returned in two or three weeks at most. However, it turns out that it will not be the case, just realized after two or three weeks later :) At that time, I did not give any importance to the situation instead waited more, more and more.\nAt the end of four months, I decided to take an action, I have tried to reach them through their \u0026ldquo;customer services\u0026rdquo; phone number ( +90 850 2525 572 ). However it connects you to a stupid automated replies, do you remember those ? it replies like, e.g \u0026ldquo;if you would like to learn about your product, press 1 \u0026ldquo;, \u0026ldquo;if you would like to learn about warranty process, press 2\u0026rdquo; and so on. Although I tried all possibilities to reach them through phone, I could not be successfull enough to get them on the line.\nAfterwards, I realized that they are actually responding to questions/complains if you contact with them through their contact form. ( - https://www.krc.com.tr/contact-form - ) At the beginning, I sent some messages through this form, however they replied me with same message everytime.\nThe message was:\n You have an active record with the number \u0026ldquo;XXX-XXXXXXX-XXXXXX\u0026rdquo; when the necessary control is made based on your request. You will receive a response as soon as possible.\n It was really annoying to receive same replies without providing any information about why they did not sent back the product to me all these past four months. Since I annoyed to them, I decided to do an automated way of sending messages ( - by providing same message content - ) through their contact form using Python and Github actions.\nI can not deal with them everyday, but automation can :)\nSince there will not be explanation of the code, you can think that it is just a piece of code which automatically fills form and send it using bs4 and selenium.\n( - The code may include some bugs or unneceassary statements, you may want to update it to re-use)\nYou can see it in action from readme file of the project.\nGithub workflow file contains cron job and as well as manually execution of the code.\non: workflow_dispatch: # enable manual run inputs: git-ref: description: Git Ref (Optional) required: false schedule: # cron job run each day at 10.00 AM according to GMT+03:00  - cron: \u0026#39;0 7 * * *\u0026#39; The repository is using secrets to fill out the contact form. Everyday, it sends message to krc.com.tr, takes screenshot of contact page and commits it to screenshots folder and finally finishes the process.\n- name: Complain on KRC run: | python main.py env: PHONE_NUMBER: ${{ secrets.PHONE_NUMBER }} COMPLAIN_MESSAGE: ${{ secrets.COMPLAIN_MESSAGE }} EMAIL: ${{ secrets.EMAIL }} Push changes (- screenshot -) with timestamp using a bot user to the repository.\n- name: Commit SS push run: |git config --global user.email \u0026#34;robotcuk@randommail.com\u0026#34; git config --global user.name \u0026#34;robotcuk\u0026#34; git add \u0026#39;screenshots/contact-page-*.png\u0026#39; git commit -m \u0026#34;${{ steps.date.outputs.date }} Complain is done to KRC on ${{ steps.date.outputs.date }}\u0026#34; git push origin -f main env: GITHUB_TOKEN: ${{ secrets.ROBOTCUK }} All code regarding to described process can be found here: https://github.com/merkez/krccomplain\nTo see it in action you can check out its readme file or watch it on youtube: https://youtu.be/zXrbjEpA_20\nIt can be used for anyone else who would like to complain to https://www.krc.com.tr through contact form until they update the website or put reCAPTCHA version 2.\nOh by the way, the rest of the story and updates about it, is given on project readme file, check it out if you wonder how it is ended up. ( spoiler alert: - not as fantastic as you may think - )\nRight, I like to automate the stuff.\n","permalink":"https://mrturkmen.com/posts/automation-for-complain/","summary":"I am alive !!!\n  Right, I did not post anything for long time and could not imagine that this will be my next blog post after long time, but it is indeed.\n  This is a story about a company who hold the product (- sent for warranty -) for five months and did not respond any emails.\nLet\u0026rsquo;s start from the beginning of the story, the product itself is not important, instead how they (- company -) approached to situation is important.","title":"github-actions: complain messages to a company in automated way"},{"content":"Imagine a scenario where you have a monolithic application which uses a config file to store information about log directories, cert dirs and other service information. As an example to it following config file (- it is taken and modified from Haaukins project which I work on- ) can be considered:\nhost: http: myapplication.mrturkmen.com port: insecure: 8080 secure: 8081 tls: enabled: false certfile: \u0026#34;/home/mrturkmen/certs/cert.crt\u0026#34; certkey: \u0026#34;/home/mrturkmen/certs/cert.key\u0026#34; cafile: \u0026#34;/home/mrturkmen/certs/ca.crt\u0026#34; files: ova-directory: \u0026#34;/home/mrturkmen/ova\u0026#34; users-file: \u0026#34;/home/mrturkmen/configs/users.yml\u0026#34; exercises-file: \u0026#34;/home/mrturkmen/configs/exercises.yml\u0026#34; frontends-file: \u0026#34;/home/mrturkmen/configs/frontends.yml\u0026#34; prodmode: true vpn-service: grpc: \u0026#34;vpnservice.mrturkmen.com:4000\u0026#34; auth-key: random-auth-key sign-key: random-sign-key tls: enabled: true certfile: \u0026#34;/home/mrturkmen/certs/cert.crt\u0026#34; certkey: \u0026#34;/home/mrturkmen/certs/cert.key\u0026#34; cafile: \u0026#34;/home/mrturkmen/certs/ca.crt\u0026#34; In this config file we have some set of keys which are defined to be used inside the application, however let\u0026rsquo;s say we would like to update some values from the config file. Then in normal cases (-if no hot reload kind of function implemented- ), user needs to restart entire application. It means application will have some down time, it may be less or more however it is not good way of doing it, in particular to update only a value from config file.\nIn this point, os signals can be used to update config file without restarting or closing the application. There are some other libraries which it is possible to enable watch on config file mode. It means the library will immediately notify entire application when there is change on the config file. However in this scenario, I assume that there is no such a library or framework is integrated.\nHere I am considering the situation from Go language perspective, this may differ or not needed for some programming languages or frameworks.\nChannel and go routine will be used to listen any SIGHUP signal to the process of the application.\nFirst of all, it is nice to create the function which will re-assign config variable of the application when SIGHUP signal received.\nfunc (a *application) ReloadConfig(confFile *string) error { conf, err := NewConfigFromFile(*confFile) if err != nil { return err } a.conf = conf // re-assign applicaiton config file \treturn nil } When SIGNUP signal received by user given function above needs to be called to update configuration file.\nHere is the code which listens any SIGHUP signal to the application\nfunc handleHotConfigReload(confFile *string, reload func(confFile *string) error) { c := make(chan os.Signal, 1) // channel to wait os.Signal \tsignal.Notify(c, syscall.SIGHUP) go func() { // go routine to do not block other requests on the application \t\u0026lt;-c log.Info().Msgf(\u0026#34;Hot reload for config file...\u0026#34;) if err := reload(confFile); err != nil { log.Error().Msgf(\u0026#34;Error on reloading config file: %s\u0026#34;, err) os.Exit(1) } log.Info().Msgf(\u0026#34;Config is updated !\u0026#34;) }() } This function can be called before or after the application started. It can be called as shown below:\nhandleHotConfigReload(confFilePtr, func(confFile *string) error { return a.ReloadConfig(confFilePtr) // a is application struct \t}) Then it can be tested with :\n$ kill -SIGHUP \u0026lt;process-id\u0026gt; It will create SIGHUP signal on the process to call ReloadConfig function.\nThis is how OS Signal can be used to update configuration file in an application which is written in Go, when you do not have already implemented library or framework.\n","permalink":"https://mrturkmen.com/posts/hot-reload-with-os-signals/","summary":"Imagine a scenario where you have a monolithic application which uses a config file to store information about log directories, cert dirs and other service information. As an example to it following config file (- it is taken and modified from Haaukins project which I work on- ) can be considered:\nhost: http: myapplication.mrturkmen.com port: insecure: 8080 secure: 8081 tls: enabled: false certfile: \u0026#34;/home/mrturkmen/certs/cert.crt\u0026#34; certkey: \u0026#34;/home/mrturkmen/certs/cert.key\u0026#34; cafile: \u0026#34;/home/mrturkmen/certs/ca.crt\u0026#34; files: ova-directory: \u0026#34;/home/mrturkmen/ova\u0026#34; users-file: \u0026#34;/home/mrturkmen/configs/users.","title":"go[channels]: hot config reload with os signal "},{"content":" THE REPOSITORY: https://github.com/merkez/ubuntu-packer\nIn this blog post, provisioning and customizing images using packer will be shown with a template repository.\nIf you are asking or wondering what is Packer, the official definition is :\n Packer is a free and open source tool for creating golden images for multiple platforms from a single source configuration. (From Official Website).\n This post includes provisioning of ubuntu image on AWS and local.\nBuild Custom Ubuntu 20.04 LTS on Local In an ideal repository of Packer template, it would be nice to have a skeleton where it includes uploads, http, scripts folders along packer configuration file with a readme. Overall, the structure of folder might look like this :\n‚îú‚îÄ‚îÄ http ‚îÇ ‚îî‚îÄ‚îÄ preseed.cfg # required to change defualt values of ubuntu image ‚îú‚îÄ‚îÄ readme.md # readme file to have instructions about what to do ‚îú‚îÄ‚îÄ scripts # scripts/ dir, includes scripts to run on custom image ‚îÇ ‚îú‚îÄ‚îÄ cleanup.sh # cleans up /tmp  ‚îÇ ‚îú‚îÄ‚îÄ install_tools.sh # installs custom tools ‚îÇ ‚îî‚îÄ‚îÄ setup.sh # setting up config in system wise ‚îú‚îÄ‚îÄ ubuntu-20.04.json # packer config for ubuntu 20.04 ‚îî‚îÄ‚îÄ uploads # directory to upload files to custom image  ‚îî‚îÄ‚îÄ .gitkeep In this setup, http/preseed.cfg defines answers to the questions which may be asked during installation of Ubuntu operating system. More information regarding to preseed.cfg file can be checked from its wiki\n  scripts folder composed of bash scripts, chef, ansible or any other installer configuration files or scripts which will install customized tools and define settings of ubuntu image.\n  uploads folder includes all files, deb packages, or any other files which will be copied to image which will be inside customized image.\n  Anatomy of Packer Configuration File Any packer file composed of three main components which are ;\nBuilders Define the desired platform and platform configurations, including API Key information and desired source images. Example snippet is given from the Packer file:\n\u0026#34;builders\u0026#34;: [ { \u0026#34;boot_command\u0026#34;: [ \u0026#34;\u0026lt;esc\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;esc\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;/install/vmlinuz\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; auto\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/ask_detect=false\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/layoutcode=us\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/modelcode=pc105\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; debconf/frontend=noninteractive\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; debian-installer=en_US\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; fb=false\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; initrd=/install/initrd.gz\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; kbd-chooser/method=us\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; keyboard-configuration/layout=USA\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; keyboard-configuration/variant=USA\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; locale=en_US\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; netcfg/get_domain=vm\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; netcfg/get_hostname=ubuntu\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; grub-installer/bootdev=/dev/sda\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; noapic\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; -- \u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026lt;wait\u0026gt;\u0026#34; ], \u0026#34;boot_wait\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ova\u0026#34;, \u0026#34;disk_size\u0026#34;: 25240, \u0026#34;guest_additions_path\u0026#34;: \u0026#34;VBoxGuestAdditions_{{.Version}}.iso\u0026#34;, \u0026#34;guest_os_type\u0026#34;: \u0026#34;Ubuntu_64\u0026#34;, \u0026#34;headless\u0026#34;: true, \u0026#34;http_directory\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;iso_checksum\u0026#34;: \u0026#34;sha256:f11bda2f2caed8f420802b59f382c25160b114ccc665dbac9c5046e7fceaced2\u0026#34;, \u0026#34;iso_urls\u0026#34;: [ \u0026#34;iso/ubuntu-20.04.1-legacy-server-amd64.iso\u0026#34;, \u0026#34;https://cdimage.ubuntu.com/ubuntu-legacy-server/releases/20.04/release/ubuntu-20.04.1-legacy-server-amd64.iso\u0026#34; ], \u0026#34;shutdown_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39;|sudo -S shutdown -P now\u0026#34;, \u0026#34;ssh_password\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;ssh_port\u0026#34;: 22, \u0026#34;ssh_timeout\u0026#34;: \u0026#34;10000s\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;virtualbox-iso\u0026#34;, \u0026#34;vboxmanage\u0026#34;: [ [ \u0026#34;modifyvm\u0026#34;, \u0026#34;{{.Name}}\u0026#34;, \u0026#34;--memory\u0026#34;, \u0026#34;2048\u0026#34; ], [ \u0026#34;modifyvm\u0026#34;, \u0026#34;{{.Name}}\u0026#34;, \u0026#34;--cpus\u0026#34;, \u0026#34;1\u0026#34; ] ], \u0026#34;virtualbox_version_file\u0026#34;: \u0026#34;.vbox_version\u0026#34;, \u0026#34;vm_name\u0026#34;: \u0026#34;ubuntu_vm_ubuntu_20_{{timestamp}}\u0026#34; } ] In the builders config, we are defining some set of keys in JSON file, which are very obvious from its name, we are considering to build image locally. All the keys are important in given builders config however most important and might need to update time to time is iso_urls which are the places where packer download iamges and customize it according to your scripts. Another crucial key is to have headless value true which means that there will be no GUI running when packer command is executed to run the Packer JSON file.\nProvisioner Defines how to configure the image most likely by your using existing configuration management tools like Ansible, Chef, Puppet or pure bash scripts.\nIn our example, bash scripts will be provided to install tools and update configuration of ubuntu image to make it customized. Provisioner section of a Packer JSON file can be seen as below:\n\u0026#34;provisioners\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;file\u0026#34;, \u0026#34;source\u0026#34;:\u0026#34;uploads\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/home/ubuntu\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/install_tools.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/setup.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/cleanup.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; } ] Here we are defining existing bash scripts in order to execute in the process of customizing Ubuntu image. The steps under provisioners are pretty clear.\n  The content of uploads file will be uploaded to home directory /home/ubuntu\n  In second step, install_tools.sh will be executed and other steps will be followed in order.\n  Post Processors Related to the builder, runs after the image is built, it is generally used to generate or apply artifacts. In this example, it is not required however more information can be found here: post processors\nCommunicator How packer works on the machine image during the creation. By default it is over SSH communication and it does not need to be defined explicitly. More information can be found here: communicator\nOver all packer file can be seen as follow:\n{ \u0026#34;builders\u0026#34;: [ { \u0026#34;boot_command\u0026#34;: [ \u0026#34;\u0026lt;esc\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;esc\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;/install/vmlinuz\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; auto\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/ask_detect=false\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/layoutcode=us\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/modelcode=pc105\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; debconf/frontend=noninteractive\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; debian-installer=en_US\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; fb=false\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; initrd=/install/initrd.gz\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; kbd-chooser/method=us\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; keyboard-configuration/layout=USA\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; keyboard-configuration/variant=USA\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; locale=en_US\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; netcfg/get_domain=vm\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; netcfg/get_hostname=ubuntu\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; grub-installer/bootdev=/dev/sda\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; noapic\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; -- \u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026lt;wait\u0026gt;\u0026#34; ], \u0026#34;boot_wait\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ova\u0026#34;, \u0026#34;disk_size\u0026#34;: 25240, \u0026#34;guest_additions_path\u0026#34;: \u0026#34;VBoxGuestAdditions_{{.Version}}.iso\u0026#34;, \u0026#34;guest_os_type\u0026#34;: \u0026#34;Ubuntu_64\u0026#34;, \u0026#34;headless\u0026#34;: true, \u0026#34;http_directory\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;iso_checksum\u0026#34;: \u0026#34;sha256:f11bda2f2caed8f420802b59f382c25160b114ccc665dbac9c5046e7fceaced2\u0026#34;, \u0026#34;iso_urls\u0026#34;: [ \u0026#34;iso/ubuntu-20.04.1-legacy-server-amd64.iso\u0026#34;, \u0026#34;https://cdimage.ubuntu.com/ubuntu-legacy-server/releases/20.04/release/ubuntu-20.04.1-legacy-server-amd64.iso\u0026#34; ], \u0026#34;shutdown_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39;|sudo -S shutdown -P now\u0026#34;, \u0026#34;ssh_password\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;ssh_port\u0026#34;: 22, \u0026#34;ssh_timeout\u0026#34;: \u0026#34;10000s\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;virtualbox-iso\u0026#34;, \u0026#34;vboxmanage\u0026#34;: [ [ \u0026#34;modifyvm\u0026#34;, \u0026#34;{{.Name}}\u0026#34;, \u0026#34;--memory\u0026#34;, \u0026#34;2048\u0026#34; ], [ \u0026#34;modifyvm\u0026#34;, \u0026#34;{{.Name}}\u0026#34;, \u0026#34;--cpus\u0026#34;, \u0026#34;1\u0026#34; ] ], \u0026#34;virtualbox_version_file\u0026#34;: \u0026#34;.vbox_version\u0026#34;, \u0026#34;vm_name\u0026#34;: \u0026#34;ubuntu_vm_ubuntu_20_{{timestamp}}\u0026#34; } ], \u0026#34;provisioners\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;file\u0026#34;, \u0026#34;source\u0026#34;:\u0026#34;uploads\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/home/ubuntu\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/install_tools.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/setup.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/cleanup.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; } ], \u0026#34;variables\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;0.1\u0026#34; } } How to run locally This file can be run from the place where ubuntu-20.04.json file is located.\n$ packer build ubuntu-20.04.json It will start to build custom image by installing tools which are defined under scripts and configure username and password according to preseed.cfg and setup.sh files.\nBuild Custom Ubuntu 20.04 LTS on Cloud It is more practical and preferrable to use if you already have an cloud option to consider. This packer configuration will create custom image directly on cloud and save it to AMIs to your AWS account.\nThe anatomy of packer files is similar, only section which needs to be changed compared to local one, is builders section. It is defining all required AWS variables and AMIs to customize.\nAs an cloud example AWS will be used to create custom image.\nBuilders on Cloud \u0026#34;builders\u0026#34;: [ { \u0026#34;type\u0026#34;:\u0026#34;amazon-ebs\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;{{user `aws_region`}}\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;{{user `aws_access_key`}}\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;{{user `aws_secret_key`}}\u0026#34;, \u0026#34;subnet_id\u0026#34;: \u0026#34;{{user `aws_subnet_id`}}\u0026#34;, \u0026#34;security_group_id\u0026#34;: \u0026#34;{{user `aws_security_group`}}\u0026#34;, \u0026#34;source_ami_filter\u0026#34;: { \u0026#34;filters\u0026#34;: { \u0026#34;virtualization-type\u0026#34;: \u0026#34;hvm\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ubuntu/images/*ubuntu-focal-20.04-amd64-server-*\u0026#34;, \u0026#34;root-device-type\u0026#34;: \u0026#34;ebs\u0026#34; }, \u0026#34;owners\u0026#34;: [\u0026#34;099720109477\u0026#34;], \u0026#34;most_recent\u0026#34;: true }, \u0026#34;instance_type\u0026#34;: \u0026#34;{{user `instance_type`}}\u0026#34;, \u0026#34;ssh_username\u0026#34;:\u0026#34;ubuntu\u0026#34;, \u0026#34;ami_name\u0026#34;: \u0026#34;ubuntu-ami-custom_{{timestamp}}\u0026#34; } ] In this configuration, all keys are important to consider, however there are some which are crucial and required to run it. More information about the keys can be found here: Amazon AMI Builder\nWe would like to create a custom Ubuntu-20.04 image on cloud and save it as AMI to run it later, we are searching its pattern from available AMIs on AWS Management Console or it can be found through out this website : https://cloud-images.ubuntu.com/locator/ec2/\nOnce you have declared which AMI to customize, it needs to be located under source_ami_filter with wildcards and owners. Setting most_recent to true means that when this Packer JSON file is executed it will fetch and customize last updated AMI.\nAccess Key, Secret Key are required and should not be exposed to public in any moment, if exposed, they need to be updated immediately. They will be used to communicate with AWS to fire up instances to create custom image according to given settings defined in builders and provisioners.\nThe values of keys are defined in variables and parsed from out of it.\n\u0026#34;variables\u0026#34;: { \u0026#34;aws_access_key\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;aws_secret_key\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;aws_region\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;aws_vpc\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;aws_subnet\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ami_name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ami_description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;builder_name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;username\u0026#34;:\u0026#34;ubuntu\u0026#34;, \u0026#34;instance_type\u0026#34;:\u0026#34;t2.medium\u0026#34;, \u0026#34;tarball\u0026#34;: \u0026#34;\u0026#34; }, In variables section, username, instance_type, aws_access_key, aws_secret_key variables should be set correctly to create the image on cloud. Other variables are optional and variables section can be populated more.\nCustomize settins on Cloud On cloud builds, cloud configuration file should be used instead of preseed.cfg to customize settings. The defaults.cfg file where it contains custom settings such as default username, password, changing visudo file and more. Example defaults.cfg can be as follow:\n#cloud-config system_info: default_user: name: ubuntu sudo: [\u0026#34;ALL=(ALL) NOPASSWD:ALL\u0026#34;] lock_passwd: false plain_text_passwd: \u0026#39;ubuntu\u0026#39; More information regarding to defaults.cfg file can be found here and customized more: https://cloudinit.readthedocs.io/en/latest/topics/examples.html\nHow to run Once variables are set, it can be run in same way with the local one.\n$ packer build aws_packer.json Complete packer JSON file : aws_packer.json\nAs a summary, Packer is really cool tool to use to automate the process of creating custom images and it can be used for Dockers as well. For local example in this post, it will produce OVA file to import, on cloud it will generate custom AMI under your AWS account.\nAll scripts and config files can be found in this repository: https://github.com/merkez/ubuntu-packer\n","permalink":"https://mrturkmen.com/posts/build-with-packer/","summary":"THE REPOSITORY: https://github.com/merkez/ubuntu-packer\nIn this blog post, provisioning and customizing images using packer will be shown with a template repository.\nIf you are asking or wondering what is Packer, the official definition is :\n Packer is a free and open source tool for creating golden images for multiple platforms from a single source configuration. (From Official Website).\n This post includes provisioning of ubuntu image on AWS and local.","title":"packer: build custom images on cloud and local "},{"content":"fail2ban A while ago, I was checking servers' logs to see any suspicious activities going on from outside. I noticed that the servers both staging/testing and production servers are receiving a lot of brute force SSH attacks from variety of countries which are shown in table below.\n List of IP Addresses ( who are doing SSH Brute Forcing ) ** Information on the table gathered from: [ https://www.maxmind.com/en/geoip-demo ]\n Ban failed attempts Although servers have no password login, they are kept brute forcing on SSH port. Well, fail2ban was one of obvious solution to block those IP addresses permanently or temporarily. I prefered to block them all permanently until manual unblocking has been done by me.\nThe steps for installing fail2ban is pretty obvious, you are doing same things like, apt-get update \u0026amp;\u0026amp; apt-get install fail2ban. After installation completed, configuration is much more important.\nFollowing steps will guide you to block any ip address who are brute forcing on SSH.\n  Copy template file  $ cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local   Set Ban time\nIt is possible to set ban time permanent or temporarily. I preffered to setup permanent, so for this reason I have changed bantime = -1. Save and exit from the file when you are done.\n  $ vim /etc/fail2ban/jail.conf # Permanent ban  bantime = -1  Create custom rules for SSH  $ vim /etc/fail2ban/jail.d/sshd.local [sshd] enabled = true port = ssh filter = sshd logpath = /var/log/auth.log # place of ssh logs  maxretry = 4 # maximum number of attempts that user can do  (*Maxretry value and log file can be changed according to your setup.)\n  Make the rules persistent\nIn order to make the rules persistent which means, the blocked IPs will not be deleted after restart of fail2ban service or restart of server. It requires to have some tricks to be done inside iptables rules under fail2ban. Add following cat and echo commands at the end of actionstart and actionban respectively .\n  $ vim /etc/fail2ban/action.d/iptables-multiport.conf . . . actionstart = iptables -N fail2ban-\u0026lt;name\u0026gt; iptables -A fail2ban-\u0026lt;name\u0026gt; -j RETURN iptables -I \u0026lt;chain\u0026gt; -p \u0026lt;protocol\u0026gt; -m multiport --dports \u0026lt;port\u0026gt; -j fail2ban-\u0026lt;name\u0026gt; cat /etc/fail2ban/persistent.bans | awk \u0026#39;/^fail2ban-\u0026lt;name\u0026gt;/ {print $2}\u0026#39; \\  | while read IP; do iptables -I fail2ban-\u0026lt;name\u0026gt; 1 -s $IP -j \u0026lt;blocktype\u0026gt;; done . . . actionban = iptables -I fail2ban-\u0026lt;name\u0026gt; 1 -s \u0026lt;ip\u0026gt; -j \u0026lt;blocktype\u0026gt; echo \u0026#34;fail2ban-\u0026lt;name\u0026gt; \u0026lt;ip\u0026gt;\u0026#34; \u0026gt;\u0026gt; /etc/fail2ban/persistent.bans  Save and restart service  $ systemctl restart fail2ban These are most basic steps to block IP addresses who are actively brute forcing to servers. After some time, I am able to see them with following command :)\n$ sudo fail2ban-client status sshd Status for the jail: sshd |- Filter | |- Currently failed:\t12 | |- Total failed:\t107 | `- File list:\t/var/log/auth.log `- Actions |- Currently banned:\t16 |- Total banned:\t16 `- Banned IP list:\t171.239.254.84 184.102.70.222 180.251.85.85 103.249.240.208 159.65.194.150 117.217.35.114 113.164.79.129 61.14.228.170 116.110.30.245 43.239.80.181 77.222.130.223 14.255.137.219 184.22.195.230 125.25.82.12 116.110.109.90 115.76.168.231 It is growing in time however at least they are not able to brute force the server with same IP addresses. There are plenty of other ways to make SSH port much more secure and effective however I think having updated ssh daemon/client, passwordless login and fail2ban will be enough in most of the cases. Therefore, while I was doing this stuff, although there are plenty of guides over there, I wanted to note down how I did it to come back and check if something happens.\nTake care !\n","permalink":"https://mrturkmen.com/posts/fail2ban/","summary":"fail2ban A while ago, I was checking servers' logs to see any suspicious activities going on from outside. I noticed that the servers both staging/testing and production servers are receiving a lot of brute force SSH attacks from variety of countries which are shown in table below.\n List of IP Addresses ( who are doing SSH Brute Forcing ) ** Information on the table gathered from: [ https://www.maxmind.com/en/geoip-demo ]","title":"fail2ban: block ssh bruteforce attacks "},{"content":"In this post, deployment process of an application with Ansible will be explained. Traditionally applications can be deployed in different ways, quite similar approach to deploy applications like in Ansible is executing bash script which has ssh commands. To give an example, Travis continuous integration has a feature where a bash script can be defined to deploy application and through given instructions within bash script, application can be successfully deployed.\nDetails regarding to deployment using Travis bash scripting can be found here\nTravis Script Deployment I would like to give an real case example from one of the project which I work on. We were using Travis script deployment for a while and it works pretty well. The bash script which I use in our deployment process is given below:\n#!/usr/bin/env bash f=dist/hknd_linux_amd64/hknd amigo=./svcs/amigo user=ntpd hostname=sec02.lab.es.aau.dk keyfile=./travis_deploy_key deploy_path=/data/home/ntpd/daemon/hknd amigo_path=/data/home/ntpd/daemon/svcs/amigo if [ -f $f ]; then echo \u0026#34;Deploying \u0026#39;$f\u0026#39; to \u0026#39;$hostname\u0026#39;\u0026#34; chmod 600 $keyfile ssh -i $keyfile -o StrictHostKeyChecking=no $user@$hostname sudo /bin/systemctl stop hknd.service scp -i $keyfile -o StrictHostKeyChecking=no $f $user@$hostname:$deploy_path scp -i $keyfile -r -o StrictHostKeyChecking=no $amigo $user@$hostname:$amigo_path ssh -i $keyfile -o StrictHostKeyChecking=no $user@$hostname sudo /bin/systemctl start hknd.service else echo \u0026#34;Error: $fdoes not exist\u0026#34; exit 1 fi As you can observe from the bash script, every step of the deployment is given as ssh/scp commands. There is no harm regarding to it as long as it contains few steps. However, as time pass more configurations, applications will required to be deployed, updated, modified and checked, then it might turn into headache. Therefore, having well structured deployment steps using Ansible will put us to safe side.\nBefore jumping into deployment with Ansible, I would like to point out some factors which can be counted as disadvantages of not integrating Ansible to deployment process.\n Not common way of utilizing resources Not well structured deployment scripts which has high potential of being not working very well. Having plain ssh commands increase likelihood of having issues regarding to settings, deployments and more.  There are many more drawbacks of using pure bash scripts in deployment process, however, these issues may not be applicable for all them.\nFor our case, I would like to convert our bash script given above to Ansible which has more elegant structure and easy to manage.\nMove to Ansible Since the bash script does not contain complex instructions, it would be very easy to convert it into Ansible playbooks. Before starting to convert it into Ansible playbook, necessary ssh connection should be set correctly for development and production environments. (- test environment as well if required -).\nSetting ssh connection between server and ansible user is pretty straitforward, it contains following steps;\n Generate SSH Key pair Copy public key to authorized_keys on server side Encrypt private key Have decrypt script to use private key on CI without compromising it.  Overall simplified flow for deployment is given below :\nAs it is declared from overall picture above, we need to provide encrypted ssh key and script for decryption together, in order to use plain private key to access the server.\nIn this setup, Github CI will be control node which will have access to server where we would like to deploy the application.\nLet\u0026rsquo;s start to complete steps,\n  Generate SSH Key pair\n$ ssh-keygen You can keep everything default or provide some information about the questions when you run it. Once, execution of command finished, there will be public and private key, you need to append the public key to user' authorized keys file on server. Afterwards, connection should be established, you may want to test it using traditional ssh command.\n  Encrypt Private Key\nIn order to use the ssh key which is generated before, we need to encrypt the key, I preferred to use gpg tool, there are many examples about it on internet, you can check it if you wish.\n$ gpg --symmetric --cipher-algo AES256 \u0026lt;private-key-file\u0026gt; The command will prompt you to provide passphrase to encrpyt and decrypt the private key when required. Choose strong and long passphrase. Once it is done, include encrpyted file into git. (- which means commit it as well-)\n  Once they have completed, the rest is structing Ansible playbook to deploy the file to server.\nExample Repo I am going to create a repository on Github to demonstrate what I have described earlier in action.\nFor the demo purposes, I will upload a service file to server and start it, simplified version of given bash commands above.\nAnsible playbook will contain following;\n Stopping already running service Changing binary file of the service Starting it again  The tasks can be extended according to user needs however to keep it short and show how Ansible could be used on continuous integration, I will continue to have minimal playbook.\nLink to example repository: https://github.com/merkez/ansible-deploy\nThe structure of the repository as following:\nAs it can be observed from the figurre above, I have only three tasks which are combined under main.yml.\nSome configuration regarding to Ansible, such as private key, inventory file location declaration is saved to file ansible.cfg among ssh connection configuration.\nInventory file contains server(s) to deploy the application.\nThis post is not about how to write ansible playbooks, hence, I am going to skip to explain it. If you would like to check and understand it you can check following repositories for examples;\n DevOps Learning Journey Handwritten notes about Ansible  Decrypt script is crucial file which is decrypting encrypted private key to access the server.\nDO NOT FORGET TO SET YOUR SECRET_PASSPHRASE TO SECRETS OF THE REPOSITORY\nThe workflow file The workflow file for this repository is pretty straitforward to create as well, what needs to be done is that ansible should be installed into environment. Afterwards, running ansible playbook command after decrpyting the encrypted private key will complete the tasks.\nThe generated workflow is for giving demonstration, in normal production case, the pipeline should NOT be broken, each step from testing to production deployment should be as much as automated.\nThe completed workflow file:\n# This is a basic workflow to help you get started with Actions name: CI # Controls when the action will run.  on: # Triggers the workflow on tagged commits  push: tags: - \u0026#39;*.*.*\u0026#39; # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called \u0026#34;build\u0026#34; build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 - name: Install Ansible run: |sudo apt update -y sudo apt install software-properties-common -y sudo apt-add-repository --yes --update ppa:ansible/ansible sudo apt install ansible -y - name: Set Execute command to bash script run: chmod +x ./.github/scripts/decrypt.sh # Runs a single command using the runners shell - name: Decrypt large secret run: ./.github/scripts/decrypt.sh env: SECRET_PASSPHRASE: ${{ secrets.SECRET_PASSPHRASE }} - name: Escalate Private Key Permissions run: chmod 400 ~/.privkey - name: Run ansible command run: | ansible-playbook -i ./inventory main.yml env: ANSIBLE_CONFIG: ./ansible.cfg - name: Clean Key run: rm -rf ~/.privkey The final result from Github actions:\nKeep in mind that this is just a minor portion of a long pipeline which has all unit tests, checks, linting and integration tests. Without proper pipeline in place, having Ansible might not be logical or required. Consider your cases when you would like to move to deployment with Ansible.\nCheers !\n","permalink":"https://mrturkmen.com/posts/deploy-with-ansible/","summary":"In this post, deployment process of an application with Ansible will be explained. Traditionally applications can be deployed in different ways, quite similar approach to deploy applications like in Ansible is executing bash script which has ssh commands. To give an example, Travis continuous integration has a feature where a bash script can be defined to deploy application and through given instructions within bash script, application can be successfully deployed.","title":"ansible: deploy easily in simple steps"},{"content":"While watching video tutorial about Ansible, I took some notes and created following PDF file.\nINTRODUCTION TO ANSIBLE HANDWRITTEN NOTES\n","permalink":"https://mrturkmen.com/posts/introduction-to-ansible-notes/","summary":"While watching video tutorial about Ansible, I took some notes and created following PDF file.\nINTRODUCTION TO ANSIBLE HANDWRITTEN NOTES","title":"ansible: introductory handwritten notes"},{"content":"In some moments, Youtube algorithm is working perfect, but sometimes it shows a video from ten years ago from nowhere. For the moments where it shows and suggests videos/playlists to us, we might want to save the list of playlist and watch in some other time. It could be on a plane, train, bus, whenever you are planning to spent some time. However, taking the URL of a playlist and saving it to your cute note program might not be sufficient enough. There is a high chance that it will be forgotten or missed, therefore, I thought that it would be nice to have an automated way of saving playlists on somewhere and download them when I need. (- in particular, when there is no or limited internet connection -).\nIn this blog post, I will go through a simple project which downloads all the videos in a playlist and generates seperate tar.gz files for each playlist to release on Github using Github actions.\nRequirements Whenever starting a project, it is always nice to imply divide and conquer approach if you know what you would like to achieve. Divide and conquer approach will hugely assist you during the development and planning no matter what is the size of the project. As first step, let\u0026rsquo;s define what we need for accomblishing such a thing. ]\n A library/program which downloads Youtube videos from given URL. A library/program which compress the downloaded videos to minimize the size. A workflow on Github Actions to trigger releases.  When the given components are clarified, only one more step left to have. There should be main component which combines the requirements given above. For this purpose, I will use Go programming language.\nAvailable Tools When existing libraries, tools and open source projects checked for the first requirement, there are some on Github, namely;\n annie: üëæ Fast, simple and clean video downloader youtube-dl: Command-line program to download videos from YouTube.com and other video sites you-get: Dumb downloader that scrapes the web ytdl: YouTube download library and CLI written in Go  These are the tools which enable users to download Youtube videos by providing the URL or the ID.(- some of them supports different social media platforms too, e.g vimeo -).\nTo make things simpler, I chose to use youtube-dl, since it is more promising than others and formats the output very well according to user output pattern.\nAnother point is to clarify which tool/library should I use to compress the downloaded videos from Youtube. With help of a little bit googling, I found out that pigz is quite nice tool which compress given folder/file in paralel by using all available cores of the machine. The second requirement is cleared as well, now it is time to combine both of them in one and add Github workflow on top it.\nI will mention about the Github workflow file, after structure of the program which automates the process.\nCode Structure To make things faster (- in terms of development time -), I decided to go with using pre-existing binary file to execute commands, what I mean by that is basically having a pre-installed tool (youtube-dl \u0026amp; pigz) on the system before using this application.\nIf the readme file of youtube-dl is checked, youtube-dl can be installed as command line tool into your environment. It means that we can call the tool whenever we need from our application. There are other ways to accomblish it as well, such as instead of using pre-existing binary file, we can implement the functions in our application. However, the main idea of this post is NOT about how to create or use the library, the main idea is to present how it easy to have automated way of retrieving Youtube playlist videos and saving to Github Releases. The other requirement regarding to compress can be used in similar way. (- using a pre-existin command from system -)\nTo make things simple and extendable (- which means in case of more integration of tools we should be able to accomblish it without changing, many lines of code -), I will generate a main Client struct which will have exec function and it will be overridden according to command we pass.\nThe main client struct :\ntype Client struct { //youtube-dl client  YoutubeDL *YoutubeDL // Tar client \tTar *Tar // Used to enable root command \tsudo bool // flags to service \tflags []string // enable debug or not \tdebug bool // Implementation of ExecFunc. \texecFunc ExecFunc // Implementation of PipeFunc. \tpipeFunc PipeFunc } Client struct has some fields which enables us to override whenever we want, the struct contains exec(cmd string, args ...string) ([]byte, error), shellPipe(stdin io.Reader, cmd string, args ...string) ([]byte, error), and shellExec(cmd string, args ...string) ([]byte, error) functions. It can be extended according to our requirements in the future. The explanations of the functions are given on top of functions inside the source code.\nFor the youtube-dl client, I implemented only a function (-the client functionalities are really easy to extend-), which downloads all videos on given playlist by using pre-existing command line tool youtube-dl.\npackage client type YoutubeDL struct { c *Client } // exec executes an ExecFunc using \u0026#39;youtube-dl\u0026#39;. func (ytdl *YoutubeDL) exec(args ...string) ([]byte, error) { return ytdl.c.exec(\u0026#34;youtube-dl\u0026#34;, args...) } // DownloadWithOutputName generates Folder named with Playlist name // downloads videos under given playlist url to Folder func (ytdl *YoutubeDL) DownloadWithOutputName(folderName, url string) error { cmds := []string{\u0026#34;-o\u0026#34;, folderName + \u0026#34;/%(playlist_index)s - %(title)s.%(ext)s\u0026#34;, url} _, err := ytdl.exec(cmds...) return err } For any other additinal tool to use, it is extremely practical to add, for tar tool I have implemented following for specific purpose ( -which is compressing downloaded videos in paralel- ).\npackage client type Tar struct { c *Client } // exec executes an ExecFunc using \u0026#39;tar\u0026#39; command. func (tr *Tar) exec(args ...string) ([]byte, error) { return tr.c.exec(\u0026#34;tar\u0026#34;, args...) } // CompressWithPIGZ using tar with pigz compress program to compress given data func (tr *Tar) CompressWithPIGZ(fileName, folderToCompress string) error { cmds := []string{\u0026#34;--use-compress-program=pigz\u0026#34;, \u0026#34;-cf\u0026#34;, fileName, folderToCompress} _, err := tr.exec(cmds...) if err != nil { return err } return nil } Now, it is clear that for the both statements in the requirements section has been done. However, I wanted to keep track of what I have downloaded and release, for this reason, I have created two different csv files. They are called playlist-list.csv and old-playlist-list.csv under resources/ directory in the repository, playlist-list.csv will include all list of playlist URLs with preferred folder name to download. Futhermore, as you can guess, old-playlist-list.csv will include all the playlists which are downloaded and released. Once the playlist is downloaded and released with Github actions playlist-list.csv will be wiped and all content will be appended into old-playlist-list.csv file.\nIt will give easy way of checking what has been downloaded and released.\nThe code for reading and writing to csv files are pretty easy, and can be checked under main.go in the repository.\nWorkflow File The workflow file will include some steps, which are;\n Install pigz : required to compress data in parallel. Install youtube-dl : required to download playlist from given URL. Build Binary : required to have combined binary which handles both download and compress using pre-existing tools on the system. Create Release : the step which initializes releases. Run Binary : executes the program Upload videos to Github releases : uploads downloaded content to releases. Remove playlist and append downloaded playlists to old list : updates the list inside the playlist file and commits on master branch.  The steps given above are clickable to see inside the workflow on repository.\nThis small project is created for exclusive purpose, and it is very suitable to extend functionalities. However, there are many gaps regarding to the project such as;\n it does NOT check the given playlists whether they have been already released or not. it does NOT split created tar.gz files into 2 GB splits ( since it is required to have a file on Github releases under 2GB, but there is NO limitation for overall size of files on Github releases.) Does NOT have error handling mechanism and more.  These are the points which appears when the project is checked at first glance, however there are more missing points which could be done. However, the main aim was to give idea how to accomblish automated way of downloading youtube videos and releasing with Github actions.\nI am personally using it for personal needs whenever I find useful playlist, I include it into playlist-list.csv file and pushing the changes by tagging the commit in semantic versioning format.\nThere are tons of other services which could be integrated such as Slack, Discord, Mail or any another notification systems and more, however, to keep the post short and do not bother you, it is enough for now as it is.\nThe rule for the workflow could be easily changed, like instead of running it in tagged commits, it can run in scheduled way by changing run condition only, as shown below.\nname: Download \u0026amp; Release Youtube Playlists on: schedule: - cron: \u0026#39;0 0 * * *\u0026#39; # it means every day at midnight the workflow will run If you require or would like to have more features, or fixes, suggestions and etc, you are more welcome to open issues.\nGithub Limitations Since we are using Github actions, we have some limitations regarding to usage of it.\nThe limitations regarding to file sizes in releases, according to Github Statement here:Distributing large binaries\nBasically, a file size which will be uploaded to releases should NOT exceeds 2 GB. However, keep in mind that it is per file, there is NO limitation for overall size of the release :). It means that repository will be updated to split files into chunks if size of the file exceeds 2 GB. So, in case of 15 GB of playlist, it should be uploaded in 2GB chunks to releases. (- a feature which is NOT exists on youtubeto yet -)\nThere are some more limitations:\nJob execution time - Each job in a workflow can run for up to 6 hours of execution time. If a job reaches this limit, the job is terminated and fails to complete.\nWorkflow run time - Each workflow run is limited to 72 hours. If a workflow run reaches this limit, the workflow run is cancelled.\nMore details about limmitations on Github Actions: Usage Limits\nIt is good to keep in mind the given limitations above.\nJob execution time and Workflow run time can be easily fixed if you have your own server.\nIf you would like to run Github Actions in your server, there is no limitation regarding to Job execution time and Workflow run time.\nCheck out how to setup Github Actions for your server from here:\nSetup self hosted runners\nRepository youtubeto: Automated Youtube PlayList Releaser\nDemo \n","permalink":"https://mrturkmen.com/posts/download-release-youtube-playlists/","summary":"In some moments, Youtube algorithm is working perfect, but sometimes it shows a video from ten years ago from nowhere. For the moments where it shows and suggests videos/playlists to us, we might want to save the list of playlist and watch in some other time. It could be on a plane, train, bus, whenever you are planning to spent some time. However, taking the URL of a playlist and saving it to your cute note program might not be sufficient enough.","title":"youtubeto: download and save playlists to releases on Github"},{"content":" In this post, I will be describing to setup a workflow to build and release your Latex files through Github actions. First of all, keep in mind that this post is not about what is Latex and how to use it.\nIt is extremely nice to integrate daily development tools such as CI/CD to your preparation of paper, without any hassle. Why is that because it is cool to track of what has been changed on a paper over time. In fact, having a couple of people who are responsible in different parts of paper, sometimes blocks others. Therefore, having such a workflow will increase productivity for everyone in a group. Whenever pull request created to main branch, it will be easy to check typos, logic errors and missing points by others.\n Latex preparation Setup Github Actions Proof of Concept  Latex preparation I am assuming that you have agreed to work on Latex template to complete a paper. In this case, there is only small step left to do, create a Github repository (-it should be on Github, Github Actions will be used-) and push all files of your Latex template. (-in general, in following structure-)\n|-sections | introduction.tex | related_works.tex | problem.tex | solution.tex | conclusion.tex |- main.tex |- references.bib The given example structure can be changed according to your wishes, however important and logical part is that having main.tex on root directory of repository.\nOnce it is set, there is only one step to complete which is setting up Github Action workflows.\nSetup Github Actions There are a few different Github Actions to use for compiling Latex document to PDF on marketplace. Most preferred one is https://github.com/xu-cheng/latex-action and it is quite easy to integrate and use.\nIt basically creates generated PDF file from provided Latex file, it can be set in workflow file as given below: (- Note that this workflow runs on tagged commits which has a tag with *.*.* pattern -)\nname: Build LaTeX document on: tags: - \u0026#39;*.*.*\u0026#39; # semantic versioning  jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v2 - name: Compile LaTeX document uses: xu-cheng/latex-action@v2 with: root_file: main.tex However, setting up only this job is not sufficient enough to have completed workflow, we require to more jobs which are Create Release and Upload Release. As you may guess from their name, first one will create the release and second one will upload provided file to releases page. It can be setup as following\nname: Release Compiled PDF  on: push: tags: - \u0026#39;*.*.*\u0026#39; jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v2 - name: Compile LaTeX document uses: xu-cheng/latex-action@v2 with: root_file: main.tex - name: Create Release id: create_release uses: actions/create-release@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: tag_name: ${{ github.ref }} release_name: Release ${{ github.ref }} draft: false prerelease: false - name: Upload Release Asset id: upload-release-asset  uses: actions/upload-release-asset@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: upload_url: ${{ steps.create_release.outputs.upload_url }}  asset_path: ./main.pdf asset_name: main.pdf asset_content_type: pdf The given workflow is completed version of what you might have at the end. In summary, it builds PDF from provided Latex file, creates release and upload file to release. For more details, you can check information on each action page.\nProof of Concept Here is example repository to check completed version.\nhttps://github.com/merkez/latex-on-ci-cd\n","permalink":"https://mrturkmen.com/posts/build-release-latex/","summary":"In this post, I will be describing to setup a workflow to build and release your Latex files through Github actions. First of all, keep in mind that this post is not about what is Latex and how to use it.\nIt is extremely nice to integrate daily development tools such as CI/CD to your preparation of paper, without any hassle. Why is that because it is cool to track of what has been changed on a paper over time.","title":"auto-latex: generate and handle latex through github actions"},{"content":"In this post, I am going to write demo for a tool which I have just met, it is called Evans. It is basically universal gRPC client. What it means ? Basically when you have gRPC server and would like to test gRPC calls without creating client, you can test server side calls with Evans. It is known that gRPC is very common communication method between microservices, it can be used for internal and external communication. I do not have intention to explain what gRPC is in this post since it is not the purpose. If required documentation of gRPC can be investigated.\nCreate a simple gRPC server To demonstrate and see how evans works, a running gRPC should be exists, for this reason, I am going to provide a sample gRPC server. For this purpose, I will use Go programming language, however gRPC is supporting more programming languages which you may more familiar than Go.\ngRPC server is basically an API endpoint where clients can make requests, since it is an API, first thing could be to define which methods will be used for this service.\nFor simplicity and purpose of this post, I have created a basic microservice which will has four calls namely, Add,Delete,List and Find. Since the purpose is to understand how evans works, the gRPC server does not need to be complex or includes lots of calls.\nSample repository A sample microservice is created for demonstration purposes and all codes are available here : https://github.com/merkez/BookShelf\nIf you have already a running gRPC server, you can direcly pass to demonstration of evans, if not, you can clone https://github.com/merkez/BookShelf repository and test evans out.\nDefining calls In my opinion, it is always nice to prepare proto file before hand, because it is like a contract which creates your main service. Let\u0026rsquo;s imagine you would like to add, delete, list and find the books that you have read or wish to read. For this purpose, microservice should have at least four different calls which are Add, Delete, List and Find. There is no limit to have more calls however in order to do not get out of topic, I am keeping it small.\nFollowing proto file would be enough for BookShelf service.\n// it is important to declare syntax version syntax = \u0026#34;proto3\u0026#34;;service BookShelf { rpc AddBook(AddBookRequest) returns (AddBookResponse) {} rpc ListBook (ListBooksRequest) returns (ListBooksResponse) {} rpc DelBook (DelBookRequest) returns (DelBookResponse){} rpc FindBook (FindBookRequest) returns (FindBookResponse){}}message AddBookRequest { BookInfo book = 1; message BookInfo { string isbn =1; string name =2; string author=3; string addedBy=4; }}message AddBookResponse { string message = 1;}message ListBooksRequest {// no need to have anything // could be extended to list books based on category ... }message ListBooksResponse { repeated BookInfo books =1; message BookInfo { string isbn =1; string name =2; string author=3; string addedBy=4; }}message DelBookRequest { string isbn =1;}message DelBookResponse { string message =1;}message FindBookRequest { string isbn =1;}message FindBookResponse { Book book = 1; message Book { string isbn =1; string name =2; string author=3; string addedBy=4; }}Once proto file is declared, it becomes more easy to continue. For the demostration purposes, I will store information of books in memory. However, as you know, it is NOT acceptable for any production level application.\nCompile Proto file proto files are great since once you have defined what you need, you can directly generate codes in available languages which are represented in gRPC supported languages. The generation of codes in your desired language is pretty straitforward, I am going to generate the code for Go programming language.\n$ protoc -I proto/ proto/bs.proto --go_out=plugins=grpc:proto It will generate ready to use Go source code for your rpc calls which are defined in proto file.\nAfterwards, necessary piece of codes should be implemented, which are in memory store and book struct. For the aim of this post, I assumed that you have created all rest of the code as given in example gRPC server (-BookShelf-).\nNOTE: Generating source code through protoc requires to have protoc tool to be installed before hand. Installation of protoc is over here\nRun gRPC server The post is not covering all aspects of gRPC, proto buffers, Go language and those are not the intention of this post. Therefore, I am assuming that you had gRPC server and would like to test out and see whether your proto contract is running correctly without creating client side codes. Once it is confirmed that your gRPC calls are running without encouraging any unseen problems, then creating client side code will be much easy without any problem.\nYou can start gRPC server with:\n$ go run server/main.go BookShelf gRPC server is running .... Once gRPC server is up and running, you can use evans tool for inspecting gRPC server for available inquires.\nDemonstration of EVANS Evans is an open source project which is available at Github and I found it pretty useful, in particular, for people who have no idea what kind of calls are available in proto file, as it states its explanation, it is universal gRPC client. Installation of evans and more information is given its readme file.\nIt has plenty of features which are very handy to use for automating and testing some stuff on top of existing gRPC server or new one.\nLet\u0026rsquo;s make a demo, when you are using evans your gRPC server should be up and running, in order to make communication with universal gRPC client - evans. I assumed that you have followed readme file of evans and installed it correctly.\nNote that port 9000 is given because it is port of gRPC server.\n‚ùØ evans -r -p 9000 ______ | ____| | |__ __ __ __ _ _ __ ___ | __| \\ \\ / / / _. | | \u0026#39;_ \\  / __| | |____ \\ V / | (_| | | | | | \\__ \\  |______| \\_/ \\__,_| |_| |_| |___/ more expressive universal gRPC client BookShelf@127.0.0.1:9000\u0026gt; show services +-----------+----------+------------------+-------------------+ | SERVICE | RPC | REQUEST TYPE | RESPONSE TYPE | +-----------+----------+------------------+-------------------+ | BookShelf | AddBook | AddBookRequest | AddBookResponse | | BookShelf | ListBook | ListBooksRequest | ListBooksResponse | | BookShelf | DelBook | DelBookRequest | DelBookResponse | | BookShelf | FindBook | FindBookRequest | FindBookResponse | +-----------+----------+------------------+-------------------+ BookShelf@127.0.0.1:9000\u0026gt; As you can observe all calls which are used in proto file can be used through evans, moreover its usage is pretty straitforward.\nYou can check demonstration video below.\n‚ö†Ô∏è You may wish to change the video quality to 1080p60\nIf you have any questions, fix, or something else, do not hesitate to contact with me.\n","permalink":"https://mrturkmen.com/posts/grpc-calls-with-evans/","summary":"In this post, I am going to write demo for a tool which I have just met, it is called Evans. It is basically universal gRPC client. What it means ? Basically when you have gRPC server and would like to test gRPC calls without creating client, you can test server side calls with Evans. It is known that gRPC is very common communication method between microservices, it can be used for internal and external communication.","title":"evans: universal gRPC client demonstration"},{"content":"In recent post, which is Setup Highly Available Kubernetes Cluster with HAProxy , a highly available Kubernetes cluster is created. However, once I started to dig in and deploy some stuff to cluster, I realized that I am not able to connect any deployed application or services. For instance, when an web application is deployed using HAProxy load balancer (endpoint), and check from kubectl (on client side), its status is running. However, that application could not be reached from outside world although I re-patch an external IP address by following command\n$ kubectl patch svc \u0026lt;application-name\u0026gt; -n \u0026lt;name-of-namespace\u0026gt; -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;, \u0026#34;externalIPs\u0026#34;:[\u0026#34;\u0026lt;haproxy-ip-address\u0026gt;\u0026#34;]}}\u0026#39; After some searching and reading, I realized that worker nodes require their own ingress controllers in order to forward traffic between them in case of load. I will be giving more information of how I fix the issue, however let\u0026rsquo;s learn some basic terms and general information about ingress controller.\nWhat is ingress controller ? The best and simple explanation to this question is coming from Kubernetes official documentation over here, as they are expressing that ;\n Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.\n  An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontend to help handle the traffic.\n Whenever you have services which are running inside a cluster and would like to access them, you need to setup ingress controller for that cluster. The missing part was having no ingress controller on worker nodes in my k8s cluster. Everything was working however there was no access to them from outside world, that\u0026rsquo;s why ingress controller should take place in cluster architecture.\nIn this post, I will go for NGINX ingress controller with its default setup, however there are plenty of different ingress controllers which you may go for. I might change NGINX to Traefik in future but it depends on requirements yet for now, I will go with nginx ingress controller. The reason is that, it is super easy to setup, super rich with different features, included Kubernetes official documentation and fulfill what I am expecting for now.\nUpdates to cluster Let\u0026rsquo;s briefly what I have explained in previous post;\n Create VMs Setup SSH connection Use KubeSpray to deploy cluster Create HAProxy and establish SSH connection with all nodes.  I have noticed that when deploying cluster, some add-ons should be enabled in order to use ingress controller from cluster with external HAProxy load balancer. Now, since cluster deployment was established with Ansible playbooks, it is not needed to setup everything from scratch. All modified configuration can be re-deployed without effecting any resource which is exists on cluster setup. It means that, I can enable required parts in configuration file and re-deploy cluster as I did on previous post.\n  Enable ingress controller from inventory file inside KubeSpray\n$ vim inventory/mycluster/group_vars/k8s-cluster/addons.yml # Nginx ingress controller deployment ingress_nginx_enabled: false -\u0026gt; true Once this configuration part is updated from existing KubeSpray configuration files, k8s cluster should be redeployed with same command in previous post\nAssumption : previous configured KubeSpray settings are used.\n$ ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml It will take a while and update all necessary parts which are required.\n  Include Ingress API object to route traffic from external HAProxy server to internal services\nTo include Ingress API object, HAProxy configuration file should be modified, following lines should be added to /etc/haproxy/haproxy.cfg file.\n$ vim /etc/haproxy/haproxy.cfg frontend kubernetes-ingress-http bind *:80 default_backend kubernetes-worker-nodes-http backend kubernetes-worker-nodes-http balance leastconn option tcp-check server worker1 10.0.128.81:80 check fall 3 rise 2 server worker2 10.0.128.137:80 check fall 3 rise 2 server worker3 10.0.128.156:80 check fall 3 rise 2 In given configuration balancing algorithm is leastconn which can be changed into any load balancer algorithm which is supported by HAProxy, however leastconn algorithm is fitting more to what I would like to achieve that\u0026rsquo;s why it is declared as leastconn. Note that this configuration addition is on top of added part on previous post.\nOnce HAProxy configuration is updated, HAProxy should be restarted systemctl restart haproxy. It is all for HAProxy configuration, now let\u0026rsquo;s dive into setting up NGINX Ingress Controller.\n  Setup NGINX Ingress Controller It is super simple to deploy and setting up NGINX ingress controller since it is well documented and explains required parts in detail. To setup NGINX Ingress Controller, I will follow official guideline which is exists on NGINX Ingress Controller Installation.\nImage is taken from (https://www.nginx.com/products/nginx/kubernetes-ingress-controller/#resources)\nIn normal cases, the situation is as given figure above, however, since in existing k8s cluster, I am using HAProxy for communicating with clients, I need NGINX ingress controller inside worker nodes which will manage running applications/services by communicating with HAProxy and eventually, the services will be accessible from outside world.\nIf I summarize how overview diagram will look like in my case is like in given figure below.\nIt can be observed that, in given k8s cluster overview, HAProxy is in front, it communicates with clients, afterwards transmitting request based on defined rule on HAProxy configuration. Each worker node has NGINX ingress controller, what exactly it means, whenever a request appear to cluster, worker nodes will agree between each other and response back to user without having any problem. Since NGINX ingress controller is capable of load balancing inside worker nodes as well.\nThere is also Ingress Resource Rules part inside cluster, what it does, is that all routing rules based on path forwarded given service, an example on this is given below.\nSteps to create NGINX Ingress controller All steps shown below for installation of NGINX Ingress Controller taken from https://docs.nginx.com/nginx-ingress-controller/installation/\nMake sure that you are a client with administrator privilege, all steps related to NGINX ingress controller should be done through kubectl (on client computer/server)\n Clone Ingress Controller Repo  $ git clone https://github.com/nginxinc/kubernetes-ingress/ $ cd kubernetes-ingress  Create a namespace and a service account for the Ingress controller  $ kubectl apply -f common/ns-and-sa.yaml  Create a cluster role and cluster role binding for the service account  $ kubectl apply -f rbac/rbac.yaml  Create a secret with a TLS certificate and a key for the default server in NGINX  $ kubectl apply -f common/default-server-secret.yaml  Create a config map for customizing NGINX configuration:  $ kubectl apply -f common/nginx-config.yaml Afterwards, there are two different ways to run NGINX ingress controller deployment, which are as daemonset or as deployment. Main difference between those are summarized on official installation page as;\n Use a Deployment. When you run the Ingress Controller by using a Deployment, by default, Kubernetes will create one Ingress controller pod.\n  Use a DaemonSet: When you run the Ingress Controller by using a DaemonSet, Kubernetes will create an Ingress controller pod on every node of the cluster.\n I will go with DaemonSet approach, the reason is that generally when you have background-ish tasks which will run non-stateless then DaemonSet is more preferred way of running it.\n$ kubectl apply -f daemon-set/nginx-ingress.yaml Once it is applied as daemon set, the result could be checked with following command and result will be similar to given result below.\n$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ingress-47z8r 1/1 Running 0 24h pod/nginx-ingress-cmkfq 1/1 Running 0 24h pod/nginx-ingress-ft5pv 1/1 Running 0 24h pod/nginx-ingress-q554l 1/1 Running 0 24h pod/nginx-ingress-ssdrj 1/1 Running 0 24h pod/nginx-ingress-t9jml 1/1 Running 0 24h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/nginx-ingress 6 6 6 6 6 \u0026lt;none\u0026gt; 24h Deploy Example Application To test how an application will be exposed to externally from k8s cluster, an example applicaton could be deployed as given below. Note that the following example is simplest example for this context, hence, keep in mind that it might require more configuration and detailed approach then described here when you would like to deploy more complex applications.\n Create a sample NGINX Web Server (Using provided example)  nginx-deploy-main.yml\napiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 Taken from https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/\nIn given yaml deployment file above, two replicas of NGINX:1.14.2 will be deployed to cluster and it has name of nginx-deployment. The yaml explains itself very well.\nIt can be deployed either through directly from official link or from your local depends on your preferences.\n$ kubectl apply -f https://k8s.io/examples/application/deployment.yaml ## or you can do same thing with local file as given below $ kubectl apply -f nginx-deploy-main.yml Expose deployment:\n$ kubectl expose deploy nginx-deployment --port 80 Once it is deployed to cluster and exposed, there is one step left for this simple counter example is that, exposing the service and creating ingress rule (resource) in yaml file, by specifiying kind as Ingress.\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: \u0026lt;dns-record\u0026gt; (a domain like test.mydomain.com) http: paths: - path: / backend: serviceName: nginx-deployment servicePort: 80 The crucial part is serviceName and servicePort which are defining specifications of the services within cluster. The yaml specifications can be expanded as shown below, assume that you have wildcard record in your domain name server and have multiple services which are running in same port in a cluster, yaml file can be re-defined as given below.\nnginx-ingress-resource.yml\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-controller spec: rules: - host: \u0026lt;dns-record\u0026gt; (a domain like test.mydomain.com) http: paths: - path: / backend: serviceName: nginx-deployment servicePort: 80 - path: /apache backend: serviceName: apache-deployment servicePort: 80 - path: /native-web-server  backend: serviceName: native-web-server-deployment servicePort: 80 Keep in mind that all given services should be deployed before hand otherwise when a request made to any path which is not deployed, it may return either 404 or 500. There are plenty of different options to define and update the components in a k8s cluster. Therefore, all yaml files should be changed according to requirements.\nCreate ingress controller rules from provided yaml file\n$ kubectl create -f nginx-ingress-resource.yml Now, the NGINX web server deployment is ready on given DNS record in yaml file and according to request paths different services can be called which are also running inside kubernetes cluster.\nNote that, provided yaml files are just simple example of deploying NGINX web server without any certification, when certificates (HTTPS) enabled or any other type of deployment happened different configurations should be applied.\nWhen everything goes without any problem, you will have a cluster which uses NGINX Ingress controller for internal cluster routing and HAProxy as communication endpoint for clients. Keep in mind that whenever a new service or deployment take place, required configuration should be enabled in HAProxy configuration as it is enabled for port 80 applications above. Different services will have different requirements therefore it is important to catch main logic in a setup. It is all done for this post.\nCheers !\n","permalink":"https://mrturkmen.com/posts/setup-ingress-controller/","summary":"In recent post, which is Setup Highly Available Kubernetes Cluster with HAProxy , a highly available Kubernetes cluster is created. However, once I started to dig in and deploy some stuff to cluster, I realized that I am not able to connect any deployed application or services. For instance, when an web application is deployed using HAProxy load balancer (endpoint), and check from kubectl (on client side), its status is running.","title":"haproxy-with-nginx: setting them up for k8s cluster"},{"content":"The main purpose of this blog post a simple walkthrough of setting up Kubernetes cluster with external HAProxy which will be the endpoint where our kubectl client communicates over. Node specifications for this setup is given as shown in the table below. Keep in mind that all of them has access to each other with password and without password. The environment which Kubernetes cluster will stay is running on OpenStack. It means that once a configuration (ssh keys, hosts, and etc) is done for example master 1 then all other nodes could be initialized through snapshot of master 1. To be able to setup such a Kubernetes cluster easily, I will be using KubeSpray which is a repository where it has all required configuration and playbooks for setting up necessary cluster.\n Node Specification Prerequisites General Overview KubeSpray Configuration External Load Balancer Setup (HAProxy) Setup KubeSpray Configuration  The intention of this walkthrough is that setting up your own Kubernetes cluster in your own servers, this post is not very useful for people who are already using cloud provider solutions.(Kubernetes cluster as a service). You can checkout following resources listed below : (few of them :) )\nCloud Providers Solutions:\n Azure Kubernetes Service - AKS Google Kubernetes Engine - GKE Managed Kubernetes on DigitalOcean Kubernetes on AWS  Node Specification Kubernetes cluster will be setup on following nodes in the table below, note that HAProxy will run on another node and all ansible playbooks and setting up Kubernetes cluster will be managed through HAProxy. Keep in mind that all nodes + HAProxy is under same subnet internally which means that we will only one external IP address where HAProxy use and kubectl clients communicate. All instances are running on ubuntu_18.04, it means that the instructions and steps may not work with another system.\nPrerequisites  Nodes Requirements of KubeSpray Setting up SSH Key Across Nodes Getting snapshot ( -it is optional -) Setting up login with password  General Overview The following sketch is general overview of how Kubernetes cluster will look like at the end of this walkthrough, the figure is super overviewed version of cluster.\nIn given figure above, nodes do not have any external IP adress however, including HAProxy, all of them in same subnet, only HAProxy has external IP address which will be reachable by kubectl clients.\nBefore moving installation step of Kubernetes cluster, we need to setup a sample master node (instance) with predefined configuration. Since we will have only one server which is open to outside world, we need to make sure that there is a connection between HAProxy and sample master node. I am currently calling it sample master node, it is because, preliminary configurations such as authentication with password, disabled swap area and ssh keys will be all configured. This sample master node should be started and accesible over HAProxy, which means that in order to access to sample master node, I should do following;\n SSH to HAProxy using SSH key (Password Login disabled) like ssh -i ~/.ssh/id_rsa \u0026lt;username\u0026gt;@\u0026lt;ha-proxy-external-ip\u0026gt; Copy SSH Key to HAProxy, which let you in to sample master node Then SSH to sample master node with same approach. (ssh ~/.ssh/masternode.pem \u0026lt;username\u0026gt;@\u0026lt;master-node-ip\u0026gt;  After you are inside sample master node, now, some configurations and setting should be done. Afterwards, we can initialize other five nodes from snapshot of configured sample master node.\nSteps:\n Enable Password Login if not enabled already.  $ echo \u0026#34;PermitRootLogin yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config $ sed -i -E \u0026#39;s/PasswordAuthentication no/PasswordAuthentication yes/g\u0026#39; /etc/ssh/sshd_config  Specify Password for ROOT  $ sudo su $ passwd Given commands will ask new unix password for root user. Define the password and do not forget or lose it. Since we will gonna use snapshot of this configured machine, all settings will be same, I did like that to shortcut the process.\n Disable swap area (RUN ALL COMMANDS AS ROOT)  $ swapoff -a Afterwards, exit from sample master node, create snapshot of that node (it is called volume snapshot in OpenStack), once you have successfully created snaphot, all five other nodes should be initialized from snapshot of this sample master node. This way, there is no need to repeat same steps described above.\nIn case of not having possibility to create snapshot follow given steps (if and if only, you could NOT create snapshot and initialize other five nodes from the snapshot)\n  Create all nodes (workers and masters)\n  Enable SSH connection to all nodes from HAPRoxy server.\n  From HAProxy server, execute following steps. (-Make sure that you have configured SSH connection with ROOT priviledges and have access to all nodes from HAProxy node -)\nOnce you are sure that you have SSH access to all nodes from HAProxy through SSH, implement following steps.\n Install parallel-ssh (-to run a command in parallel on nodes-) (run with ROOT priviledges)  $ apt-get update \u0026amp;\u0026amp; apt-get install -y pssh  Install HAProxy (as ROOT priviledges)  $ apt-get install -y haproxy  Modify /etc/hosts (-For easy communication through nodes-)  Append worker and master node IPs to /etc/hosts file\n$ vim /etc/hosts 10.0.128.156 worker3 10.0.128.137 worker2 10.0.128.81 worker1 10.0.128.184 master3 10.0.128.171 master2 10.0.128.149 master1  Create nodes text file on home directory  $ cat nodes worker3 worker2 worker1 master3 master2 master1 Since IP addresses of them defined in /etc/hosts file, system can now recognize and connect IPs of them through just by name\n Generate and Copy SSH Key to all nodes (Required for easy communication)  If there is already a SSH key (like in ~/.ssh/id_rsa), you can use it as well.If not, you can do following step\n$ ssh-keygen # will prompt passphrase, you can leave empty , NOTE THAT IF YOU DO NOT HAVE SSH KEY, GENERATE IT. $ for i in $(cat nodes); ssh-copy-id $i; done The for loop given as second command will copy ssh key to all nodes, then accesing any node without password will be flawless.Like given command below;\n$ ssh master1 # in defualt uses same username with terminal session  Disable swap area on all nodes (Note that if you are using snapshot method, no need to do this step)  $ parallel-ssh -h nodes -i \u0026#34;swapoff -a\u0026#34; Parallel SSH tool is handy to complete tasks in parallel for multiple hosts.\nKubeSpray Configuration KubeSpray is a repository to setup Kubernetes clusters with predefined configuration settings using Ansible playbooks. The usage of KubeSpray is pretty straightforward, as default settings, KubeSpray is using internal load balancers in each worker node, which means that when you setup a Kubernetes cluster using default values of KubeSpray, you will have following arch overview.\nHowever, in this guide, external load balancer approach will be used to setup cluster, if you wish to leave everything as default with KubeSpray, you can skip this External Load Balancer Setup part.\nExternal Load Balancer Setup (HAProxy) Modify configuration file of HAProxy to enable external LoadBalancer, copy this following configuration and append to /etc/haproxy/haproxy.cfg. (end of file)\nlisten kubernetes-apiserver-https bind \u0026lt;your-haproxy-internal-ip\u0026gt;:8383 mode tcp option log-health-checks timeout client 3h timeout server 3h server master1 \u0026lt;your-master1-ip\u0026gt;:6443 check check-ssl verify none inter 10000 server master2 \u0026lt;your-master2-ip\u0026gt;:6443 check check-ssl verify none inter 10000 server master3 \u0026lt;your-master3-ip\u0026gt;:6443 check check-ssl verify none inter 10000 balance roundrobin Balance algorithm is roundrobin however you can change it from list of available balance algorithms provided by HAProxy.\nOnce it is done, save and restart HAProxy service.\n$ systemctl restart haproxy Setup KubeSpray Configuration Since external load balancer will be used, there is few things to be done to change default values in KubeSpray. Following steps will be done on HAProxy node.\n Clone the project and prepare environment  $ git clone https://github.com/kubernetes-sigs/kubespray $ apt-get install -y python3-pip # install pip3 if not installed $ cd kubespray  Follow the guide on KubeSpray README.md file  Following instructions taken from KubeSpray README.md\n# Install dependencies from ``requirements.txt`` sudo pip3 install -r requirements.txt # Copy ``inventory/sample`` as ``inventory/mycluster`` cp -rfp inventory/sample inventory/mycluster # Update Ansible inventory file with inventory builder declare -a IPS=(10.0.128.149 10.0.128.171 10.0.128.184 10.0.128.81 10.0.128.137 10.0.128.156) CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}  Modify generate hosts YAML file  When you check inventory/mycluster/hosts.yaml file, you will notice that it created two master nodes, which we require three, add missing one properly to that list as shown below.\nall: hosts: master1: ansible_host: 10.0.128.149 ip: 10.0.128.149 access_ip: 10.0.128.149 master2: ansible_host: 10.0.128.171 ip: 10.0.128.171 access_ip: 10.0.128.171 master3: ansible_host: 10.0.128.184 ip: 10.0.128.184 access_ip: 10.0.128.184 worker1: ansible_host: 10.0.128.81 ip: 10.0.128.81 access_ip: 10.0.128.81 worker2: ansible_host: 10.0.128.137 ip: 10.0.128.137 access_ip: 10.0.128.137 worker3: ansible_host: 10.0.128.156 ip: 10.0.128.156 access_ip: 10.0.128.156 children: kube-master: hosts: master1: master2: master3: kube-node: hosts: master1: master2: master3: worker1: worker2: worker3: etcd: hosts: master1: master2: master3: k8s-cluster: children: kube-master: kube-node: calico-rr: hosts: {} Once it is done, the other thing which should be modified to use external load balancer HAProxy, is all.yaml file located under inventory/mycluster/group_vars/all/.\nall.yml is general configuration file which specifies main configurations of your cluster, it uses Nginx load balancer by default which means that each worker node has its own local nginx load balancer as given second figure above. If not specified anything else.\n Disable default load balancer  $ vim inventory/mycluster/group_vars/all/all.yml loadbalancer_apiserver_localhost: false  Add external load balancer HAProxy.  $ vim inventory/mycluster/group_vars/all/all.yml ## External LB example config apiserver_loadbalancer_domain_name: \u0026#34;\u0026lt;domain-name-of-lb\u0026gt;\u0026#34; loadbalancer_apiserver: address: 10.0.128.193 port: 8383  Initialize cluster deployment  # under kubespray/ directoy  $ ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml It will take around 10-15 minutes which depens on your cluster and if everything goes well, at the end of deployment through Ansible you will not face with any problem. If so, you can test it by SSH to master node and try kubectl cluster-info.\n$ kubectl cluster-info Kubernetes master is running at ..... To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. It means that Kubernetes cluster with three master and three worker nodes available to use.\nNote that the default configuration of cluster could be changed more however before attempting to change default configuration, make sure that you did correct research on what to change on KubeSpray default settings. Otherwise, there might be problems regarding to customized configuration settings.\nFor more information stay updated and watch KubeSpray regarding to issues, pitfalls and more.\nLast step for this post is creating kubectl configuration for your personal/work computer to access the cluster. Install kubectl on your environment. Afterwards copy configuration from master node to your ~/.kube/ as config.\nSince we have only one endpoint, configuration file should be copied to HAProxy Server then your computer, through rsync or scp\n On HAProxy Server  $ scp root@master1:/etc/kubernetes/admin.conf config # will copy admin.conf as config  $ cp config /home/ubuntu/ # copy to a user home dir $ chown ubuntu:ubuntu /home/ubuntu/config # change owner of the file   On your personal/work computer  $ scp -i ~/.ssh/haproxy.pem ubuntu@\u0026lt;ha-proxy-ip\u0026gt;:/home/ubuntu/config ~/.kube/ Now, you should be able to get and dump your cluster information as in master nodes.\n$ kubectl cluster-info Kubernetes master is running at ..... To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. There are lots of configurations and different settings regarding to Kubernetes cluster environment and generally using Cloud Provider solutions are less painful or painless. However, sometimes it is less costly to setup your own environment and having full access to anything could be better for learning under the hood things or creating highly customized environments. It really depends on your situation therefore it is up to you to go and setup your own Kubernetes cluster or use it as service from cloud providers.\nBy the way, thanks for giving time to checkout the post üòâ\n","permalink":"https://mrturkmen.com/posts/install-ha-kubernetes-cluster/","summary":"The main purpose of this blog post a simple walkthrough of setting up Kubernetes cluster with external HAProxy which will be the endpoint where our kubectl client communicates over. Node specifications for this setup is given as shown in the table below. Keep in mind that all of them has access to each other with password and without password. The environment which Kubernetes cluster will stay is running on OpenStack. It means that once a configuration (ssh keys, hosts, and etc) is done for example master 1 then all other nodes could be initialized through snapshot of master 1.","title":"haproxy: setting it up for highly available k8s cluster"},{"content":"VPN Kuralƒ±m Bug√ºn sizlere kendinize ait VPN sistemi nasƒ±l kurulur, onu anlatmak istiyorum, daha √∂nce ƒ∞ngilizce olarak, yayƒ±nladƒ±m fakat T√ºrk√ße bir kaynaƒüƒ±n da faydalƒ± olabileceƒüini d√º≈ü√ºnd√ºm. Burada anlatƒ±lanlar, ubuntu ailesine (16.04,18.04) ait sunucular √ºzerinde test edilmi≈ütir.\nƒ∞lk olarak bulut hizmeti saƒülayan bir ≈üirketten bu DigitalOcean, Google Cloud, Microsoft Azure veya Amazon olabilir, sunucu kiralƒ±yorsunuz, en ucuzu ve makul olanƒ± DigitalOcean tarafƒ±ndan sunulan aylƒ±k 5 dolar olan sunucu diyebilirim. Sunucuyu kiraladƒ±ktan ve ssh baƒülantƒ±sƒ±nƒ± saƒüladƒ±ktan sonra VPN kurulumuna ge√ßebiliriz.\nVPN hakkƒ±nda tam bilgisi olmayan arkada≈ülar i√ßin ≈üu ≈üekilde √∂zetlenebilir, sizin i√ßin olu≈üturulmu≈ü sanal bir baƒülantƒ± noktasƒ± gibi d√º≈ü√ºnebilirsiniz. Yani VPN\u0026rsquo;e baƒülandƒ±ktan sonra bilgisayarƒ±nƒ±zdan √ßƒ±kan ve bilgisayarƒ±nƒ±za gelen aƒü trafik ≈üifrelenmi≈ü olarak i≈ülenir. √ú√ß√ºnc√º parti yazƒ±lƒ±mlarƒ±n veya MITM gibi saldƒ±rƒ±larƒ±n √∂n√ºne ge√ßmi≈ü olursunuz.\nNeden kendi VPN sistemi kurmalƒ±yƒ±z ? √á√ºnk√º ≈üu anda var olan b√ºt√ºn VPN sistemleri, √ºcretsiz olarak hizmet saƒülasa dahi, sizin bilgilerinizin satƒ±lmasƒ±, ar≈üivlenmesi ve gerektiƒüinde ilgili birimlere aktarƒ±lmasƒ± amacƒ±yla kaydedilmektedir. Bunun ne gibi zararlarƒ± olabilir gelin birlikte ≈ü√∂yle bir sƒ±ralayalƒ±m:\n Oltalama saldƒ±rƒ±larƒ±na sadece sizin bilebileceƒüiniz bilgiler ile maruz kalma. Ziyaret ettiƒüiniz siteler tarafƒ±ndan reklam bombardƒ±manƒ±na maruz kalma. Ki≈üisel bilgilerinizin reklam veren ajanslara satƒ±lmasƒ±, bu durum bir√ßok ki≈üi tarafƒ±ndan tam olarak anla≈üƒ±lamƒ±yor, yani ≈üu ≈üekilde anla≈üƒ±lamƒ±yor, internet √ºzerinden alƒ±≈üveri≈ü yapan A ki≈üisi, kendine ait bilgilerin, onun bilgilerini satacak ki≈üiler tarafƒ±ndan deƒüersiz olduƒüuna inanƒ±yor ve hi√ßbir gizlilik saƒülamadan internet kullanƒ±mƒ±na devam ediyor. Bu sonunda o ki≈üiye zarar vermese bile o ki≈üinin konu≈ütuƒüu, g√∂r√º≈üt√ºƒü√º veya birlikte √ßalƒ±≈ütƒ±ƒüƒ± arkada≈ülara zarar verebiliyor.  Burada sƒ±ralananlar sadece buzdaƒüƒ±nƒ±n g√∂r√ºnen ucu bile diyemeyiz, g√ºn√ºm√ºzde veri i≈üleme teknikleri ve yakla≈üƒ±mlarƒ± √∂yle geli≈ümi≈ütir ki siz bile kendinize ait olan bir ≈üeyin varlƒ±ƒüƒ±na farkƒ±nda olmadan onlar i≈ülemleri tamamlamƒ±≈ü oluyor :).\nBu ve bunlardan √ßok daha fazla nedenden dolayƒ± VPN kullanƒ±mƒ± ≈üart diyebilirim. Peki bunu nasƒ±l yapacaƒüƒ±z, bu kƒ±sƒ±mdan sonra sizin bir bulut saƒülayƒ±cƒ±sƒ± tarafƒ±ndan sunucunu kiraladƒ±ƒüƒ±nƒ±zƒ± ve ssh baƒülantƒ±sƒ±nƒ± saƒüladƒ±ƒüƒ±nƒ±zƒ± varsayƒ±yorum.\nBu g√∂nderide WireGuard VPN uygulamasƒ± kullanƒ±lacaktƒ±r. WireGuard VPN uygulamasƒ± a√ßƒ±k kaynaklƒ± bir uygulama olup, saƒüladƒ±ƒüƒ± imkanlar sayesinde diƒüer VPN uygulamalarƒ±na (OpenVPN ve diƒüerleri) kƒ±yasla √ßok daha hƒ±zlƒ± ve g√ºvenilirdir.\nSunucu Ayarlarƒ± VPN uygulamasƒ±nƒ± kiraladƒ±ƒüƒ±mƒ±z sunucu √ºzerine kuralƒ±m.\n$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y $ sudo add-apt-repository ppa:wireguard/wireguard $ sudo apt-get update $ sudo apt-get install wireguard Uygulamayƒ± √ßekirdek g√ºncellemeleri ile birlikte g√ºncellemek i√ßin gerekli komutu girelim.\n$ sudo modprobe wireguard A≈üaƒüƒ±da verilen komut girildiƒüinde beklenen sonu√ß.\n$ lsmod | grep wireguard wireguard 217088 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard Anahtarlarƒ± √ºretelim\n$ cd /etc/wireguard $ umask 077 $ wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey VPN Konfigurasyon dosyasƒ±nƒ± /etc/wireguard/wg0.conf ayarlayalƒ±m.\n[Interface] PrivateKey = \u0026lt;daha-√∂ncesinde-√ºretilen-gizli-anahtar\u0026gt; Address = 10.120.120.2/24 Address = fd86:ea04:1111::1/64 SaveConfig = true PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o ens3 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o ens3 -j MASQUERADE; ip6tables -D FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -D POSTROUTING -o ens3 -j MASQUERADE ListenPort = 51820 Burada √∂nemli nokta ens3, ip tables komutu i√ßerisinde yer alan en3, sunucudan sunucuya farklƒ±lƒ±k g√∂sterebilir, bundan dolayƒ± sizin sunucunuzda ne ise aƒü kartƒ±nƒ±n ismi onu girmelisiniz. ifconfig  komutu sayesinde √∂ƒürenilebilir.\nBir diƒüer √∂nemli nokta ise daha √∂ncesinde 4. adƒ±mda √ºretilen privatekey, i√ßeriƒüinin PrivateKey alanƒ±na girilmesidir.\nAƒü trafiƒüini y√∂nlendirme\n/etc/sysctl.conf dosyasƒ± i√ßerisine a≈üaƒüƒ±da verilen bilgileri girerek kaydediniz.\nnet.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 Bilgiler gerekli dosyaya kaydedildikten sonra a≈üaƒüƒ±daki komutlar sƒ±rasƒ± ile girilmelidir.\n$ sysctl -p $ wg-quick up wg0 Komutlarƒ±n girilmesi ve herhangi bir sorun g√∂r√ºlmemesi durumda ve wg komutu terminale girildikten sonra a≈üaƒüƒ±da verilen √ßƒ±ktƒ±ya benzer bir √ßƒ±ktƒ± g√∂receksiniz.\n$ wg interface: wg0 public key: loZviZQpT5Sy4gFKEbk6Vc/rcJ3bH84L7TUj4qMB918= private key: (hidden) listening port: 51820 Eƒüer herhangi bir sorun ile kar≈üƒ±la≈ümazsanƒ±z bu adƒ±ma kadar, bu demek oluyor ki, sunucu tarafƒ±nda i≈üiniz ≈üimdilik tamamlandƒ± geriye sadece kendi bilgisayarƒ±mƒ±zƒ±, telefonumuzu vs VPN sunucusuna baƒülamak kaldƒ±.\nKullanƒ±cƒ± Ayarlarƒ± Kullanƒ±cƒ±larƒ±n kendi bilgisayar ortamlarƒ±nda, telefonlarƒ±nda, tabletlerinde veya diƒüer sunucularƒ±nda kullanabileceƒüi uygulamalarƒ± buradan indirebilirsiniz.\nGerekli uygulamayƒ± kendi ortamƒ±nƒ±za indirdikten sonra tek yapmanƒ±z gereken, VPN sunucusu tarafƒ±nda ayarladƒ±ƒüƒ±mƒ±z VPN\u0026rsquo;e baƒülanmak, bunun i√ßin gerekli olan sadece konfigurasyonlarƒ± doƒüru girmek olacaktƒ±r.\nKullanƒ±cƒ± tarafƒ±nda, uygulama √ºzerinden a≈üaƒüƒ±da verilen konfigurasyona benzer bir ayarƒ± (kendi kurduƒüunuz VPN ayarlarƒ±na g√∂re privatekey ve ip adressi deƒüi≈üiklik g√∂sterecektir.) ayarlamanƒ±z gerekmektedir.\n[Interface] Address = 10.120.120.2/32 Address = fd86:ea04:1111::2/128 # note that privatekey value is just a place holder PrivateKey = KIaLGPDJo6C1g891+swzfy4LkwQofR2q82pFR6BW9VM= DNS = 1.1.1.1 [Peer] PublicKey = \u0026lt;sunucunuza-ait-public-anahtar\u0026gt; Endpoint = \u0026lt;sunucunuzun-dƒ±≈ü-ip-adresi\u0026gt;:51820 AllowedIPs = 0.0.0.0/0, ::/0 Gerekli i≈ülemler kullanƒ±cƒ± tarafƒ±nda da saƒülandƒ±ktan sonra, sunucu tarafƒ±nda bu kullanƒ±cƒ±ya baƒülantƒ± izni vermek kalƒ±yor, onuda a≈üaƒüƒ±da verilen komut ile saƒülayabilirsiniz.\n$ wg set wg0 peer \u0026lt;kullanici-public-anahtari\u0026gt; allowed-ips 10.120.120.2/32,fd86:ea04:1111::2/128 Sunucu tarafindan kullanicinin VPN baglantisi saƒüladƒ±ƒüƒ±nƒ± a≈üaƒüƒ±da verilen komut ile teyit edebilirsiniz.\n$ wg interface: wg0 public key: loZviZQpT5Sy4gFKEbk6Vc/rcJ3bH84L7TUj4qMB918= private key: (hidden) listening port: 51820 peer: Ta9esbl7yvQJA/rMt5NqS25I/oeuTKbFHJu7oV5dbA4= allowed ips: 10.120.120.2/32, fd86:ea04:1111::2/128 Daha sonrasinda, wireguard tarafƒ±ndan olu≈üturulan aƒü kartƒ±nƒ± aktivate edelim.\n$ wg-quick up wg0 G√ºvenlik Duvarƒ± ayarlarƒ± Bazen sunucu tarafƒ±nda yapmanƒ±z gereken bazƒ± g√ºvenlik duvarƒ± ayarlarƒ± bulunmakta, bunlar VPN baƒülantƒ±sƒ±nƒ± ba≈üarƒ±lƒ± bir ≈üekilde saƒülamanƒ±z i√ßin kritik √∂neme sahiptir.\n$ ufw enable VPN uygulamasƒ±na baƒülanmamƒ±zƒ± saƒülayacak portu a√ßƒ±yoruz.\n$ ufw allow 51820/udp IP tablolarƒ± ile 51820 portu i√ßin bazƒ± ayarlamalar yapƒ±yoruz.\n$ iptables -A INPUT -p udp -m udp --dport 51820 -j ACCEPT $ iptables -A OUTPUT -p udp -m udp --sport 51820 -j ACCEPT Burada √∂nemli olan kƒ±sƒ±mlardan biriside b√ºt√ºn komutlar ROOT, yani y√∂netici yetkisi ile yapƒ±lmalƒ±, aksi takdirde hata verecektir.\nBu noktadan sonra, bilgisayarƒ±nƒ±za, tabletinize veya telefonunuza kurduƒüunuz WireGuard uygulamasƒ± sayesinde sorunsuz ve g√ºvenlikli bir ≈üekilde internetinizi kullanabilirsiniz.\n","permalink":"https://mrturkmen.com/posts/vpn-kuralim/","summary":"VPN Kuralƒ±m Bug√ºn sizlere kendinize ait VPN sistemi nasƒ±l kurulur, onu anlatmak istiyorum, daha √∂nce ƒ∞ngilizce olarak, yayƒ±nladƒ±m fakat T√ºrk√ße bir kaynaƒüƒ±n da faydalƒ± olabileceƒüini d√º≈ü√ºnd√ºm. Burada anlatƒ±lanlar, ubuntu ailesine (16.04,18.04) ait sunucular √ºzerinde test edilmi≈ütir.\nƒ∞lk olarak bulut hizmeti saƒülayan bir ≈üirketten bu DigitalOcean, Google Cloud, Microsoft Azure veya Amazon olabilir, sunucu kiralƒ±yorsunuz, en ucuzu ve makul olanƒ± DigitalOcean tarafƒ±ndan sunulan aylƒ±k 5 dolar olan sunucu diyebilirim. Sunucuyu kiraladƒ±ktan ve ssh baƒülantƒ±sƒ±nƒ± saƒüladƒ±ktan sonra VPN kurulumuna ge√ßebiliriz.","title":"wireguard: kendimize √∂zel vpn kurulumu "},{"content":"Concurrency in Go Concurrency in Go, makes Go programming language very unique and attractive compared to other languages, in this section I am going to share the notes which I took when I was watching coursera video series.\nIf you did not already check previous post on Go, it could be helpful to check it out first.\n Go Notes (OOP)  Specialization serie is Programming with Google Go Keep in mind that the notes are taken from several resources mainly from the course, however post may include other resources as well, I have referenced them when required, if nothing is referenced then it means, notes are taken from the course.\nParalel Execution  Two programs execute in paralel if they execute at exactly the same time At time t, an instruction is being performed for both P1 and P2.  Why use parallel execution  Tasks may complete more quickly Example: Two Piles of dishes to wash  Two dishwashers can complete twice as fast as one   Some tasks must be performed sequentially  Example: Wash dish, dry dish  Must wash before you can dry     Some tasks are parallelizable and some are not  Von Neumann Bottleneck Speedup without Parallelism  Can we achieve speedup without Parallelism ? Design faster processors  Get speedup without changing software   Design processor with more memory  Reduces the Von Heumann bottleneck Cache access time=1 clock cycle Main memory access time = ~100 clock cycles Increasing on-chip cache improves performance    Moore\u0026rsquo;s Law  Predicted that transistor density would double every two years Smaller transistors switch faster Not a physical law, just an observation Exponential increase in density would lead to exponential increase in speed  Power Wall Power / Temperature Problem  Transistors consume power when they switch Increasing transistor density leads to increased power consumption  Smaller transistors use less power, but density scaling is much faster   High power leads to high temperature Air cooling (fans) can only remove so much heat  Dynamic Power  P = a * CFV^2 a is percent of time switching C is capacitance (related to size) F is the clock frequency V is voltage swing (from low to high) Voltage is important 0 to 5V uses much more power then 0 to 1.3V  Dennard Scaling  Voltage should scale with transistor size Keeps power consumption and temperature, low Problem: Voltage can\u0026rsquo;t go too low  Must stay above threshold voltage Noise problems occur   Problem: Does not consider leakage power Dennard scaling must stop  Multi-Core Systems  P = a * CFV^2 Cannot increase frequency Can still add processor cores, without increasing frequency  Trend is apparent today   Parallel execution is needed to exploit multi-core systems Code made to execute on multiple cores Different programs on different cores  Concurrent vs Parallel Concurrent Execution  Concurrent execution is not necessarily the some as parallel execution Concurrent: start and end times overlap Parallel: execute at exactly the same time   Parallel tasks must be executed on different hardware Concurrent tasks may be executed on the same hardware  Only one task actually executed at a time   Mapping from tasks to hardware is not directly controlled by programmer  At least not in go    Concurrent Programming  Programmer determines which tasks can be executed in parallel Mapping tasks to hardware  Operating system Go runtime scheduler    Hiding Latency *(Crucial)   Concurrency improves performance even without parallelism\n  Tasks must periodically wait for something\n i.e. wait for memory X = Y+Z read Y,Z from memory May wait 100+ clock cycles    Other concurrent tasks can operate while one task is waiting\n  Concurrent programming is useful even no paralelism\n  Hardware Mapping  Programmer does not determine the hardware mapping Programmer makes parallelism possible Hardware mapping depends on many factors  Where is the data ? What are the communication costs ?    Concurrency Basics Processes  An instance of a running program Things unique to a process   Memory   a. Virtual address space b. Code, stack, heap, shared libraries  Registers   a. Program counter  Operating System  Allows many processes to execute concurrently Processes are switched quickly  20ms   User has the impression of parallelism Operating system must give processes fair access to resources  Scheduling Processes  Operating system schedules processes for execution Gives the illusion of parallel execution   OS gives fair access to CPU, memory , etc.  Context Switch  Control flow changes from one process to another   Process \u0026ldquo;context\u0026rdquo; must be swapped Air cooling (fans) can only remove so much heat  Threads and Goroutines Threads vs. Processes  Threads share some context Many threads can exist in one process   OS schedules threads rather than processes  Goroutines  Like a thread in Go Many Goroutines execute within a single OS thread  We can call Goroutines as lightweight threads in Go.\nGo Runtime Scheduler  Schedules goroutines inside an OS thread Like a little OS inside a single OS thread   Logical processor is mapped to a thread  Interleavings  Order of execution within a task is known   Order of execution between concurrent tasks is unknown Interleaving of instructions between tasks is unknown  Possible Interleavings  Many interleavings are possible Must consider all possibilities   Ordering is non-deterministic Interleavings occurs on machine code level.  Race Conditions It is happenning when multiple goroutines try to write/read from a source at the same time, could be prevented using mutex\n Outcome depends on non-deterministic ordering   Races occur due to communication  Communication Between Tasks  Threads are largely independent but not completely independent Web server, one thread per client.   Make threads for each client who are connecting to web server Image processing, 1 thread per pixel block   Some level of communication can occur between threads, let\u0026rsquo;s say we want to blur the image accordingly, then the threads which are working on neighborhood pixels should communicate with each other.  Goroutines Goroutines are new way of threading in lightweight approach.\nCreating a Goroutine  One goroutine is created automatically to execute the main() Other goroutines are created using the go keyword  package main func main () { a=1\tfoo()\ta=2 } func foo() { // does something ...  }  Main goroutine blocks on call to foo()  package main func main () { a=1\tgo foo()\ta=2 } func foo() { // does something ...  }  New goroutine created for foo() Main goroutine does not block  Exiting a Goroutine  A goroutine exits when its code is complete When the main goroutine is complete, all other goroutines exit A goroutine may not complete its execution because main completes early  Early Exit func main() { go fmt.Printf(\u0026#34;New routine\u0026#34;) fmt.Printf(\u0026#34;Main routine\u0026#34;) }  Only \u0026ldquo;Main routine\u0026rdquo; is printed Main finished before the new goroutine started  Delayed Exit func main() { go fmt.Printf(\u0026#34;New routine\u0026#34;) time.Sleep(100 * time.Milisecond) fmt.Printf(\u0026#34;Main routine\u0026#34;) }  Add a delay in the main routine to give the new routine a chance to complete \u0026ldquo;New RouteMainRoutine \u0026quot; is now printed  Timing with Goroutines  Adding a delay to wait for a goroutine is bad !! Timing assumptions may be wrong  Assumption: delay of 100 ms will ensure that goroutine has time to execute Maybe the OS schedules another thread Maybe the Go runtime schedules another goroutine   Timing is nondeterministic Need formal synchronization constructs  Synchronization  Using global events whose execution is viewed by all threads, simultaneously Want print to occur after update of x  Example\n   Task 1 Task 2     x=1    x=x+1    GB if GB print x     GLOBAL EVENT (GB) is viewed by all tasks at the same time Print must occur after update of x Synchronization is used to restrict bad interleavings  Threads in Go Threads in Go are generally handled using wait groups\nSync WaitGroup  Sync package contains functions to synchronize between goroutines sync.WaitGroup forces a goroutine to wait for other goroutines Contains an internal counter  Increment counter for each goroutine to wait for Decrement counter when each goroutine Waiting goroutine cannot continue until counter is 0    Using WaitGroup  Add() increments the counter Done() decrements the counter Wait() blocks until counter == 0  Example func foo(wg *sync.WaitGroup) { fmt.Printf(\u0026#34;New routine\u0026#34;) wg.Done() } func main() { var wg sync.WaitGroup wg.Add(1) go foo(\u0026amp;wg) wg.Wait() fmt.Printf(\u0026#34;Main Routine\u0026#34;) } Goroutine Communication  Goroutines usually work together to perform a bigger task Often need to send data to collaborate Example : Find the product of 4 integers  Make 2 goroutines, each multiplines a pair Main goroutine multiplies the 2 results   Need to send ints from main routine to the two sub-routines Need to send results from sub-routines back to main routine  Channels  Channels are used to make communication between goroutines Channels are typed Use make() to create a channel  c:=make(chan int)   Send and receive data using the \u0026lt;- operator Send data on a channel  C \u0026lt; - 3    Receive data from a channel x:= \u0026lt;- c  Example func prod(v1 int, v2 int, c chan int) { c \u0026lt;- v1*v2 } func main() { c:=make(chan int) go prod(1,2,c) go prod(3,4,c) a:= \u0026lt;-c b:= \u0026lt;-c fmt.Println(a*b) } Unbuffered Channel  Unbuffered channels cannot hold data in transit  Default is unbuffered   Sending blocks until data is received Receiving blocks until data is sent  Blocking and Synchronization  Channel communication is synchronous Blocking is the same as waiting for communication Receiving and ignoring the result is same as a Wait()  Buffered Channel Channel Capacity  Channel can contain a limited number of objects Default size 0 (unbuffered) Capacity is the number of objects, it can hold in transit Optional argument to make() defines channel capacity  c:=make(chan int, 3)    Sending only blocks if buffer is full Receiving only blocks if buffer is empty  Channel Blocking, Receive  Channel with capacity 1   First receive blocks until send occurs Second receive blocks forever  Channel Blocking, Send  Second send blocks until receive is done Receive can block until first send is done  Use of Buffering  Sender and receiver do not need to operate at exactly the same speed  Adding a channel to our goroutine Note that in this section notes are taken from: https://www.sohamkamani.com/blog/2017/08/24/golang-channels-explained\n A channel gives us a way to \u0026ldquo;connect \u0026quot; the different concurrent parts of our program. Channels can be thought of as \u0026ldquo;pipes\u0026rdquo; or \u0026ldquo;arteries\u0026rdquo; that connect the different concurrent parts of our code  Directionality out chan \u0026lt;- int `  The chan\u0026lt;- declaration tells us that you can only put stuff into the channel, but not receive anything from it. The int declaration tells us that the \u0026ldquo;stuff\u0026rdquo; you put into the channel can only be of the int datatype Although they look like seperate parts, chan\u0026lt;-int can be thought of as one datatype, that describes a \u0026ldquo;send-only\u0026rdquo; channel of integers. Similarly, an example of a \u0026ldquo;receive-only\u0026rdquo; channel declaration would look like:  out \u0026lt;- chan int   A channel can be declared without giving any directionatility, which means it can send or receive data.\n  Bi-directional channel can be created using following \u0026ldquo;make\u0026rdquo; statement\n  out :=make(chan int) After this section, notes are taken from: https://go101.org/article/channel.html if you prefer to have deep dive into channels, you may visit there.\n  Concurrent computations may share resources, generally memory resource. There are some circumstances may happen in a concurrent computing.\n  In the same period of one computation is writing data to a memory segment, another computation is reading data from the same memory segment. Then the integrity of the data read by the other computation might be not preserved.\n  In the same period of one computation is writing data to a memory segment, another computation is also writing data to the same memory segment. Then the integrity of the data stored at the memory segment might be not preserved.\n  These circumstances are called data races. One of the duties in concurrent programming is to control resource sharing among concurrent applications, so that data races will NOT happen. The ways to achieve this duty are called concurrency synchronization, or data synchronization. GO supports several data synchronization techniques. The following section will introduce one of them, channel.\nOther duties in concurrent programming include:\n Determine how many computations are needed Determine when to start, block, unblock and end a computation. Determine how to distribute workload among concurrent computations.  Most operations in Go are not synchronized. In other words, they are not concurrency-safe. These operations include value assignments, argument passing and container element manipulations. There are only a few operations which are synchronized, including the several to be introduced channel operations below.\nDO NOT COMMUNICATE BY SHARING MEMORY, SHARE MEMORY BY COMMUNICATING *(THROUGH CHANNELS)\nChannel Types and Values Like array, slice and map, each channel type has an element type. A channel can only transfer values of the element type of (the type of) the channel.\nChannel types can be bidirectional or single-directional. Assume T is an arbitrary type,\n chan T denotes a bidirectional channel type. Compilers allow both receiving values from and sending values to bidirectional channels. chan \u0026lt;-T denotes a send-only channel type. Compilers do not allow receiving values from send-only channels. \u0026lt;- chan T denotes a receive-only channel type. Compilers do not allow sending values to receive-only channels.  T is called element types of these channel types.\nValues of bidirectional channel type chan T can be implicitly converted to both send-only type chan \u0026lt;-T and receive-only type \u0026lt;-chan T , but not vice versa (even if explicitly). Values of send only type chan\u0026lt;-T cannot be converted to receive only type \u0026lt;-chan T.\nEach channel has capacity. A channel value with a zero capacity is called unbuffered channel and a channel value with a non-zero capacity is called buffered channel.\nFor more detailed explanation on channels you can visit https://go101.org/article/channel.html\nTake care ! üëãüèª\nMaybe next time, topics could be revisited by examples üòâ\n","permalink":"https://mrturkmen.com/posts/go-concur/","summary":"Concurrency in Go Concurrency in Go, makes Go programming language very unique and attractive compared to other languages, in this section I am going to share the notes which I took when I was watching coursera video series.\nIf you did not already check previous post on Go, it could be helpful to check it out first.\n Go Notes (OOP)  Specialization serie is Programming with Google Go Keep in mind that the notes are taken from several resources mainly from the course, however post may include other resources as well, I have referenced them when required, if nothing is referenced then it means, notes are taken from the course.","title":"go: concurrency notes "},{"content":"In this post, I would like to share the notes that I took when I was completing following Go series education on Coursera. I can recommend it for anyone who would like to get rapid introduction to Go programming.\nProgramming with Google Go\nI should admit that although it looks fancy, nothing can be compared to actual development and contribution to open source projects. The course itself is quite interesting and contains very handy exercises regarding to Go development mentality. I would recommend it for anyone who has some interest in Go development and do not know where to start.\nThere will be following headers and subheaders which contains some notes on Go Programming language as bulletpoints.\nOOP in Go In Go, there is no such a concept of object oriented programming as in Java, however, it is possible to imply similar approach (object oriented approach) in Go using interfaces and structs.\nClasses There is no \u0026ldquo;Class\u0026rdquo; keyword in Go.\n Collection of data fields and functions that share a well-defined responsibility  Example: Point class Used in a geometry program Data: x coordinate, y coordinate Funtions:  DistToOrigin(), Quadrant() AddXOffSet(), AddYOffset()   Classes are template Contain data fields, not data    Classes are supported with structs in Go.\ntype Point struct { X float64 Y float64 }  Structs with methods, structs and methods together allow arbitrary data and functions to be composed.  func (p Point) DistToOrig() { t:= math.Pow(p.x,2) + math.Pow(p.y,2) return math.Sqrt(t) } func main() { p1 := Point(3,4) fmt.Println(p1.DistToOrig()) } Objects  Instance of a class Contains real data Example: Point Class  Encapsulation  Data can be protected from the programmer Data can be accessed only using methods Maybe we do not trust the programmer to keep data consistent Example: Double distance to origin  Option 1: Make method DoubleDist Option 2: Trust programmer to double X and Y directly    Encapsulation is supported as following ;\nCreate a package called data which has exported function which is PrintX, since it starts with capital letter, in Go, when something starts with capital letter, it means that it is exported\npackage data var x int=1 func PrintX() { fmt.Println(x) } Main package which is starting point of any application in Go.\npackage main func main() { data.PrintX() } Controlling access to Structs\n Hide fields of structs by starting field name with a lower-case letter. Define public methods which access hidden data  package data type Point struct{ x float64 y float64 } func (p *Point) InitMe(xn,xy float64) { p.x =xn p.y =xy } func (p *Point) Scale(v float64) { p.x=p.x*v p.y=p.y*v } func (p *Point) PrintMe() { fmt.Println(p.x,p.y) } Note that all methods are public ! Since their initial character is capital letter.\npackage main func main() { var p data.Point p.InitMe(3,4) p.Scale(2) p.PrintMe() }  Access to hidden fields can only be possible through public access.  Limitation of Method   Receiver is passed implicitly as an argument to the method\n  Method cannot modify the data inside the receiver\n  Example: OffsetX() should increase x coordinate\n  package main func main(){ p1:=Point(3,4) p1.OffsetX(5) } Large Receivers\n If receiver is large, lots of copying is required  type Image [100] [100]int func main() { i1 := GrabImage() i1.BlurImage() } 10.000 ints copied to BlurImage() (Pitfalls)\nPointer Receivers\nfunc (p *Point) OffsetX (v float64) { p.x=p.x+v }  Receiver can be a pointer to a type Call by reference, pointer is passed to the method  Point Receivers, Referenceing, Dereferencing  No need to dereference  func (p *Point) OffsetX (v int) { p.x=p.x+v }   Point is referenced as p, not *p\n  No need to reference\n  package main func main() { p:=Point{3,4} p.OffsetX(5) fmt.Println(p.x) }  Do not need to reference when calling the method  Good Programming Practices\n All methods for type have pointer receivers or All methods for a type have non-pointer receivers Mixing pointer/non-pointer receivers for a type will get confusing !  Pointer receiver allows modification    Polymorphism  Ability for an object to have different forms depending on the context Example Area() function  Rectangle area is base * height Triangle area is 0.5*base*height   Identical at a high level of abstraction Different at a low level of abstraction  Inheritance (No Inheritance in GoLang)   Sublcass inherits the methods/data of the superclass\n  Example: Speaker superclass\n Speak() method, pring \u0026quot;\u0026lt;noise\u0026gt; \u0026quot;     Subclassses Cat and Dog\n Also have the Speak() method    Cat and Dog are different forms of speaker\n  Remember: Go does not have inheritance\n  Overriding   Subclass redefines a method inherited from the superclass\n  Example: Speaker, Cat, Dog\n Spekaer Speak() prints \u0026ldquo;\u0026rdquo; Cat Speak() prints \u0026ldquo;meow\u0026rdquo; Dog Speak() prints \u0026ldquo;woof\u0026rdquo;    Speak() is polymorphic\n Different implementations for each class Same signature (name, params and return)    Interface   Set of method signatures\n Name, parameters, return values Implementation is NOT defined    Used to express conceptial similarity between types.\n  Example : Shape2D interface\n  All 2D shapes must have Area() and Perimeter()\n  Satisfying an Interface  Type satisfies an interface if type defines all methods specified in the interface.  Same method signatures   Rectangle and Triangle types satisfy the Shape2D interface  Must have Area() and Perimeter() methods Additional methods are OK.   Similar to inheritance with overriding.  Example type Shape2D interface { Area() float64 Perimeter() float64 } type Triangle {...} func (t Triangle) Area() float64 {....} func (t Triangle) Perimeter() float64 {....}  Triangle type satisfies the Shape2D interface No need to state it explicitly  Interface vs Concrete Types Concrete Types  Specify the exact representation of the data and methods Complete method implementation is included  Interface Types  Specifies some method signatures Implementations are abstracted  Interface Values  Can be treated like other values  Assigned to variables Passed, returned   Interface values have two components   Dynamic Type : Concrete type which it is assigned to Dynamic Value: Value of the dynamic type  Defining an interface type type Speaker interface { Speak()} type Dog struct {name string } func (d Dog) Speak() { fmt.Println(d.name) } func main() { var s1 Speaker var d1 Dog{\u0026#34;Brian\u0026#34;} s1=d1 s1.Speak()   Dynamic type is Dog and dynamic value is d1.\n  An interface can have a nil dynamic value\n  var s1 Speaker var d1 *Dog s1=d1  d1 has no concrete value yet s1 has a dynamic type but no dynamic value  Nil Dynamic Value  Can still call the Speak() method of s1 Does not need a dynamic value to call Need to check inside the method  func (d *Dog)Speak() { if d==nil{ fmt.Println(\u0026#34;\u0026lt;noise\u0026gt;\u0026#34;) }else{ fmt.Println(d.name) }\t} var s1 Speaker var d1 *Dog s1=d1 s1.Speak() // it works, since s1 is mapped to d1 Nil Interface Value  Interface with nil dynamic type Very different from an interface with a nill dynamic value  Nil dynamic value and valid dynamic type\nvar s1 Speaker var d1 *Dog s1=d1  Can call a method since type is known, Nil dynamic type  var s1 Speaker *(there is no actually method to call)  Cannot call a method, runtime error  No dynamic type and no dynamic value then you cannot call the interface‚Ä¶\nUsing Interfaces  Need a function which takes multiple types of parameter Function foo() parameter  Type X or Type Y   Define interface Z foo() parameter is interface Z Types X and Y satisfy Z Interface methods must be those needed by foo()  Example Interface for Shapes Pool in a Yard\n I need to put a pool in my yard Pool need to fit in my yard  Total area must be limited   Pool needs to be fenced  Total perimeters must be limited   Need to determine if a pool shape satisfies criteria FitInYard()  Takes a shape as argument Returns true if the shape satisfies criteria   FitInYard()  Many Possible shape types  Rectangle, triangle, circle     FitInYards() should take many shape types Valid shape types must have  Area() Perimeter()   Any shape with these methods is OK.  type Shape2D interface { Area() float64 Perimeter() float64 } type Triangle {...} func (t Triangle) Area() float64 {...} func (t Triangle) Perimeter() float64 {...} type Rectangle {...} func(t Rectangle) Area() float64 {...} func (t Rectangle) Perimeter() float64 {....}   Rectangle and Triangle satisfy Shape2D interface.\n  FitInYard() Implementation\n  func FitInYard(s Shape2D) bool { if (s.Area() \u0026gt; 100 \u0026amp;\u0026amp; s.Perimeter() \u0026gt; 100) { return true } return false } Empty Interface  Empty interface specifies no methods All types satisfy the empty interface Use it to have a function accept any type as a parameter  func PrintMe(val interface{} ) { fmt.Println(val) } Type Assertions Concealing Type Differences__  Interfaces hide the differences between types  func fitInYard(s Shape2D)bool { if (s.Area() \u0026gt;100 \u0026amp;\u0026amp; s.Perimeter()\u0026gt;100){ return true } return false }  Sometimes you need to treat different types in different ways  Exposing Type Differences   Example: Graphics program\n  DrawShape() will draw any shape\nfunc DrawShape (s Shape2D) {..... }   Underlying API has different drawing functions for each shape\nfunc DrawRect (r Rectangle) {.... func DrawTriangle(t Triangle) {...   Concrete type of shape s must be determined\n  Type Assertions for Disambiguation  Type assertions can be used to determine and extract the underlying concrete type  func DrawShape(s Shape2D) bool { rect,ok :=s.(Rectangle) if ok { DrawRect(rect) } tri,ok := s.(Triangle) if ok { DrawRect(tri) } }  Type assertion extracts Rectangle from Shape2D  Concrete type in parentheses   If interface contains concrete type  rect == concrete type, ok == true   If interface does not contain concrete type  rect==zero, ok==false    Type Switch  Switch statement used with a type assertion  func DrawShape(s Shape2D) bool { switch:= sh:=s.(type) { case Rectangle: DrawRect(sh) case Triangle: DrawTri(sh) } } Error Interface  Many Go programs return error interface objects to indicate errors  type error interface { Error() string }  Correct operation : error==nil  Incorrect operation: Error() print error message  Handling Errors f,err := os.Open(\u0026#34;/harris/text.txt\u0026#34;) if err!=nil { fmt.Println(err) return }  Check whether the error is nil If it is not nil, handle it fmt package calls the error() method to generate string to print  Keep in mind that the topics which are mentioned on this post is just brief summary, it means that all subheaders and headers can be extended to any size, however these are just bulletpoints and overall information in Functions, Methods, and Interfaces in Go module of specialization serie.\nIn next post, notes which are taken from concurrency module of the specialization serie will be posted.\nTake care ! üëãüèª\n","permalink":"https://mrturkmen.com/posts/go-notes/","summary":"In this post, I would like to share the notes that I took when I was completing following Go series education on Coursera. I can recommend it for anyone who would like to get rapid introduction to Go programming.\nProgramming with Google Go\nI should admit that although it looks fancy, nothing can be compared to actual development and contribution to open source projects. The course itself is quite interesting and contains very handy exercises regarding to Go development mentality.","title":"go: object oriented programming notes "},{"content":"Why VPN? It is crucial to do not expose your personal details or not being attacked by someone when you are connected to public endpoints such as coffee, airport, hotel and guest WIFI points. Furthermore, sometimes, people require to have organizatinal VPN access if organization itself does not provide one. For instance when students in universities have taken into consideration, it is quite important to reach resources that university is providing, it could be IEEE library access or enclosed resource which is only exclusive to internal network of the organization.\nNo matter what is your intention to use VPN, it is quite useful to have in any case.\nThere are lots of tutorials and explanations regarding to setting up WireGuard however I could not manage to find proper instructions for MacOSX users that\u0026rsquo;s why I thought it could be useful to pack up in once in a post. I hope, this would be handy for anyone who are seeking for brand new tutorial for using WireGuard on MacOSx.\nPreliminary information; there is no such a concept of server and client in WireGuard, all devices are called as peers. However, in sake of understanding better, I will refer virtual private server as WireGuard server however, it is actually a PEER !\nPrerequirement Wireguard is a decent VPN solution with all recent crypto features compared to other open source VPN solutions furthermore, it is lightweight.\nWireguard works based on key-pair relation between server and client just like ssh connection. The setup for personal usage is quite simple to do. Moreover, it is super fast, simple and more performant than any other open source VPN solution. Let\u0026rsquo;s start build our own VPN solution using WireGuard.\n Prepare the environment to setup WireGuard  Following scenario is made on ubuntu 18.04, however almost for any linux distribution the steps are more or less same.\n  Virtual private server (VPS), cheapest VPS with ubuntu 18.04 could be enough to get your own personal VPN. It can be rented over ;\n Google Cloud: Google provides $300 for 12 months for new comers to Google Cloud. Amazon AWS: Amazon has some free tiers which could be suitable for running your own VPN solution. Check it out from their website. Digital Ocean: Provides cheapest VPS on demand like $5 per month, which could be enough for personal VPN. Furthermore, you can claim $50 for Digital Ocean from Github Students Package, if you have student account on Github. Microsoft Azure: If you have Github student account, you can take some advantages of Microsoft cloud services as well. Check it out from Github Student Pack    Setup Peer 1  [-Peer 1 is the Server, which will route all or some of your network traffic- ]  I assumed that you have bought your VPS and ready to go for installation steps for WireGuard. Make sure that you have done with your SSH connection.\n$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y $ sudo add-apt-repository ppa:wireguard/wireguard $ sudo apt-get update $ sudo apt-get install wireguard Syncing Wireguard with kernel updates, which means that whenever there is an update for your linux kernel, wireguard module will be updated too.\n$ sudo modprobe wireguard Afterwards, you can ensure that the module is loaded as following;\n$ lsmod | grep wireguard wireguard 217088 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard Generate Keys Keys are backbone of WireGuard, extremely important step to take into.\n$ cd /etc/wireguard $ umask 077 $ wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey After given commands above, you will have publickey and privatekey in /etc/wireguard/ directory.\nSetup Configuration In this step, an interface should be defined in order to route all traffic from clients over rented VPS. Common interface for WireGuard is wg0.\nCreate a configuration file on VPS as /etc/wireguard/wg0.conf through your favorite text editor (vim, nano, vi).\n[Interface] PrivateKey = \u0026lt;generated-private-key-here\u0026gt; Address = 10.120.120.2/24 Address = fd86:ea04:1111::1/64 SaveConfig = true PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o ens3 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o ens3 -j MASQUERADE; ip6tables -D FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -D POSTROUTING -o ens3 -j MASQUERADE ListenPort = 51820   ens3 is main network interface fo your server, if it is different change it with correct one, you can observe your by typing ifconfig.\n  PrivateKey is the key which was generated in generate keys step.\n  Forward Traffic Traffic which comes into VPS should be forwarded properly, in order to set it up, change /etc/sysctl.conf file as follows.\nUsing your text editor, add following two lines of configuration into /etc/sysctl.conf\nnet.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 After that, to activate them immeditaly, type;\n$ sysctl -p It is time to bring VPN interface up;\n$ wg-quick up wg0 If there is no problem or complain when the command above called, you should be able to get following results from given command below.\n$ wg interface: wg0 public key: loZviZQpT5Sy4gFKEbk6Vc/rcJ3bH84L7TUj4qMB918= private key: (hidden) listening port: 51820 Setup Peer 2 (-client-) In my case, I will gonna go over for setup for MacOS client, however, the logic remains same for any other client that you will use.\nAll required appliations are downloadable from given adress below.\n Applications for all distribution  After installation of your application from given official website (https://www.wireguard.com/install/), it is time to add peers to server, and configure Peer 2 (client).\nSetup Configuration (client) If you are using MacOsX, you need to add following configuration under /usr/local/etc/wireguard/wg0.conf, however if you are using Linux, /etc/wireguard/wg0.conf would be correct place to put the configuration given below.\n[Interface] Address = 10.120.120.2/32 Address = fd86:ea04:1111::2/128 # note that privatekey value is just a place holder PrivateKey = KIaLGPDJo6C1g891+swzfy4LkwQofR2q82pFR6BW9VM= DNS = 1.1.1.1 [Peer] PublicKey = \u0026lt;your server public key\u0026gt; Endpoint = \u0026lt;your server public ip\u0026gt;:51820 AllowedIPs = 0.0.0.0/0, ::/0 If you have already installed GUI application from official installation website of WireGuard (https://www.wireguard.com/install/), it could be nice to arrange that configuration file as shown below.\nYou will be able to create your configuration file through WireGuard GUI application as follows.\nNote that the PrivateKey value is just a placeholder, it is not valid value to put into, you need to take care of your own private key which is generated by WireGuard.\n PublicKey : This should be same with the public key when you have generated the keys on server in this step PrivateKey: This is the key which is generated by WireGuard when you start to create new ocnfiguration tunnel from the applicaiton that you have installed from official site. EndPoint: IP address or A record of your VPS.  Add Peer When you are done with setting up configuration in client side, you need to let your other peer (server) know your device. It can be achieved by following command on server.\n$ wg set wg0 peer \u0026lt;client-public-key\u0026gt; allowed-ips 10.120.120.2/32,fd86:ea04:1111::2/128  client-public-key: It is a placeholder, change it with your own public key which can be seen in your WireGuard application, e.g; it is shown in given figure above.  When everything goes fine, you will be able to list peers by executing following command on server side.\n$ wg interface: wg0 public key: loZviZQpT5Sy4gFKEbk6Vc/rcJ3bH84L7TUj4qMB918= private key: (hidden) listening port: 51820 peer: Ta9esbl7yvQJA/rMt5NqS25I/oeuTKbFHJu7oV5dbA4= allowed ips: 10.120.120.2/32, fd86:ea04:1111::2/128 Last but not least step is to up wg0 interface on client side.(peer 2)\n$ wg-quick up wg0 Now you can check your IP address location through https://whatismyipaddress.com/\nFirewall configuration It can be handy to enable ufw and allow port 51820/udp for traffic routing.\n Check ufw status  $ ufw status verbose If not enabled, enable it by ; (before enabling ufw make sure that you have already allowed port 22 otherwise you may face some problems in return)  $ ufw enable Allow port 51820/udp  $ ufw allow 51820/udp Sometimes it is required to setup iptables, if that is the case use following rule.\n$ iptables -A INPUT -p udp -m udp --dport 51820 -j ACCEPT $ iptables -A OUTPUT -p udp -m udp --sport 51820 -j ACCEPT Note: Run commands with a user which has root privileges\nHow Traffic Looks Like Following figures are just snippet of some network traffic which is captured by WireShark. In the first figure, WireGuard is disabled, however in second image, WireGuard is enabled, you can easily distinguish the difference.\nIn ordinary usage (without VPN), your network traffic is seen as follows;\nWhen you have configured your WireGuard to route all your traffic, here is how it is look like;\nIt can be observed that all the traffic which goes out from client side has been encrypted with WireGuard.\nAll in all, it is quite handy to have your own private VPN access which decreases the likelihood of being attacked. Apart from VPN capabilities of WireGuard, it stabilize WIFI connections and actually improves quality of WiFi connection on clients. Keep in mind that you need to create exclusive key pairs for each client who would like to route all or some of his/her traffic over VPN.\nReferences  Quick Start Guide Installation Guide Unofficial WireGuard Docs Performance Limitation  ","permalink":"https://mrturkmen.com/posts/setup-free-vpn/","summary":"Why VPN? It is crucial to do not expose your personal details or not being attacked by someone when you are connected to public endpoints such as coffee, airport, hotel and guest WIFI points. Furthermore, sometimes, people require to have organizatinal VPN access if organization itself does not provide one. For instance when students in universities have taken into consideration, it is quite important to reach resources that university is providing, it could be IEEE library access or enclosed resource which is only exclusive to internal network of the organization.","title":"wireguard: set it up in few steps"},{"content":"Summary In this post, one of the well known (-open to discuss :)-) error of \u0026ldquo;No space left on device\u0026rdquo; which is caused due to Docker will be solved with different approaches.\nNote: \u0026ldquo;No space left on device\u0026rdquo; error can be caused due to any other reason than docker itself. Hence, it would be nice to make sure that the error is caused due to docker volumes.\nYou can check whether it is caused due to docker volumes or not by following steps over here\nNote that the explanations and examples may differ according to your environment, hence instead of taking these information as rules, applying them as a reference would be much better approach.\nThe system information for particular scenario described on post is given below:\nSystem information  PRETTY_NAME: Debian GNU/Linux 9 (stretch) NAME: Debian GNU/Linux VERSION_ID: 9 VERSION: 9 (stretch) KERNEL_RELEASE: 4.9.0-6-amd64  Docker daemon version Engine:\n Version: 19.03.8 API version: 1.40 (minimum version 1.12) Go version: go1.12.17 Git commit: afacb8b7f0 Built: Wed Mar 11 01:24:36 2020 OS/Arch: linux/amd64 Experimental: false  Although, it is NOT fully required to have same or somehow closer version of OS and Docker daemon, it could be nice to know exactly which conditions the following fix can work.\nMost of the cases, the following information could be valid for all Debian based systems however it is not for sure.\nThe Problem Docker can be used for most of the cases although now Kubernetes or any other orchestration systems preffered to used. That kind of orchestration systems may not cause this problem, hence they mostly provide auto scaling and runs on cloud. However, when you have a server with all responsibility, you may face this exact problem.\nThe problem is installation of docker into any system with default settings may create head ache in the future. If you are planning to use docker, intensively, the main reason of that, docker is generally using\n /var/lib/docker  place for docker volumes. In most scenarios, servers do not use root path for storing information, instead the appropriate approach for storing information on server is creating data volumes which has huge capacity and ability to extend or shrink time to time. The problem starts to show off itself when docker containers have been used for long period of time.\nEnsure about the problem It is quite handy to check whether the error of \u0026ldquo;No space left on device\u0026rdquo; caused by docker volumes. To do that following simple bash commands can be used.\n$ df -h /var/lib This will display free and used disk space for /var/lib path. Afterwards, you can see how is the difference between free and used spaces among usage percentage. An example is given below, it is taken from the system that I mentioned at the beginning in system information section.\n$ df -h /var/lib Filesystem Size Used Avail Use% Mounted on /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/mapper/data-data 9.1T 402G 8.2T 5% /data udev 252G 0 252G 0% /dev /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / proc 0 0 0 - /proc /dev/sda3 173G 162G 2.4G 99% / tmpfs 51G 275M 51G 1% /run /dev/sda3 173G 162G 2.4G 99% / /dev/nvme0n1 1.5T 775G 618G 56% /scratch /dev/sda3 173G 162G 2.4G 99% / sysfs 0 0 0 - /sys /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / As you can observe from the output above, I have plenty of spaces for volume /dev/nvme0n1 and /dev/mapper/data-data, however I was getting \u0026ldquo;No space left on the device\u0026rdquo; error, because root path became full due to docker volumes.\nIt can be quickly checked by the command below.\n$ df /var/lib/docker Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda3 180572828 166898524 4432040 98% / Temporary solution A temporary solution would be pruning docker volumes by running ;\n$ docker volume prune it will prune all volumes which are NOT in-use, if the volumes which are creating this bunch of data are in-use, prune command will not have any effect on the error. Although it works, it is a temporary solution, it may re-trigger the error in future. To resolve this issue permanently following approach could be used: Permanent Solution\nThe approach of having temporary solution can be extended with cronjobs, however, it is definitely not nice way of handling issues.\nHowever, it could be nice to have cron jobs which prune docker system time to time, because old docker images, containers and some other leftovers can create bunch of reclaimable storage. Docker prune commands do not have effect on resources (containers, volumes and networks) which are under use.\nYou can setup a cronjob which weekly prunes system, as provided below.\n$ crontab -e 0 0 * * 0 docker system prune -f # this will run at 00:00 on Sunday everyweek 0 0 * * 0 docker volume prune -f # will remove all (NOT in use) volumes weekly 0 0 * * 0 docker network prune -f # will remove all (NOT in use) networks  You can write a script according to your needs then place it to cron job as well. Before adding cron jobs, make sure $USER has valid permissions for docker group.\nNow, we are sure that the error caused due to docker volumes which means we can proceed with a permanent solution.\nIf it the error is not caused due to docker volumes, it could be caused from running short of inodes or basically not enough storage area.\nPermanent Solution Permanent solution is using one of storage volume for storing docker volumes, instead of using root path. These are the steps that you may take;\nStop docker service !\n$ systemctl stop docker Rsync all data under /var/lib/docker to other directory under storage volume\n$ mkdir -p /data/mnt # creates plave to use for docker volumes $ rsync -a /var/lib/docker /data/mnt/ # will sync everything  Rename default docker volumes path, this for taking a backup until we have success at the end.\n$ mv /var/lib/docker /var/lib/dockerbckp Create symbolic link to actual place\n$ ln -s /data/mnt/docker /var/lib/docker Enable DOCKER_OPTS\n$ vim /etc/default/docker DOCKER_OPTS=\u0026#34;--dns 8.8.8.8 --dns 8.8.4.4 -g /data/mnt/docker\u0026#34; Uncomment DOCEKR_OPTS add -g /data/mnt/docker as shown above in /etc/default/docker\nStart Docker daemon\n$ systemctl start docker If there was no error during implementation of steps, you can now test your setup.\nNote that thee should NOT be space between curly brackets when listing mount point only.\n$ docker volume create 2f2bd462b89c39bb641e7daf01048c5d811dd7796d7f89d250ea82c4532d2707 $ docker volume inspect --format { {.Mountpoint} } 2f2bd462b89c39b641e7daf01048c5d811dd7796d7f89d250ea82c4532d2707 /data/mnt/docker/volumes/2f2bd462b89c39bb641e7daf01048c5d811dd7796d7f89d250ea82c4532d2707/_data As you can observe above docker is using /data/mnt/docker for docker volumes, now we can safely remove old backup data. rm -rf /var/lib/dockerbckp\nI would like to mention that there is an information about changing docker volume instead implementing all steps mentioned above, just specifiying path under /etc/docker/daemon.js file would work (-example daemon.js is given-). However, when I tried that approach, it did not worked.\nExample\n$ vim /etc/docker/daemon.js { \u0026#34;data-root\u0026#34;: \u0026#34;/mnt/docker-data\u0026#34;, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } Check this documentation: https://docs.docker.com/config/daemon/systemd/I hope, this post can help others to find required information quickly and implement necessary steps to get to work.\nReferences:  https://github.com/moby/moby/issues/10613 https://github.com/moby/moby/issues/10613 https://success.docker.com/article/error-message-no-space-left-on-device-in-default-machine https://forums.docker.com/t/how-do-i-change-the-docker-image-installation-directory/1169  ","permalink":"https://mrturkmen.com/posts/no-space-left-on-device/","summary":"Summary In this post, one of the well known (-open to discuss :)-) error of \u0026ldquo;No space left on device\u0026rdquo; which is caused due to Docker will be solved with different approaches.\nNote: \u0026ldquo;No space left on device\u0026rdquo; error can be caused due to any other reason than docker itself. Hence, it would be nice to make sure that the error is caused due to docker volumes.\nYou can check whether it is caused due to docker volumes or not by following steps over here","title":"error: no space left on this device "},{"content":"Giri≈ü Elasticsearch √ºzerinde b√ºy√ºk boyuttaki verileri hƒ±zlƒ± bir ≈üekilde i≈ülemek √ßaba gerektiren i≈ülerden biridir. Bu yazƒ±da bir √ßalƒ±≈üma esnasƒ±nda yapƒ±lan elasticsearch performans iyile≈ütirmelerini ve nasƒ±l yapƒ±ldƒ±ƒüƒ±nƒ± anlatmaya √ßalƒ±≈üacaƒüƒ±m.\nBu iyile≈ütirme i≈ülemlerinin nasƒ±l yapƒ±ldƒ±ƒüƒ±na ge√ßmeden once elasticsearch mimarisinde bulunan bazƒ± bile≈üenlerden bahsetmekte yarar var.\n Cluster : Elasticsearch bir veya birden fazla bilgisayarda entegre ≈üekilde √ßalƒ±≈üabilir ve bu elasticsearch √ºn √ßalƒ±≈ütƒ±ƒüƒ± makinelere NODE denir. Cluster (K√ºme) ise bu node‚Äôlarƒ±n olu≈üturduƒüu gruba verilen yapƒ±ya denir. Index : Elasticsearch √ºzerinde veriler indexlerde tutulur, index basit olarak d√∂k√ºmanlarƒ±n toplandƒ±ƒüƒ± ve tutulduƒüu yapƒ±dƒ±r. Shard: Elasticsearch √º birden fazla makine √ºzerinde (sanal veya fiziksel makine) tutabildiƒüimizden dolayƒ±, indekslerde tutulan veriler bu cluster adƒ± verdiƒüimiz ortamlarda daƒüƒ±tƒ±k (distributed) ≈üekilde tutulur. Bu i≈ülemin y√∂netim kƒ±smƒ±nƒ± elasticsearch otomatik olarak halleder. Replica: Elasticsearch normalde (default) her indeks i√ßin 5 ana shard ve 1 replica olu≈üturur, yani her bir indeks 5 adet shard‚Äôa sahip ve her shard bir replica i√ßermektedir. A≈üaƒüƒ±da bu durumu g√∂steren bir ekran g√∂r√ºnt√ºs√º verilmi≈ütir.  http://\u0026lt;elk-ip\u0026gt;:9200/_cat/shards Bu performans iyile≈ütirme adƒ±mlarƒ±, tek sunucu (node) √ºzerinde √ßalƒ±≈üan Elasticsearch √ºzerinde yapƒ±lmƒ±≈ütƒ±r, yani daƒüƒ±tƒ±k bir sistem √ºzerinde iyile≈ütirme yapmak buradaki anlatƒ±lacaklardan farklƒ± olacaktƒ±r. (Bazƒ± kƒ±sƒ±mlarƒ± benzerlik g√∂sterse dahi)\nNot : Bu iyile≈ütirme i≈ülemleri, √ßalƒ±≈ütƒ±rƒ±lan sunucu sayƒ±sƒ±na (daƒüƒ±tƒ±k sistem ise), internet hƒ±zƒ±na (daƒüƒ±tƒ±k sistem ise), sunucuda √ßalƒ±≈üan i≈ületim sisteminden, kullandƒ±ƒüƒ± disk, CPU ve RAM kapasitesine g√∂re deƒüi≈üiklik g√∂sterebilir.Paralel Bulk indeksleme Elasticsearch √ºzerinde indeksleme i≈ülemi birka√ß farklƒ± y√∂ntem ile yapƒ±labilmektedir bunlardan bazƒ±larƒ±, tek tek indeksleme, bulk indeksleme ve parallel indekslemedir.\nTek tek indeksleme y√∂ntemi, tahmin edeceƒüiniz √ºzere veri b√ºy√ºk olduƒüunda tercih edilecek bir y√∂ntem deƒüildir, nedeni ise her kayƒ±t i√ßin elasticsearche istekte bulunmasƒ±ndan dolayƒ±dƒ±r. Yani 10000 adet satƒ±r i√ßin 10000 istek g√∂nderilecek demektir, bunun yerine bulk indeksleme tercih edilir 10000 adet kayƒ±t i√ßin tek istek g√∂nderimi yapar b√∂ylece hem istek sayƒ±sƒ± minimuma indirilmi≈ü olur, hem de indeksleme s√ºresi azaltƒ±lmƒ±≈ü olur. Bunun bir adƒ±m daha geli≈ümi≈üi ise paralel bulk tƒ±r, bu indeksleme y√∂nteminde ise birden fazla thread ile veri elasticsearche g√∂nderilecektir saƒülar. Bizim √ßalƒ±≈ümamƒ±zda paralel bulk i≈ülemi kullanƒ±lmƒ±≈ütƒ±r.\nBu √ßalƒ±≈ümada, Elasticsearch √ºn Python mod√ºlleri kullanƒ±lmƒ±≈ütƒ±r, bu mod√ºlde paralel bulk kullanƒ±mƒ± a≈üaƒüƒ±daki ≈üekildedir.\nParalel bulk kullanabilmek i√ßin Python generator kullanmak tercih edilen y√∂ntemlerden biridir, nedeni hem ram kullanƒ±mƒ± az olur, hemde tekrarlƒ± (iterate) bir yapƒ±ya sahiptir.\n√ñrnek generator yapƒ±sƒ± :\ndef gendata(docs_list): for json in docs_list: yield { \u0026#34;_index\u0026#34;: \u0026#34;herhangibirsey\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;isim\u0026#34;:json[\u0026#39;isim\u0026#39;], \u0026#34;soyisim\u0026#34;:json[\u0026#39;soyisim\u0026#39;], \u0026#34;sehir\u0026#34;: json[\u0026#39;sehir\u0026#39;], \u0026#34;yas\u0026#34;:json[\u0026#39;yas\u0026#39;], \u0026#34;meslek\u0026#34;:json[\u0026#39;meslek\u0026#39;] } Bu generator yapƒ±sƒ±nda, gendata fonksiyonu docs_list adƒ±nda bir liste alƒ±yor ve bu listenin i√ßeriƒüi ≈üu ≈üekilde olduƒüunu varsayƒ±yoruz:\ndocs_list= [{\u0026#34;isim\u0026#34;: \u0026#34;Mehmet\u0026#34;,\u0026#34;soyisim\u0026#34;: \u0026#34;Ataklar\u0026#34;,\u0026#34;sehir\u0026#34;: \u0026#34;Kocaeli\u0026#34;,\u0026#34;yas\u0026#34;: 45,\u0026#34;meslek\u0026#34;: \u0026#34;Ogretmen\u0026#34;}] gendata fonksiyonu docs_list listesi icerisindeki her bir dokumandan gereken alanlari alarak indeksleme fonksiyonuna vermektedir. Parallel bulk, Python script √ºzerinden ≈üu ≈üekilde √ßaƒürƒ±labilir.\nfor response in parallel_bulk(elasticDeamon, gendata(doc_records), thread_count=7): pass Indeks yenileme aralƒ±ƒüƒ±nƒ± kaldƒ±rma (refresh_interval) Node √ºzerinde bulunan indeks e, bulk indexleme i≈ülemi yapƒ±lƒ±rken, indeks yenileme aralƒ±ƒüƒ± bulk indeksleme s√ºresi boyunca ortadan kaldƒ±rƒ±lmalƒ±dƒ±r. √á√ºnk√º elasticsearch √ºn her yenileme yapmasƒ± sunucu √ºzerinde segment olu≈üturmasƒ±nƒ± saƒülamaktadƒ±r, bu hem makinen kaynaklarƒ±na dezavantaj olarak yansƒ±maktadƒ±r, ram ve cpu kullanƒ±mƒ±nƒ± artƒ±ran pahalƒ± bir i≈ülemdir.\nKibana √ºzerinde bulunan ‚ÄúDev Tools‚Äù kƒ±smƒ±ndan a≈üaƒüƒ±daki verilen komut ile kaldƒ±rƒ±labilir.\nPUT /\u0026lt;indeks-ismi\u0026gt;/_settings { \u0026#34;index\u0026#34;: { \u0026#34;refresh_interval\u0026#34;: -1 } } Terminal √ºzerinden:\ncurl -X PUT \u0026#34;\u0026lt;elk-ip\u0026gt;:9200/\u0026lt;index-ismi\u0026gt;/_settings\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;index\u0026#34; : { \u0026#34;refresh_interval\u0026#34; : -1 } } \u0026#39; Bulk indeksleme i≈ülemi sona erdiƒüinde ise, aynƒ± komutlar kullanƒ±larak, yenileme aralƒ±ƒüƒ± ‚Äúnull‚Äù a e≈üitlenebilir. B√∂ylece kullanƒ±cƒ± kibana √ºzerinden, yenileme aralƒ±ƒüƒ±nƒ± kendisi ayarlayabilir.\nPUT /\u0026lt;index-ismi\u0026gt;/_settings { \u0026#34;index\u0026#34;: { \u0026#34;refresh_interval\u0026#34;: null } } Indeks kopyalarƒ±nƒ± devre dƒ±≈üƒ± bƒ±rakmak (Replica) Kulaƒüa ho≈ü gelmesede indeks kopyalarƒ±nƒ± (replicas) devre dƒ±≈üƒ± bƒ±rakmak indeksleme hƒ±zƒ±nƒ± artƒ±rƒ±r, en b√ºy√ºk dezavantajƒ± indeksi herhangi bir hata durumunda veri kaybƒ±na kar≈üƒ± savunmasƒ±z bƒ±rakƒ±r.\nKibana ‚ÄúDevTools‚Äù kƒ±smƒ±ndan kopyalarƒ±n devre dƒ±≈üƒ± bƒ±rakƒ±lmasƒ±.\nPUT /\u0026lt;indeks-ismi\u0026gt;/_settings { \u0026#34;index\u0026#34; : { \u0026#34;number_of_replicas\u0026#34; : 0 } } Terminal √ºzerinden:\ncurl -X PUT \u0026#34;\u0026lt;elk-ip\u0026gt;:9200/\u0026lt;index-ismi\u0026gt;/_settings\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;index\u0026#34; : { \u0026#34;number_of_replicas\u0026#34; : 0 } } \u0026#39; Swap alanƒ±nƒ± kaldƒ±rmak. (Sunucu √ºzerindeki) Elasticsearch√º hƒ±zlƒ± yapan fakt√∂rlerden en √∂nemlisi ram √ºzerinden i≈ülem yapmasƒ±dƒ±r. Linux sunucularƒ±nda bulunan swap alanƒ±, ram de yeterli alan kalmadƒ±ƒüƒ±nda veya ram √ºzerinde uzun s√ºre i≈ülem yapƒ±lmayan (aktif olmayan) dosyalarƒ±n disk √ºzerinde kƒ±sa s√ºreliƒüine saklanmasƒ±ndan olu≈üan alandƒ±r. Bu elasticsearh i√ßin dezavantaj olabilmektedir, elasticsearch√ºn tamamen ram √ºzerinden i≈ülem yapmasƒ±nƒ± saƒülamak adƒ±na swap alanƒ±nƒ± kaldƒ±rmak indeksleme ve arama yapma hƒ±zƒ±nƒ± artƒ±racaktƒ±r.\nSwap alanƒ±nƒ± ge√ßici olarak ≈üu ≈üekilde kaldƒ±rabilirsiniz, terminal √ºzerinden bu komutu yazmanƒ±z yeterlidir.\n$ swapoff -a Swap alanƒ±nƒ± tamamen kaldƒ±rabilmek i√ßin ‚Äúroot‚Äù yetkisi ile /etc/fstab dosyasƒ± i√ßerisinde swap kelimesi ge√ßen kƒ±smƒ± yorum satƒ±rƒ± yapmanƒ±z yeterli olacaktƒ±r.\nSwap alanƒ±nƒ± ortadan kaldƒ±rdƒ±ktan sonra sunucu √ºzerinde √ßalƒ±≈üan elasticsearch ayarlarƒ±nda ufak bir deƒüi≈üiklik yapmak gerekecektir.\n/etc/elasticsearch/elasticsearch.yml\nelasticsearch.yml dosyasƒ± i√ßerisine ≈üu parametreyi eklemeniz gerekmektedir.\nbootstrap.mlockall: true Bu i≈ülem elasticsearch √ºn tamamen RAM √ºzerinden i≈ülem yapmasƒ±nƒ± saƒülayacaktƒ±r.\nJVM Heap Alanƒ±nƒ± Artƒ±rmak Elasticsearch JVM heap, verileri hƒ±zlƒ± bir ≈üekilde i≈ülemek ve veriler √ºzerindeki i≈ülemleri yapabilmek i√ßin elasticsearche √∂zel olarak ayrƒ±lmƒ±≈ü bir alan. Bu alan normalde (default olarak) 1 GB alana sahiptir, eƒüer sunucu √ºzerinde yeterli miktarda RAM mevcut ise bu alanƒ± artƒ±rmak indeksleme ve i≈ülem yapma hƒ±zƒ±nƒ± artƒ±racaktƒ±r.\nBurada √∂nemli olan JVM Heap alanƒ± 64 Bit yapƒ±ya sahip bir sunucu i√ßin maksimum 32 GB a kadar artƒ±rƒ±lmalƒ±dƒ±r, sunucu √ºzerinde √ßok daha fazla RAM olsa dahi 32 GB limiti ge√ßmemek gerekmektedir. Bununla ilgili detaylƒ± a√ßƒ±klamaya buradan eri≈üebilirsiniz: https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html\nJVM Heap alanƒ± ayarlanƒ±rken genellikle fiziksel ram boyutunun yarƒ±sƒ± kadar heap alanƒ± vermek tercih edilir, 32 GB boyutunu ge√ßmeyecek ≈üekilde.\nJVM Heap ayarlarƒ± ≈üu ≈üekilde yapƒ±labilir, Debian tabanlƒ± bir i≈ületim sisteminde elasticsearch √ºn bulunduƒüu dizin altƒ±nda jvm.options adƒ±nda bir dosya bulunmaktadƒ±r.\nEƒüer heap alanƒ±nƒ± 16 GB ayarlamak isterseniz(fiziksel RAM in en az 32 GB olduƒüundan emin olunuz ), jvm.options dosyasƒ± i√ßerisine ≈üu ≈üekilde kaydedebilirsiniz.\n/etc/elasticsearch/jvm.options ## bu jvm.options dosyasƒ± i√ßerisine a≈üaƒüƒ±da verilen parametler girilir. -Xms16GB -Xmx16GB Bu parametreler, jvm.options dosyasƒ± i√ßerisine kaydedildikten sonraki adƒ±mda ise elasticsearch servisini yeniden ba≈ülatmayƒ± unutmayƒ±nƒ±z.\nsudo service elasticsearch restart SSD veya RAID 0 disk kullanƒ±mƒ± HDD disklere g√∂re √ßok hƒ±zlƒ± olan SSD diskler, elasticsearch √ºn veriyi daha hƒ±zlƒ± i≈ülemesine, verimliliƒüi artƒ±rmasƒ±na direkt olarak etki edecektir. RAID diskleri kullanƒ±rken RAID 0 haricindeki tiplerini kullanmak tercih edilmez.\nBu kƒ±sƒ±mda elasticsearch performansƒ±nƒ± artƒ±rmak i√ßin yapƒ±lmasƒ± gerekli olabilecek bazƒ± adƒ±mlardan bahsedildi bunlar √∂zet olarak.\n Paralel bulk indekslemek JVM heap alanƒ± artƒ±rmak ƒ∞ndeks kopyalarƒ± devre dƒ±≈üƒ± bƒ±rakmak ƒ∞ndeks yenileme aralƒ±ƒüƒ±nƒ± devre dƒ±≈üƒ± bƒ±rakmak Sunucu Swap alanƒ±nƒ± kaldƒ±rmak SSD veya RAID 0 Disk Kullanmak  Bu, elasticsearch performans iyile≈ütirme adƒ±mlarƒ±nƒ± g√∂steren birinci kƒ±sƒ±m, ikinci kƒ±sƒ±mda, elasticsearch √ºzerinde indeks olu≈ütururken, mapping sisteminin verimize g√∂re nasƒ±l yapƒ±landƒ±rƒ±lmasƒ± gerektiƒüinden, indeks √ºzerinde otomatik olarak olu≈üturulan bazƒ± alanlarƒ±n kaldƒ±rƒ±lmasƒ±ndan, optimum shard sayƒ±sƒ±nƒ±n belirlenmesinden, indeks performans (benchmarking) √∂l√ß√ºmlerinden ve Grafana √ºzerinden elasticsearch deƒüerlerinin (CPU,I/O, RAM, DISK kullanƒ±mƒ±nƒ±n) izlenmesi anlatƒ±lacaktƒ±r.\nBu √ßalƒ±≈üma esnasƒ±nda yararlanƒ±lan kaynaklar https://blog.codecentric.de/en/2014/05/elasticsearch-indexing-performance-cheatsheet\u0026gt;https://www.elastic.co/guide/en/elasticsearch/reference/master/tune-for-indexing-speed.htmlhttps://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.htmhttps://elasticsearch-py.readthedocs.io/en/master","permalink":"https://mrturkmen.com/posts/elasticsearch-performans-art%C4%B1r%C4%B1m%C4%B1/","summary":"Giri≈ü Elasticsearch √ºzerinde b√ºy√ºk boyuttaki verileri hƒ±zlƒ± bir ≈üekilde i≈ülemek √ßaba gerektiren i≈ülerden biridir. Bu yazƒ±da bir √ßalƒ±≈üma esnasƒ±nda yapƒ±lan elasticsearch performans iyile≈ütirmelerini ve nasƒ±l yapƒ±ldƒ±ƒüƒ±nƒ± anlatmaya √ßalƒ±≈üacaƒüƒ±m.\nBu iyile≈ütirme i≈ülemlerinin nasƒ±l yapƒ±ldƒ±ƒüƒ±na ge√ßmeden once elasticsearch mimarisinde bulunan bazƒ± bile≈üenlerden bahsetmekte yarar var.\n Cluster : Elasticsearch bir veya birden fazla bilgisayarda entegre ≈üekilde √ßalƒ±≈üabilir ve bu elasticsearch √ºn √ßalƒ±≈ütƒ±ƒüƒ± makinelere NODE denir. Cluster (K√ºme) ise bu node‚Äôlarƒ±n olu≈üturduƒüu gruba verilen yapƒ±ya denir.","title":"elasticsearch: performans artƒ±rƒ±mƒ± "},{"content":"√ñzet Terminal √ºzerinden kullanƒ±lan en me≈ühur yazƒ± d√ºzenleme programƒ± VIM hakkƒ±nda komutlar ve bazƒ± kullanƒ±≈ülƒ± linux komutlarƒ±\nScript nasƒ±l yazƒ±lƒ±r.   Dosya olu≈üturulur ve dosyanƒ±n ba≈üƒ±na terminalin yolu eklenir #!/bin/bash\n  terminal komutlarƒ±nƒ± bu yolun altƒ±na yazƒ±nƒ±z.\n  daha sonra dosyanƒ±n iznini ayarlayƒ±nƒ±z. chmod +x dosya_ismi\n  terminal scriptini ≈üu ≈üekilde √ßalƒ±≈ütƒ±rƒ±labilirsiniz. ./dosya_ismi\n  √∂rnek bash scripti\n  !/bin/bash echo \u0026#34;Adƒ±nzƒ± giriniz: \u0026#34; read isim echo \u0026#34;Sifrenizi giriniz\u0026#34; read sifre if [[ ( $isim == \u0026#34;admin\u0026#34; \u0026amp;\u0026amp; $sifre == \u0026#34;random\u0026#34; ) ]]; then echo \u0026#34;Ba≈üarƒ±lƒ±\u0026#34; else echo \u0026#34;Ba≈üarƒ±sƒ±z\u0026#34; fi VIM Birden fazla dosya √ºzerinde √ßalƒ±≈ümak $ vim *.txt # birden fazla txt ile dosyasƒ±nƒ± aynƒ± anda a√ßmanƒ±zƒ± saƒülar (eƒüer birden fazla dosya mevcut ise) $ :wall veya :qall # b√ºt√ºn a√ßƒ±k dosyalardan yaz veya √ßƒ±k komutudur. $ vim -o *.txt # birden fazle txt dosyasƒ±nƒ± a√ßar ve yatay d√ºzlemde g√∂sterir, dikey d√ºzlem i√ßin -O parametresi kullanƒ±lara $ :args *.txt # txt ile biten b√ºt√ºn dosyalarƒ± argument listesine aktarƒ±r. $ :all # b√ºt√ºn dosyalarƒ± yatay d√ºzlemde ayƒ±rƒ±r $ CTRL-w # birden fazla pencere arasƒ±nda gezmenizi saƒülar $ :split # aynƒ± dosyayƒ± iki farklƒ± pencerede g√∂sterir. $ :split \u0026lt;acƒ±lacak_dosya\u0026gt; # dosyayƒ± yeni bir pencerede a√ßar $ :vsplit # brden fazla pencereyi dikey komunda ayƒ±rƒ±r, tablolar i√ßin √ßok kullanƒ±≈ülƒ±dƒ±r. \u0026#34;:set scrollbind \u0026#34; komutu ile a√ßƒ±k dosyalar da aynƒ± anda yukarƒ± a≈üaƒüƒ± yapabilirsiniz.  $ :close # bulunduƒüunuz pencereyi kapatƒ±r $ :only # bulunduƒüunuz pencere hari√ß diƒüerlerinin tamamƒ±nƒ± kapatƒ±r.  Hece Kontrol√º \u0026amp; S√∂zl√ºk $ aspell -c \u0026lt;dosya\u0026gt; # verilen dosyada heceleri kontrol eder, terminal komutudur $ aspell -l \u0026lt;dosya\u0026gt; # terminal komutu $ :! dict \u0026lt;cumle\u0026gt; # c√ºmlenin anlamƒ±nƒ± kontrol etmenizi saƒülar $ :! wn \u0026#39;cumle\u0026#39; -over # c√ºmlenin e≈ü anlamlƒ±larƒ±nƒ± g√∂sterir Dosyayƒ± yazdƒ±rma $ :ha # b√ºt√ºn dosyayƒ± yazdƒ±rƒ±r $ :#,#ha # (#,#) ile belirtilen alandaki metini yazdƒ±rƒ±r Birle≈ütime / Ekleme Komutu $ :r \u0026lt;dosya_ismi\u0026gt; # a√ßƒ±k olan dosya i√ßerisine, # aynƒ± dizinde olan ba≈üka bir dosyayƒ± eklemek i√ßin bu komut kullanƒ±labilir, # imlecin hizasƒ±ndan sonra ekleme yapar Geri Alma / Yeniden Alma $ u # en son yaptƒ±ƒüƒ±nƒ±z deƒüi≈üikliƒüi geri alƒ±r $ U # yaptƒ±ƒüƒ±nƒ±z b√ºt√ºn deƒüi≈üiklikleri geri alƒ±r $ CTRL-R # geri alƒ±nmƒ±≈ü bir kƒ±smƒ± yeniden getirmenizi saƒülar. Kopyalama \u0026amp; Yapƒ±≈ütƒ±rma $ yy # imlecin bulunduƒüu satƒ±rƒ± kopyalar, 2 satƒ±r kopyalamak i√ßin 2yy kullanƒ±labilir. $ p # kesilen/kopyalanan i√ßeriƒüi imle√ßten ba≈ülayacak ≈üekilde yapƒ±≈ütƒ±rƒ±r  Silme/Kesme (NORMAL modda uygulanƒ±r. Yani Vim komut satƒ±rƒ±nda deƒüil. EXE modunda deƒüil. ) $ x # imlecin √ºzerinde bulunduƒüu karakteri siler. $ dw # imlecin bulunduƒüu kelimeyi sonuna kadar siler (Bo≈üluklar dahil ) $ de # imlecin bulunduƒüu kelimeyi sonuna kadar siler (Bo≈üluklar hari√ß ) $ cw # kelimenin geriye kalan kƒ±smƒ±nƒ± siler ve sizi ekleme moduna alƒ±r, ekleme modundan ESC ile √ßƒ±kabilirsiniz. $ c$ # bulunduƒüu satƒ±rƒ± tamamen siler ve sizi ekleme moduna alƒ±r ekleme modundan ESC ile √ßƒ±kabilirsiniz.  $ d$ # imlecten itibaren satƒ±rƒ± siler e $ dd # satƒ±rƒ± tamamen siler, imlecin nerede olduƒüunun √∂nemi yoktur $ 2dd # ileriki 2 satƒ±rƒ± siler, benzer sekilde 3dd : uc satƒ±r siler, 4dd: dort satƒ±r siler, (imlecten bagƒ±msƒ±z) Koyma $ p # kesilen/kopyalanan i√ßeriƒüi imle√ßten ba≈ülayacak ≈üekilde yapƒ±≈ütƒ±rƒ±r  Dosya i√ßerisinde arama (Vim) (bu kƒ±sƒ±mda genelde d√ºzenli ifadeler kullanƒ±lƒ±r ) $ /aramak_istediƒüiniz_d√ºzen # yazdƒ±ƒüƒ±nƒ±z ifadeyi a√ßƒ±k olan belge i√ßerisinde arar ve hepsini i≈üaretler $ ?aramak_istediƒüiniz_d√ºzen # yazdƒ±ƒüƒ±nƒ±z ifadeyi a√ßƒ±k olan belge i√ßerisinde arar ama i≈üaretlemez, n ile ileriki kelimeyi g√∂rebilirsiniz.  $ :set ic # kelimelerin b√ºy√ºk/k√º√ß√ºk harf ayrƒ±mƒ±nƒ± ortadan kaldƒ±rƒ±r $ :set hls # aranan ve bulunan kelimeleri vurgulu ≈üekilde g√∂sterir. D√ºzenli ifadeler ile metin y√∂netimi $ :s/harf1/harf2/ # harf1, harf2 ile deƒüi≈ütirilir fakat sadece ilk kar≈üƒ±la≈ümada yapƒ±lƒ±r $ :s/harf1/harf2/g # b√ºt√ºn dosya i√ßerisindeki harf1, harf2 ile deƒüi≈ütirilir. $ :s/harf1/harf2/gc # yukarƒ±daki i≈ülemin aynƒ±sƒ±nƒ± onay alarak yapmak i√ßin \u0026#34;c\u0026#34; eklenir $ :#,#s/harf1/harf2/g # (#,#) arasƒ±ndaki satƒ±rlarda bulunan harf1, harf2 ile deƒüi≈ütirilir. $ :%s/harf1/harf2/g # t√ºm dosyadaki harf1 ifadesi harf2 ile deƒüi≈ütirilir. $ :%s/\\(harf1\\)\\(.*\\)/\\1/g # harf1 sonrakisindeki b√ºt√ºn satƒ±rlarƒ± siler. $ :%s/\\(SL\\dm\\d\\d\\d\\d\\d\\.\\d\\)\\(.*\\)/\\1\\t\\2/g # SL1m12345.1 ve tanƒ±mƒ± arasƒ±na TAB bo≈üluƒüu ekler  $ :%s/\\n/ifade/g #Satƒ±r verilen ifade ile deƒüi≈ütirilir. $ :%s/\\(^SL\\dm\\d\\d\\d\\d\\d.\\d\\t.\\{-}\\t.\\{-}\\t.\\{-}\\t.\\{-}\\t\\).\\{-}\\t/\\1/g # 5 ve 6.ncƒ± TAB taki (5. Kolondaki), i√ßeriƒüi \u0026#34;{-}\u0026#34; ile degi≈ütirir.  $ :#,#s/\\( \\{-} \\|\\.\\|\\n\\)/\\1/g # (#,#) verilen aralƒ±kta ne kadar c√ºmle olduƒüunu hesaplar $ :%s/\\(E\\{6,\\}\\)/\u0026lt;font color=\u0026#34;green\u0026#34;\u0026gt;\\1\u0026lt;\\/font\u0026gt;/g # 6 dan fazla E ge√ßen kƒ±sƒ±mlarƒ±, HTML renkleri ile vurgular. $ :%s/\\([A-Z]\\)/\\l\\1/g # B√ºy√ºk harfleri, k√º√ß√ºk harfler ile deƒüi≈ütirir, \u0026#39;%s/\\([A-Z]\\)/\\u\\1/g\u0026#39; , bu ise k√º√ß√ºk harfleri b√ºy√ºk harfler ile deƒüi≈ütirir. $ :g/ifade/ s/\\([A-Z]\\)/\\l\\1/g | copy $ # ifade yeni olu≈üturulan ifade ile deƒüi≈ütirilir e≈üdeƒüer olanlar copy $ ile yazdƒ±rƒ±lƒ±r.  HTML D√ºzenleme -metini HTML formatƒ±na cevirme $ :runtime! syntax/2html.vim # vim i√ßerisinde bu komutu √ßalƒ±≈ütƒ±rƒ±nƒ±z. Vim i√ßerisinden terminal komutu √ßalƒ±≈ütƒ±rma $ :!\u0026lt;terminal_komutu\u0026gt; \u0026lt;ENTER\u0026gt; # terminal komutunu vim i√ßerisinden √ßalƒ±≈ütƒ±rƒ±r $ :sh terminal ile vim arasƒ±nda gezmenizi saƒülar Tablo d√ºzenleyicisi olarak Vim' i kullanmak $ v # karakterleri se√ßmek i√ßin g√∂rsel mod ba≈ülatƒ±lƒ±r. $ V # satƒ±rlarƒ± se√ßmek i√ßin g√∂rsel mod ba≈ülatƒ±lƒ±r. $ CTRL-V # blok g√∂rsel se√ßim yapmanƒ±zƒ± saƒülar. $ :set scrollbind # aynƒ± ayna ayrƒ±lan iki ayrƒ± dosyada gezinti yapmanƒ±zƒ± saƒülar.  Vim ayarlarƒ±nƒ± deƒüi≈ütirmek - .vimrc dosyasƒ± i√ßerisindeki parametreler isteƒüinize g√∂re deƒüi≈ütirilebilir.\nKullanƒ±≈ülƒ± terminal komutlarƒ± $ cat \u0026lt;dosya1\u0026gt; \u0026lt;dosya2\u0026gt; \u0026gt; \u0026lt;sonuc\u0026gt; # dosya1 ve dosya2 yi sonuc dosyasina kopyalar ve sonuc dosyasini olusturur. $ paste \u0026lt;dosya1\u0026gt; \u0026lt;dosya2\u0026gt; \u0026gt; \u0026lt;p_sonuc\u0026gt; # iki farklƒ± kaynaktan gelen girdiyi, aralarƒ±nda TAB bo≈üluƒüu olacak ≈üekilde aynƒ± dosya (p_sonuc) i√ßerisine yapƒ±≈ütƒ±rƒ±r. $ cmp \u0026lt;dosya1\u0026gt; \u0026lt;dosya2\u0026gt; # iki dosyanƒ±n aynƒ± olup olmadƒ±gƒ±nƒ± size bildirir. $ diff \u0026lt;dosya1\u0026gt; \u0026lt;dosya2\u0026gt; # iki dosya arasƒ±ndaki farklƒ±lƒ±klarƒ± g√∂sterir $ head -\u0026lt;numara\u0026gt; \u0026lt;dosya\u0026gt; # verdiƒüiniz numara kadar ilk X satƒ±rƒ± yazdƒ±rƒ±r. $ tail -\u0026lt;numara\u0026gt; \u0026lt;dosya\u0026gt; # verdiƒüiniz numara kadar son X satƒ±rƒ± yazdƒ±rƒ±r.  $ split -l \u0026lt;numara\u0026gt; \u0026lt;dosya\u0026gt; # dosyanƒ±n satƒ±rƒ±larƒ±nƒ± ayƒ±rƒ±r. $ csplit -f out dosya_ismi \u0026#34;%^\u0026gt;%\u0026#34; \u0026#34;/^\u0026gt;/\u0026#34; \u0026#34;{*}\u0026#34; # dosya_ismini \u0026gt; den itibaren bir√ßok farklƒ± k√º√ß√ºk dosyalar olu≈üturur. $ sort \u0026lt;dosya_ismi\u0026gt; # dosya i√ßerisindekileri sƒ±ralar -b argument kullanƒ±lƒ±rsa bo≈üluklarƒ± yok sayar. $ sort -k 2,2 -k 3,3n girdi_dosyasƒ± \u0026gt; cƒ±ktƒ± # -k argument i kolon i√ßin, -n sayƒ±sal olarak sƒ±ralar ve tablo ≈üeklinde kaydeder.  $ sort girdi_dosyasƒ± | uniq \u0026gt; cƒ±ktƒ± # uniq komutu aynƒ± olan verileri dahil etmez. $ join -1 1 -2 1 \u0026lt;tablo1\u0026gt; \u0026lt;tablo2\u0026gt; # tablo1 ve tablo2 yi birle≈ütirir, -1 dosya1, 1:kolon1; -2dosya2, col2.  $ sort tablo1 \u0026gt; tablo1a; sort tablo2 \u0026gt; tablo2a; join -a 1 -t \u0026#34;`echo -e \u0026#39;\\t\u0026#39;`\u0026#34; tablo1a tablo2a \u0026gt; tablo3 # \u0026#39;-a \u0026lt;tablo\u0026gt;\u0026#39; : verilen tablonun b√ºt√ºn kayƒ±tlarƒ±nƒ± yazdƒ±rƒ±r. Normalde yazdƒ±rma i≈ülemi iki tabloda ortak olan kƒ±sƒ±mlarƒ± yazdƒ±rƒ±r. \u0026#39;-t \u0026#34;`echo -e \u0026#39;\\t\u0026#39;`\u0026#34; -\u0026gt;\u0026#39;  : TAB bo≈üluƒüu kullanarak tablolarƒ± √ßƒ±ktƒ± dosyasƒ±na yazdƒ±rƒ±r. $ cat tablom | cut -d , -f1-3 # cut komutu : tablonun belirlenen kƒ±sƒ±mlarƒ± alƒ±r, -d alanlarƒ±n nasƒ±l ayrƒ±lacaƒüƒ±nƒ± belirtilsiniz. -d : burada , olarak belirlenmi≈ütir, normalde TAB bo≈üluk, -f tablonun kolonlarƒ±nƒ± belirtir, kolon 1 den 3 e.  Kullanƒ±≈ülƒ± tek satƒ±r komutlar $ for i in *.input; do mv $i ${i/isim\\.eski/isim\\.yeni}; done # isim.eski adƒ±ndaki dosyanƒ±n ismini, isim.yeni olarak deƒüi≈ütirir. Komutu test etmek i√ßin, do mv komutu √∂n√ºne \u0026#34;echo\u0026#34; konulabilir.  $ for i in *.girdi; do ./uygulama $i; done # bir √ßok dosya i√ßin verilen uygulamayƒ± √ßalƒ±≈ütƒ±rƒ±r.  $ for i in *.girdi; do komut -d /veri/../veri_tabanƒ± -i $i \u0026gt; $i.out; done # komut for d√∂ng√ºs√º i√ßerisinde *.girdi √ºzerinde √ßalƒ±≈üƒ±r ve *.out dosyasƒ± olu≈üturur.  $ for i girdi *.pep; do hedef -db /usr/../veri_tabanƒ± -seed $i -out $i; done # hedef in √ºzerinde for d√∂ng√ºs√º √ßalƒ±≈ütƒ±rƒ±lƒ±r ve √ßƒ±ktƒ± dosyasƒ± yazdƒ±rƒ±lƒ±r. $ for j girdi 0 1 2 3 4 5 6 7 8 9; do grep -iH \u0026lt;ifade\u0026gt; *$j.seq; done #  verilen ifadeyi girdi \u0026gt; 10.000 dosyaya kadar arar ve ne kadar o ifade ge√ßtiƒüini yazdƒ±rƒ±r. $ for i in *.pep; do echo -e \u0026#34;$i\\n\\n17\\n33\\n\\n\\n\u0026#34; | ./program $i \u0026gt; $i.out; done # etkile≈üimli programƒ± √ßalƒ±≈ütƒ±rƒ±r ve girdi/√ßƒ±ktƒ± sorar.  Basit Perl Komutlarƒ± $ perl -p -i -w -e \u0026#39;s/ifade1/ifade2/g\u0026#39; girdi_dosyasƒ± # girdi dosyasƒ± i√ßerisindekileri verilen ifadelere g√∂re deƒüi≈üimini yapar. \u0026#39;-p\u0026#39; bu komut yedek bir dosya olu≈üturur  $ perl -ne \u0026#39;print if (/ifade1/ ? ($c=1) : (--$c \u0026gt; 0)) ; print if (/ifade2/ ? ($d = 1) : (--$d \u0026gt; 0))\u0026#39; girdi_dosyasƒ± \u0026gt; cƒ±ktƒ±_dosyasƒ± # ifade1 ve ifade2 i√ßeren satƒ±rlarƒ± ayrƒ±≈ütƒ±rƒ±r (parse eder.)  WGET (terminal √ºzerinden linki verilen dosya indirimini ger√ßekle≈ütirir.) $ wget ftp://ftp.itu.edu.tr.... # verilen linkteki dosya wget komutunun √ßalƒ±≈ütƒ±rƒ±ldƒ±ƒüƒ± dizine iner.  SCP (ƒ∞ki makine arasƒ±nda g√ºvenli kopyalama i≈ülemi saƒülar. ) Genel Kullanƒ±m. $ scp kopyalanacak_dosya kopyalanacak_yer # √ñrnekler Sunucudan dosya kopyalamak i√ßin (bilgisayarƒ±nƒ±zƒ±n terminalinden) $ scp kullanƒ±cƒ±@sunucu_ip:dosya_adƒ± . # \u0026#39;.\u0026#39; en sona nokta koyulmasƒ±, sunucu √ºzerindeki kopyalanacak dosyayƒ± bulunduƒüunuz yere kopyalamasƒ±nƒ± saƒülar.  Bilgisayarƒ±nƒ±zdan sunucuya kopyalama yapmak i√ßin. (Bilgisayar terminalinden) $ scp resim.jpg kullanƒ±cƒ±@sunucu_ip:~/belgeler/resimler/ Sunucu √ºzerinde bulunan klas√∂r√º bilgisayarƒ±mƒ±za kopyalamak i√ßin. (Bilgisayar terminalinden) $ scp -r kullanƒ±cƒ±@sunucu_ip:dizin/ ~/Masaustu Bilgisayar √ºzerinde bulunan klas√∂r√º sunucuya kopyalamak i√ßin. (Bilgisayar terminalinden) $ scp -r klas√∂r/ kullanƒ±cƒ±@sunucu_ip:dizin/ NFTP : (Dosya transfer i≈ülemlerinizi kolay ≈üekilde terminal √ºzerinden yapmanƒ±zƒ± saƒülar ) $ open ncftp $ ncftp\u0026gt; open sunucu_url # sunucuya baglantƒ± saƒülanƒ±yor.. $ ncftp\u0026gt; cd /root/Masaustu. # masaustune gecildi $ ncftp\u0026gt; get resimler.gz # masaustunde bulunan resimler.gz indirildi. $ ncftp\u0026gt; bye # gule gule mesajƒ± alƒ±ndƒ± ","permalink":"https://mrturkmen.com/posts/vim/","summary":"√ñzet Terminal √ºzerinden kullanƒ±lan en me≈ühur yazƒ± d√ºzenleme programƒ± VIM hakkƒ±nda komutlar ve bazƒ± kullanƒ±≈ülƒ± linux komutlarƒ±\nScript nasƒ±l yazƒ±lƒ±r.   Dosya olu≈üturulur ve dosyanƒ±n ba≈üƒ±na terminalin yolu eklenir #!/bin/bash\n  terminal komutlarƒ±nƒ± bu yolun altƒ±na yazƒ±nƒ±z.\n  daha sonra dosyanƒ±n iznini ayarlayƒ±nƒ±z. chmod +x dosya_ismi\n  terminal scriptini ≈üu ≈üekilde √ßalƒ±≈ütƒ±rƒ±labilirsiniz. ./dosya_ismi\n  √∂rnek bash scripti\n  !/bin/bash echo \u0026#34;Adƒ±nzƒ± giriniz: \u0026#34; read isim echo \u0026#34;Sifrenizi giriniz\u0026#34; read sifre if [[ ( $isim == \u0026#34;admin\u0026#34; \u0026amp;\u0026amp; $sifre == \u0026#34;random\u0026#34; ) ]]; then echo \u0026#34;Ba≈üarƒ±lƒ±\u0026#34; else echo \u0026#34;Ba≈üarƒ±sƒ±z\u0026#34; fi VIM Birden fazla dosya √ºzerinde √ßalƒ±≈ümak $ vim *.","title":"vim "},{"content":"√ñzet: Bu yazƒ±da linux ortamƒ±na biraz daha giri≈ü yaparak, linux ortamƒ±nda bulunan komutlar hakkƒ±nda kƒ±sa bilgilendirme yapƒ±lmasƒ± planlanmaktadƒ±r.\nGiri≈ü Neden Linux ?  Birden fazla i≈ülemi aynƒ± anda kolay ≈üekilde yapmanƒ±zƒ± saƒülar Uzaktan i≈ülemlerinizi halletmede b√ºy√ºk kolaylƒ±k saƒülar Birden fazla kullanƒ±cƒ± aynƒ± sunucuya eri≈üebilir Terminale, bir sistem √ºzerinde olan kaynaklara birden fazla eri≈üim m√ºmk√ºnd√ºr Aray√ºz olan sistemlere g√∂re daha performanslƒ±, Bedava , G√ºncel  Temeller Bu bilgilendirme dosyasƒ± i√ßin not   B√ºt√ºn komutlar b√ºy√ºk ve k√º√ß√ºk harfe duyarlƒ±dƒ±r.\n  \u0026quot;$\u0026quot;komutun ba≈ülangƒ±cƒ±nƒ± temsil eder.\n  \u0026quot;#\u0026quot; komutun sonunu temsil eder.\n  Windows bilgisayar √ºzerinden giri≈ü i√ßin  PuTTY : windows bilgisayar √ºzerinden SSH ile baglantƒ± saƒülamak i√ßin gereklidir. PuTTY programƒ±nda SSH ile baƒülantƒ± i√ßin sunucu IP adresi ve PORT numarasƒ±nƒ± bilmelisiniz.  Mac veya Linux Bilgisayarlardan Eri≈üim Bu t√ºr bilgisayarlar UNIX tabanlƒ± olduƒüundan dolayƒ± terminal √ºzerinden a≈üaƒüƒ±da verilen komutlarƒ± yazmanƒ±z yeterli olacaktƒ±r ekstradan herhangi bir programa gerek duyulmamaktadƒ±r.\n$ ssh \u0026lt;kullanƒ±cƒ±_adƒ±\u0026gt;@\u0026lt;sunucu_adresi(IP)\u0026gt; $ kullanƒ±cƒ±_adƒ±: ... $ ≈üifre: ... {% endhighlight %} #### Giri≈ü yaptƒ±ktan sonra ≈üifrenizi deƒüi≈ütirmek isterseniz ```bash $ passwd # bu komutu kullanabilirsiniz.  # bu komut sayesinde giri≈ü yapƒ±lan  # kullanƒ±cƒ± i√ßin yeni ≈üifre  # belirleyebilirsiniz. {% endhighlight %} #### Listeleme ```bash $ pwd # ≈üu anda bulunduƒüunuz konumu √ßƒ±ktƒ± olarak yazdƒ±rƒ±r $ ls # bulunduƒüunuz konumdaki dosyalarƒ± ve klas√∂rleri listeler $ ll # ls komutunun e≈üde≈üi olarak tanƒ±mlƒ± bir ifadedir genelde \u0026#34;ls -alF\u0026#34; olarak kayƒ±tlƒ±dƒ±r bash profilinde $ ll -R # dosyalar/klas√∂rler listelenir, klas√∂rler i√ßerisindeki dosyalarda listelenir $ ll -t # listeleme i≈ülemi kronolojik ≈üekilde ger√ßekle≈üir. $ stat \u0026lt;dosya_adƒ±\u0026gt; # dosyaya ait bilgileri meta bilgileri listeler $ whoami # sizin sistem tarafƒ±ndan kim olduƒüunuzu s√∂yler, yani kullanƒ±cƒ± adƒ±nƒ±zƒ± listeler $ hostname # baƒülƒ± olduƒüunu makinenin URL ni yada IP sini g√∂sterir Dosyalar ve Klas√∂rler $ mkdir \u0026lt;klas√∂r_adƒ±\u0026gt; # belirlenen isimde klas√∂r olu≈üturur $ cd \u0026lt;klas√∂r_adƒ±\u0026gt; # klas√∂r adƒ± tanƒ±mlanan klas√∂re gidersini. $ cd .. # √ºst klas√∂re gitmenizi saƒülar $ cd ../../ # iki √ºst klas√∂re gitmenizi saƒülar $ cd # ana klas√∂r√ºne gidersiniz $ rmdir \u0026lt;klas√∂r_adƒ±\u0026gt; # klas√∂r√º siler $ rm \u0026lt;dosya_adƒ±\u0026gt; # dosyayƒ± siler $ rm -r \u0026lt;klas√∂r_adƒ±\u0026gt; # klas√∂r√º ve i√ßerisindeki b√ºt√ºn dosyalarƒ± siler $ mv \u0026lt;dosyaadƒ±1\u0026gt; \u0026lt;dosyaadƒ±2\u0026gt; # isim deƒüi≈ütirmenizi saƒülar, name $ mv \u0026lt;dosyaadƒ±\u0026gt; \u0026lt;ta≈üƒ±nacak_yol\u0026gt; # dosyayƒ± belirtilen yere ta≈üƒ±r $ cp \u0026lt;dosyaadƒ±\u0026gt; \u0026lt;kopyalanacak_yol\u0026gt; # dosyayƒ± belirtilen yere kopyalar, eƒüer klas√∂r kopyalanacak ise -r parametresi eklenir.  Kƒ±sayollar $ . # sadece nokta bulunduƒüunuz dizini ifade eder $ ~/ # kullanƒ±cƒ±nƒ±n ana dizinini ifade eder $ history # yazmƒ±≈ü olduƒüunuz komutlarƒ±n kaydƒ±nƒ± tutar ve bu komut ile eri≈üebilirsiniz $ !\u0026lt;komut_sƒ±ralamasƒ±\u0026gt; # daha √∂nce yazmƒ±≈ü oldunuz komutu sƒ±ralamasƒ±nƒ±n numarasƒ±nƒ± vererek calƒ±≈ütƒ±rabilirsiniz. $ yukarƒ±(asagƒ±)_oklarƒ± # ge√ßmi≈ü komutlar arasƒ±nda gezmenizi saƒülar $ \u0026lt;tamamlanmamƒ±≈ü_yol_veya_dosyaadƒ±\u0026gt; TAB # Tab a bastƒ±ƒüƒ±nƒ±zda sistem otomatik tamamlama i≈ülemini ger√ßekle≈ütirir. $ \u0026lt;tamamlanmamƒ±≈ü komut\u0026gt; SHIFT\u0026amp;TAB # komutu otomatik tamamlar $ Ctrl a # imlecin en ba≈üa gitmesini saƒülar $ Ctrl e # imlecin en sona gitmesini saƒülar $ Ctrl d # imlec altƒ±ndaki karakteri siler $ Ctrl k # imlecin bulunduƒüu saƒülar $ Ctrl y # Ctrl k ile alƒ±nan i√ßerik yapƒ±≈ütƒ±rƒ±lƒ±r. Yardƒ±m Alma $ man # genel yardƒ±m $ man wc # wc komutu hakkƒ±nda yardƒ±m almanƒ±zƒ± saƒülar $ wc --help # wc komutu hakkƒ±nda yardƒ±m almanƒ±zƒ± saƒülar $ info wc # wc komutu hakkƒ±nda detaylƒ± bilgi almanƒ±z saƒülanƒ±r $ apropos wc # wc komutuna ait b√ºt√ºn yardƒ±m dosyalarƒ± g√ºncellenir. Aradƒ±ƒüƒ±nƒ±z Dosyayƒ± Bulma Y√∂ntemleri Arama yapmak $ find -name \u0026#34;*aramakistediƒüinizdesen*\u0026#34; # girdiƒüiniz desene g√∂re bulunduƒüunuz dizinde arama yapmanƒ±zƒ± saƒülar. $ find /usr/local -name \u0026#34;*klas*\u0026#34; # isminin i√ßerisinde klas ge√ßen dosyalarƒ± ve klaks√∂rleri listeler. $ find /usr/local -iname \u0026#34;*klas*\u0026#34; # yukarƒ±daki komutuna benzer ≈üekilde, isminin i√ßerisinde klas ge√ßen dosyalarƒ± ve klas√∂rleri listeler fakat bu durumda b√ºy√ºk veya k√º√ß√ºk harfte olmasƒ± dikkate alƒ±nmaz $ find ~ -type f -mtime -2 # 2 g√ºn i√ßerisinde deƒüi≈ütirilmi≈ü b√ºt√ºn dosyalarƒ± listeler $ locate \u0026lt;aramakistediƒüinizdesen\u0026gt; # aradƒ±ƒüƒ±nƒ±z dosyayƒ± veya klas√∂r√º sistem genelinde arar. $ which \u0026lt;uygulama_adƒ±\u0026gt; # uygulamanƒ±n nerede bulunduƒüunu g√∂sterir $ whereis \u0026lt;uygulama_adƒ±\u0026gt; # uygulamanƒ±n √ßalƒ±≈ütƒ±rƒ±labilir dosyasƒ±nƒ±n yerini g√∂sterir $ dpkg -l | grep aramakistediƒüinizpaketismi # Debian paketleri i√ßerisinde arama yaparak verilen desende bulunan paketleri listel Dosya i√ßerisinde arama yapmak $ grep aranan_kelime dosya # dosya i√ßerisinde aranan kelimenin nerelerde ge√ßtiƒüini size aktarƒ±r $ grep -H aranan_kelime # -H √ßƒ±ktƒ± dosyasƒ±nƒ± aranan kelimenin √∂n√ºne koyar $ grep \u0026#39;aranan_kelime\u0026#39; dosya | wc # burada iki komut birle≈ütirilmi≈ütir, yani grep komutunun √ßƒ±ktƒ±sƒ± wc komutuna girdi olmaktadir yani aranan kelime verilen dosyada 5 kere ge√ßiyor ise bu durumda sonuc 5 olarak d√∂nmektedir.  $ find /home/kullanƒ±cƒ±_adƒ± -name \u0026#39;*.txt\u0026#39; | xargs grep -c ^.* # verilen dizinde txt dosyalarƒ±nƒ± bularak bu dosyalar i√ßerisindeki satƒ±r sayƒ±sƒ±nƒ± hesaplayarak √ßƒ±ktƒ± vermektedir.  ƒ∞zinler \u0026amp; Hak Sahipliƒüi $ ls -al # bu komut √ßalƒ±≈ütƒ±rƒ±ldƒ±gƒ±nda buna benzer bir √ßƒ±ktƒ± g√∂r√ºnebilir : drwxrwxrwx Burada bahsi ge√ßen harflerin anlamlarƒ± a≈üaƒüƒ±daki gibi verilebilir.\n d: dizin rwx: oku, yaz, √ßalƒ±≈ütƒ±r ilk √º√ßl√º (rwx) : kullanƒ±cƒ± izinleri(u) ikinci √º√ßl√º: grup izinleri (g) √º√ß√ºnc√º √º√ßl√º: diƒüer izinleri (o) ifade etmektedir. En ba≈üta d olursa bu o dosyanƒ±n aslƒ±nda bir klas√∂r olduƒüunu ifade etmektedir.  Kullanƒ±cƒ± ve grupa, yazma ve √ßalƒ±≈ütƒ±rma izni vermek:\n$ chmod ug+rx dosya_ismi Kullanƒ±cƒ± haklarƒ±nƒ±n alƒ±nmasƒ± $ chmod ugo-rwx dosya_ismi \u0026#39;+\u0026#39; izin eklemeyi saƒülar \u0026#39;-\u0026#39; izin silmenizi saƒülar $ chmod +rx dosya_ismi/ VEYA $ chmod 755 dosya_ismi/ Hak sahipliƒüinin deƒüi≈ütirilmesi $ chown \u0026lt;kullanƒ±cƒ±_adƒ±\u0026gt; \u0026lt;dosya veya klas√∂r\u0026gt; # kullanƒ±cƒ± hak sahibini deƒüi≈ütirir $ chgrp \u0026lt;grup\u0026gt; \u0026lt;dosya veya klas√∂r\u0026gt; # grup hak sahibini deƒüi≈ütirir $ chown \u0026lt;kullanƒ±cƒ±_adƒ±\u0026gt;:\u0026lt;grup\u0026gt; \u0026lt;dosya veya klas√∂r\u0026gt; # kullanƒ±cƒ± ve grup hak sahipliƒüini deƒüi≈ütirir Kullanƒ±≈ülƒ± Linux Komutlarƒ± $ df # sistem diskinin ne kadar dolu ve bo≈ü olduƒüu bilgisini g√∂sterir $ free # ne kadar √∂nbellek (RAM) alanƒ±nƒ±n bo≈ü/dolu olduƒüunu g√∂sterir $ uname -a # i≈ületim sistemine ait temel bilgileri g√∂sterir $ bc # terminal √ºzerinden hesap makinesi kullanmanƒ±zƒ± saƒülar $ /sbin/ifconfig # sunucunun aƒü bilgilerini listeler. $ ln -s orjinal_dosyaismi yeni_dosyaismi # orjinal dosyaya link olu≈üturur $ du -sh # bulunduƒüunuz konumdaki disk kullanƒ±m bilgilerini listeler $ du -sh * # bulunduƒüunuz konumdaki dosyalarƒ±n/klas√∂rlerin kullanƒ±m bilgilerini g√∂sterir $ du -s * | sort -nr # sƒ±ralanmƒ±≈ü ≈üekilde dosyalarƒ±n/klas√∂rlerin kullanƒ±mlarƒ±nƒ± listeler ƒ∞≈ülem Y√∂netimi $ who # sisteme kimin girdiƒüini g√∂sterir $ w # sistemde kimlerin olduƒüunu g√∂sterir $ ps # arka planda √ßalƒ±≈üan i≈ülemler hakkƒ±ndaki bilgileri listeler. $ ps -e # sistemdeki b√ºt√ºn i≈ülemleri listeler $ ps aux | grep \u0026lt;kullanƒ±cƒ±_adƒ±\u0026gt; # kullanƒ±cƒ±ya ait √ßalƒ±≈ütƒ±rƒ±lan i≈ülemleri listeler $ top # CPU ve RAM deƒüerlerinin kullanƒ±m bilgilerini g√∂sterir $ mtop # birden fazla CPU i√ßin top komutunun yaptƒ±ƒüƒ±nƒ± yapar $ Ctrl z \u0026lt;enter\u0026gt; bg or fg \u0026lt;enter\u0026gt; # √ßalƒ±≈üan i≈ülemleri durdurur, arka plana atar (bg) veya √∂n plana getirir (fg) $ Ctrl c # yeni ba≈ülamƒ±≈ü olan i≈ülemi durdurur $ kill \u0026lt;i≈ülem_no\u0026gt; # belirlenen i≈ülemi sonlandƒ±rƒ±r, i≈ülem ID sine g√∂re belirlenir $ renice -n \u0026lt;√∂nemlilik_deƒüeri\u0026gt; # i≈ülemin √∂nemlilik deƒüerini deƒüi≈ütirmenizi saƒülar  Text dosyalarƒ±nƒ± okumak $ less \u0026lt;dosya_ismi\u0026gt; # belirtilen dosyayƒ± terminal √ºzerinden okumanƒ±zƒ± saƒülar G :dosyanƒ±n sonuna gider, g : dosyanƒ±n ba≈üƒ±na gider.  $ more \u0026lt;dosya_ismi\u0026gt; # dosya i√ßeriƒüini g√∂sterir √ßƒ±kmak i√ßin q ya basƒ±lmasƒ± gereklidir. $ cat \u0026lt;dosya_ismi\u0026gt; #dosya i√ßeriƒüini terminale yazdƒ±rƒ±r Metin D√ºzenleyicileri VI ve VIM Terminal tabanlƒ± g√º√ßl√º metin d√ºzenleyicidir. Vi genelde linux tabanlƒ± b√ºt√ºn sistemlerde mevcuttur, vim, vi nin geli≈ümi≈üidir.\nEMACS Grafik tabanlƒ± metin d√ºzenleyicidir. Bu d√ºzenleyiciye ait olan klavye d√ºzeni hakkƒ±nda bilginiz olmalƒ±dƒ±r. B√ºt√ºn linux ve unix tabanlƒ± sistemlerde mevcuttur.\nXEMACS EMACS in √ßok daha geli≈ümi≈üidir, yazƒ±m hatalarƒ±, web ve metini iyi bir ≈üekilde kontrol etmek m√ºmk√ºnd√ºr fakat normalde y√ºklenmi≈ü olmaz.\nPICO Terminal tabanlƒ± basit metin d√ºzenleyicidir, buna ait klavye d√ºzeni bilinmelidir.\nVIM Temelleri Temeller $ vim dosya_ismi # dosya_ismin de dosya olu≈üturur veya yazma modunda a√ßar $ i # vim \u0026#39;in i√ßerisine girdikten sonra i a√ßƒ±k olan dosyaya bir ≈üeyler yazmanƒ±za olanak saƒülar. $ ESC # dosya d√ºzenleme modundan √ßƒ±kƒ±lƒ±r $ : # vim i√ßerisinde kullanacaƒüƒ±nƒ±z komutlar : ile ba≈ülar $ :w # vim i√ßerisinde :w yaptƒ±ƒüƒ±nƒ±zda yazdƒ±ƒüƒ±nƒ±zƒ± kaydeder. $ :q # bu komut vim den √ßƒ±kmanƒ±zƒ± saƒülar $ :q! # hi√ß bir≈üeyi kaydetmeden √ßƒ±kmanƒ±zƒ± saƒülar. $ :wq # kaydederek √ßƒ±kar $ R # vim i√ßerisinde √∂zellik deƒüi≈ütirmenizi saƒülar $ r # imlecin bulunduƒüu karakteri deƒüi≈ütirmenizi saƒülar $ q: # vim i√ßerisinde yazdƒ±ƒüƒ±nƒ±z komutlarƒ±n kaydƒ±nƒ± g√∂sterir $ :w yeni_dosyaadƒ± # yeni dosyaya kaydeder. $ :#,#w yeni_dosyaadƒ±# belirlenen (#,#) aralƒ±ktaki metini yeni dosyaya kaydeder. $ :# belirlenen (#) satƒ±ra gitmenizi saƒülar Yardƒ±m $ vimtutor # vim i√ßerisinde nasƒ±l √ßalƒ±≈ütƒ±ƒüƒ±na dair bilgileri i√ßeren tur ba≈ülatƒ±lƒ±r. $ :help # vim i√ßerisinde yardƒ±m a√ßar, √ßƒ±kmak i√ßin q komutu kullanƒ±lƒ±r. $ :help \u0026lt;konu\u0026gt; # belirlenen konu hakkƒ±nda yardƒ±m a√ßar $ :help \u0026lt;konu\u0026gt; CTRL-D # belirlenen konunun ge√ßtiƒüi b√ºt√ºn yardƒ±m d√∂k√ºmanƒ±nƒ± listeler. $ :\u0026lt;yukarƒ±-asagƒ± tuslarƒ±\u0026gt; # daha √∂nceki yaptƒ±ƒüƒ±nƒ±z komutlar arasƒ±nda gezmenizi saƒülar. Dosya i√ßerisinde gezme (vim i√ßerisinde) $ $ # bulunduƒüunuz satƒ±rƒ±n en sonuna gider $ A # bulunduƒüunuz satƒ±rƒ±n en sonuna yazma modunu a√ßarak gider $ 0 (sƒ±fƒ±r) # satƒ±rƒ±n ba≈ülangƒ±cƒ±na gider $ CTRL-g # imlecin nerede olduƒüu ve o satƒ±r hakkƒ±nda bilgi verir $ SHIFT-G # imleci dosyanƒ±n en sonuna getirir G√∂r√ºnt√º (vim i√ßerisinde) WRAPPING AND LINE NUMBERS $ :set nowrap # kelimelerin kaymamalarƒ±nƒ± saƒülar $ :set number # satƒ±r numaralarƒ±nƒ± g√∂sterir Ar≈üivleme ve Sƒ±kƒ±≈ütƒ±rma $ tar -cvf dosya_ismi.tar klas√∂r/ # verilen klas√∂r i√ßin ar≈üiv olu≈üturur $ tar -czvf dosya_ismi.tgz klas√∂r/ # verilen klas√∂r i√ßin ar≈üivlenmi≈ü ve sƒ±kƒ±≈ütƒ±rƒ±lmƒ±≈ü dosya olu≈üturur.  Ar≈üivleri g√∂r√ºnt√ºleme $ tar -tvf dosya_ismi.tar $ tar -tzvf dosya_ismi.tgz √áƒ±kartma $ tar -xvf dosya_ismi.tar $ tar -xzvf dosya_ismi.tgz $ gunzip dosya_ismi.tar.gz $ tar zxf blast.linux.tar.Zs Basit y√ºkleme i≈ülemleri RPM y√ºklemeleri $ rpm -i uygulama_ismi.rpm $ rpm --query \u0026lt;paket_ismi\u0026gt; ## RPM versiyonunu kontrol etme i√ßin Debian paketlerinin y√ºklenmesi $ apt-cache search nmap # nmap adƒ±ndaki uygulamayƒ± debian deposundan arama yapar $ apt-cache show nmap # nmap hakkƒ±nda tanƒ±mlamayƒ±(bilgi) g√∂sterir $ apt-get install nmap # nmap \u0026#39;i sisteme kurar. $ apt-get update # sistemdeki uygulamalarƒ± ve servisleri g√ºnceller $ apt-get upgrade -u # uygulamalarƒ±n yeni versiyonu var ise y√ºkseltme yapar $ dpkg -i dosya.deb # indirilen debian dosyasƒ±nƒ±n y√ºklenmesini saƒülar $ aptitude # apt-get ile aynƒ± i≈ülemi g√∂r√ºr $ aptitude search vim # vim programƒ±nƒ± debian deposunda arar  Cihazlar Takma /√áƒ±karma usb/floppy/cdrom $ mount /media/usb $ umount /media/usb $ mount /media/cdrom $ eject /media/cdrom $ mount /media/floppy √áevresel Deƒüi≈ükenler $ xhost user@host # kullanƒ±cƒ± i√ßin √ßalƒ±≈ütƒ±rma izini ekler $ echo DISPLAY # ekranƒ±n ayarlarƒ±nƒ± g√∂sterir $ export (setenv) DISPLAY=\u0026lt;lokal_IP\u0026gt;:0 # g√∂r√ºnt√º deƒüi≈ükeninin deƒüerini deƒüi≈ütirir $ unsetenv DISPLAY # g√∂r√ºnt√º deƒüi≈ükenini siler $ printenv # kullanƒ±lan √ßevresel deƒüi≈ükenleri listeler $ $PATH # terminal √ºzerinde programlarƒ±n √ßalƒ±≈ümasƒ±nƒ± saƒülayan yollarƒ± g√∂sterir. ","permalink":"https://mrturkmen.com/posts/linux-temeller-1/","summary":"√ñzet: Bu yazƒ±da linux ortamƒ±na biraz daha giri≈ü yaparak, linux ortamƒ±nda bulunan komutlar hakkƒ±nda kƒ±sa bilgilendirme yapƒ±lmasƒ± planlanmaktadƒ±r.\nGiri≈ü Neden Linux ?  Birden fazla i≈ülemi aynƒ± anda kolay ≈üekilde yapmanƒ±zƒ± saƒülar Uzaktan i≈ülemlerinizi halletmede b√ºy√ºk kolaylƒ±k saƒülar Birden fazla kullanƒ±cƒ± aynƒ± sunucuya eri≈üebilir Terminale, bir sistem √ºzerinde olan kaynaklara birden fazla eri≈üim m√ºmk√ºnd√ºr Aray√ºz olan sistemlere g√∂re daha performanslƒ±, Bedava , G√ºncel  Temeller Bu bilgilendirme dosyasƒ± i√ßin not   B√ºt√ºn komutlar b√ºy√ºk ve k√º√ß√ºk harfe duyarlƒ±dƒ±r.","title":"debian: terminal/komut "},{"content":"√ñzet: Bu kƒ±sa yazƒ±mƒ±zda linux bilgisayarlarƒ±nƒ±n terminali √ºzerinden yapabileceƒüiniz basit i≈ülemlere dair bilgiler verilecektir.\nLinux tabanlƒ± sunucularda/bilgisayarda terminal √ºzerinden kopyalama Kopyalama i≈ülemi \u0026ldquo;cp\u0026rdquo; komutu ile yapƒ±lmaktadƒ±r, bu komuta ait format a≈üaƒüƒ±daki gibi √∂zetlenebilir.\ncp [parametreler] [kopyalanacak-dosya] [kopyalanmasi-hedeflenen-yer] Bu komutun kullanƒ±mƒ±na √∂rnek verelim, kopyalanacak dizin ve kopyalanmasƒ± gereken dosya ;\nKopyalanacak dizin : /home/geek/Masaustu/\nKopyalanacak dosya : /home/geek/Dokumanlar/resim.png\nBu durumda komut : (* Dizinlere ve dosyalara eri≈üim hakkƒ±na sahip olduƒüunuzdan emin olunuz)\ncp /home/geek/Dokumanlar/resim.png /home/geek/Masaustu/ Eƒüer bir dizin i√ßerisindeki b√ºt√ºn dosyalar kopyalanmasƒ± planlanƒ±yor ise -R parametresi kullanƒ±lmasƒ± gerekmektedir. Diyelim ki bir dizin i√ßerisinde 1.png, 2.png, 3.png \u0026hellip; 12.png gibi dosyalar var ise ve bu dizinin adƒ± \u0026ldquo;resimler\u0026rdquo; ise, resimler dizisi istenilen diziye a≈üaƒüƒ±daki komut yardƒ±mƒ± ile kopyalanabilir.\ncp -R /home/geek/Dokumanlar/resimler /home/geek/Masaustu/ Bu kƒ±sƒ±mda b√ºt√ºn veri hedeflenen dizine aktarƒ±lmaktadƒ±r.\ncp komutuna ait bazƒ± parametreler ve onlarƒ±n kƒ±sa a√ßƒ±klamalarƒ±:\n-R : verilen dizindeki b√ºt√ºn dosyalarƒ± hedeflenen dizine kopyalamak i√ßin gereklidir.\n-p : kopyalama yaparken dosyaya ait olan, olu≈üturulma, deƒüi≈ütirme, sahiplik bilgilerini deƒüi≈ütirmeden onlar ile birlikte kopyalamak i√ßin gereklidir.\nTerminal √ºzerinden yeniden ba≈ülatma Linux terminali √ºzerinden bir sunucuyu yeniden ba≈ülatmak veya kapatmak i√ßin gerekli olan komutlar ≈üu ≈üekilde √∂zetlenebilir. Bu komutlarƒ±n √ßalƒ±≈ümasƒ± i√ßin sunucu √ºzerinde y√∂netici (root) yetkilerine sahip olmalƒ±sƒ±nƒ±z.\nYeniden ba≈ülatmak i√ßin\nreboot Bu kƒ±sƒ±mda \u0026quot;Permission denied\u0026quot; veya buna benzer bir izin reddedildi mesajƒ± aldƒ±ƒüƒ±nƒ±zda aynƒ± komutu sudo eki koyarak denemelisiniz, y√∂netici yetkisi olmadƒ±ƒüƒ± durumda bu t√ºr mesajlarƒ± alƒ±rsƒ±nƒ±z.\nsudo reboot Bu komut bazen her linux daƒüƒ±lƒ±mƒ± i√ßin ge√ßerli olmayabilir bu durumda a≈üaƒüƒ±da verilen komut reboot komutuna alternatif olarak verilebilir. (* Bu komut MacOS bilgisayarlar i√ßinde kullanƒ±labilir, MacOS i≈ületim sistemleri Hybrid bir yapƒ±ya sahip olduƒüundan dolayƒ± Unix √ßekirdeƒüi i√ßermektedir.)\nshutdown -r now \u0026quot;shutdown\u0026quot; komutunda isterseniz bekleme s√ºresi ekleyebilirsiniz b√∂ylece sunucu veya bilgisayar, belirlenen bekleme s√ºresi sonunda verdiƒüiniz komutu uygulayacaktƒ±r.\nshutdown -r +30 30 dk sonrasƒ±nda sunucu yeniden ba≈ülatƒ±lacaktƒ±r, benzer ≈üekilde dk belirlemektense, istediƒüiniz saatte bu i≈ülemi yapmak isterseniz, istediƒüiniz saati a≈üaƒüƒ±daki formatta ayarlanabilir.\nshutdown -r 19:30 Sunucu, saat 19:30 da yeniden ba≈ülatƒ±lmaya ayarlanmƒ±≈ütƒ±r.\n\u0026quot;shutdown\u0026quot; komutuna ait bazƒ± parametreler ve a√ßƒ±klamalarƒ± ≈üu ≈üekilde √∂zetlenebilir.\n-r : sunucuya yeniden ba≈ülatma sinyalini vermesi i√ßin gereklidir.\n-k : sunucuya baƒülƒ± olan fakat y√∂netici yetkisinde eri≈ümemi≈ü ki≈üileri sunucudan atar.\n","permalink":"https://mrturkmen.com/posts/linux-terminalinden-basit-komutlar/","summary":"√ñzet: Bu kƒ±sa yazƒ±mƒ±zda linux bilgisayarlarƒ±nƒ±n terminali √ºzerinden yapabileceƒüiniz basit i≈ülemlere dair bilgiler verilecektir.\nLinux tabanlƒ± sunucularda/bilgisayarda terminal √ºzerinden kopyalama Kopyalama i≈ülemi \u0026ldquo;cp\u0026rdquo; komutu ile yapƒ±lmaktadƒ±r, bu komuta ait format a≈üaƒüƒ±daki gibi √∂zetlenebilir.\ncp [parametreler] [kopyalanacak-dosya] [kopyalanmasi-hedeflenen-yer] Bu komutun kullanƒ±mƒ±na √∂rnek verelim, kopyalanacak dizin ve kopyalanmasƒ± gereken dosya ;\nKopyalanacak dizin : /home/geek/Masaustu/\nKopyalanacak dosya : /home/geek/Dokumanlar/resim.png\nBu durumda komut : (* Dizinlere ve dosyalara eri≈üim hakkƒ±na sahip olduƒüunuzdan emin olunuz)","title":"debian: cp/reboot komularƒ± "}]