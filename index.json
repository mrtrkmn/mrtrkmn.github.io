[{"content":" THE REPOSITORY: https://github.com/mrtrkmnhub/ubuntu-packer\nIn this blog post, provisioning and customizing images using packer will be shown with a template repository.\nIf you are asking or wondering what is Packer, the official definition is :\n Packer is a free and open source tool for creating golden images for multiple platforms from a single source configuration. (From Official Website).\n This post includes provisioning of ubuntu image on AWS and local.\nBuild Custom Ubuntu 20.04 LTS on Local In an ideal repository of Packer template, it would be nice to have a skeleton where it includes uploads, http, scripts folders along packer configuration file with a readme. Overall, the structure of folder might look like this :\nâ”œâ”€â”€ http â”‚ â””â”€â”€ preseed.cfg # required to change defualt values of ubuntu image â”œâ”€â”€ readme.md # readme file to have instructions about what to do â”œâ”€â”€ scripts # scripts/ dir, includes scripts to run on custom image â”‚ â”œâ”€â”€ cleanup.sh # cleans up /tmp  â”‚ â”œâ”€â”€ install_tools.sh # installs custom tools â”‚ â””â”€â”€ setup.sh # setting up config in system wise â”œâ”€â”€ ubuntu-20.04.json # packer config for ubuntu 20.04 â””â”€â”€ uploads # directory to upload files to custom image  â””â”€â”€ .gitkeep In this setup, http/preseed.cfg defines answers to the questions which may be asked during installation of Ubuntu operating system. More information regarding to preseed.cfg file can be checked from its wiki\n  scripts folder composed of bash scripts, chef, ansible or any other installer configuration files or scripts which will install customized tools and define settings of ubuntu image.\n  uploads folder includes all files, deb packages, or any other files which will be copied to image which will be inside customized image.\n  Anatomy of Packer Configuration File Any packer file composed of three main components which are ;\nBuilders Define the desired platform and platform configurations, including API Key information and desired source images. Example snippet is given from the Packer file:\n\u0026#34;builders\u0026#34;: [ { \u0026#34;boot_command\u0026#34;: [ \u0026#34;\u0026lt;esc\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;esc\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;/install/vmlinuz\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; auto\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/ask_detect=false\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/layoutcode=us\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/modelcode=pc105\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; debconf/frontend=noninteractive\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; debian-installer=en_US\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; fb=false\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; initrd=/install/initrd.gz\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; kbd-chooser/method=us\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; keyboard-configuration/layout=USA\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; keyboard-configuration/variant=USA\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; locale=en_US\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; netcfg/get_domain=vm\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; netcfg/get_hostname=ubuntu\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; grub-installer/bootdev=/dev/sda\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; noapic\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; -- \u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026lt;wait\u0026gt;\u0026#34; ], \u0026#34;boot_wait\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ova\u0026#34;, \u0026#34;disk_size\u0026#34;: 25240, \u0026#34;guest_additions_path\u0026#34;: \u0026#34;VBoxGuestAdditions_{{.Version}}.iso\u0026#34;, \u0026#34;guest_os_type\u0026#34;: \u0026#34;Ubuntu_64\u0026#34;, \u0026#34;headless\u0026#34;: true, \u0026#34;http_directory\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;iso_checksum\u0026#34;: \u0026#34;sha256:f11bda2f2caed8f420802b59f382c25160b114ccc665dbac9c5046e7fceaced2\u0026#34;, \u0026#34;iso_urls\u0026#34;: [ \u0026#34;iso/ubuntu-20.04.1-legacy-server-amd64.iso\u0026#34;, \u0026#34;https://cdimage.ubuntu.com/ubuntu-legacy-server/releases/20.04/release/ubuntu-20.04.1-legacy-server-amd64.iso\u0026#34; ], \u0026#34;shutdown_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39;|sudo -S shutdown -P now\u0026#34;, \u0026#34;ssh_password\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;ssh_port\u0026#34;: 22, \u0026#34;ssh_timeout\u0026#34;: \u0026#34;10000s\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;virtualbox-iso\u0026#34;, \u0026#34;vboxmanage\u0026#34;: [ [ \u0026#34;modifyvm\u0026#34;, \u0026#34;{{.Name}}\u0026#34;, \u0026#34;--memory\u0026#34;, \u0026#34;2048\u0026#34; ], [ \u0026#34;modifyvm\u0026#34;, \u0026#34;{{.Name}}\u0026#34;, \u0026#34;--cpus\u0026#34;, \u0026#34;1\u0026#34; ] ], \u0026#34;virtualbox_version_file\u0026#34;: \u0026#34;.vbox_version\u0026#34;, \u0026#34;vm_name\u0026#34;: \u0026#34;ubuntu_vm_ubuntu_20_{{timestamp}}\u0026#34; } ] In the builders config, we are defining some set of keys in JSON file, which are very obvious from its name, we are considering to build image locally. All the keys are important in given builders config however most important and might need to update time to time is iso_urls which are the places where packer download iamges and customize it according to your scripts. Another crucial key is to have headless value true which means that there will be no GUI running when packer command is executed to run the Packer JSON file.\nProvisioner Defines how to configure the image most likely by your using existing configuration management tools like Ansible, Chef, Puppet or pure bash scripts.\nIn our example, bash scripts will be provided to install tools and update configuration of ubuntu image to make it customized. Provisioner section of a Packer JSON file can be seen as below:\n\u0026#34;provisioners\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;file\u0026#34;, \u0026#34;source\u0026#34;:\u0026#34;uploads\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/home/ubuntu\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/install_tools.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/setup.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/cleanup.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; } ] Here we are defining existing bash scripts in order to execute in the process of customizing Ubuntu image. The steps under provisioners are pretty clear.\n  The content of uploads file will be uploaded to home directory /home/ubuntu\n  In second step, install_tools.sh will be executed and other steps will be followed in order.\n  Post Processors Related to the builder, runs after the image is built, it is generally used to generate or apply artifacts. In this example, it is not required however more information can be found here: post processors\nCommunicator How packer works on the machine image during the creation. By default it is over SSH communication and it does not need to be defined explicitly. More information can be found here: communicator\nOver all packer file can be seen as follow:\n{ \u0026#34;builders\u0026#34;: [ { \u0026#34;boot_command\u0026#34;: [ \u0026#34;\u0026lt;esc\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;esc\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;/install/vmlinuz\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; auto\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/ask_detect=false\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/layoutcode=us\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; console-setup/modelcode=pc105\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; debconf/frontend=noninteractive\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; debian-installer=en_US\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; fb=false\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; initrd=/install/initrd.gz\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; kbd-chooser/method=us\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; keyboard-configuration/layout=USA\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; keyboard-configuration/variant=USA\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; locale=en_US\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; netcfg/get_domain=vm\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; netcfg/get_hostname=ubuntu\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; grub-installer/bootdev=/dev/sda\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; noapic\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg\u0026lt;wait\u0026gt;\u0026#34;, \u0026#34; -- \u0026lt;wait\u0026gt;\u0026#34;, \u0026#34;\u0026lt;enter\u0026gt;\u0026lt;wait\u0026gt;\u0026#34; ], \u0026#34;boot_wait\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ova\u0026#34;, \u0026#34;disk_size\u0026#34;: 25240, \u0026#34;guest_additions_path\u0026#34;: \u0026#34;VBoxGuestAdditions_{{.Version}}.iso\u0026#34;, \u0026#34;guest_os_type\u0026#34;: \u0026#34;Ubuntu_64\u0026#34;, \u0026#34;headless\u0026#34;: true, \u0026#34;http_directory\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;iso_checksum\u0026#34;: \u0026#34;sha256:f11bda2f2caed8f420802b59f382c25160b114ccc665dbac9c5046e7fceaced2\u0026#34;, \u0026#34;iso_urls\u0026#34;: [ \u0026#34;iso/ubuntu-20.04.1-legacy-server-amd64.iso\u0026#34;, \u0026#34;https://cdimage.ubuntu.com/ubuntu-legacy-server/releases/20.04/release/ubuntu-20.04.1-legacy-server-amd64.iso\u0026#34; ], \u0026#34;shutdown_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39;|sudo -S shutdown -P now\u0026#34;, \u0026#34;ssh_password\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;ssh_port\u0026#34;: 22, \u0026#34;ssh_timeout\u0026#34;: \u0026#34;10000s\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;virtualbox-iso\u0026#34;, \u0026#34;vboxmanage\u0026#34;: [ [ \u0026#34;modifyvm\u0026#34;, \u0026#34;{{.Name}}\u0026#34;, \u0026#34;--memory\u0026#34;, \u0026#34;2048\u0026#34; ], [ \u0026#34;modifyvm\u0026#34;, \u0026#34;{{.Name}}\u0026#34;, \u0026#34;--cpus\u0026#34;, \u0026#34;1\u0026#34; ] ], \u0026#34;virtualbox_version_file\u0026#34;: \u0026#34;.vbox_version\u0026#34;, \u0026#34;vm_name\u0026#34;: \u0026#34;ubuntu_vm_ubuntu_20_{{timestamp}}\u0026#34; } ], \u0026#34;provisioners\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;file\u0026#34;, \u0026#34;source\u0026#34;:\u0026#34;uploads\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/home/ubuntu\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/install_tools.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/setup.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; }, { \u0026#34;execute_command\u0026#34;: \u0026#34;echo \u0026#39;ubuntu\u0026#39; | {{.Vars}} sudo -S -E bash \u0026#39;{{.Path}}\u0026#39;\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;scripts/cleanup.sh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; } ], \u0026#34;variables\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;0.1\u0026#34; } } How to run locally This file can be run from the place where ubuntu-20.04.json file is located.\n$ packer build ubuntu-20.04.json It will start to build custom image by installing tools which are defined under scripts and configure username and password according to preseed.cfg and setup.sh files.\nBuild Custom Ubuntu 20.04 LTS on Cloud It is more practical and preferrable to use if you already have an cloud option to consider. This packer configuration will create custom image directly on cloud and save it to AMIs to your AWS account.\nThe anatomy of packer files is similar, only section which needs to be changed compared to local one, is builders section. It is defining all required AWS variables and AMIs to customize.\nAs an cloud example AWS will be used to create custom image.\nBuilders on Cloud \u0026#34;builders\u0026#34;: [ { \u0026#34;type\u0026#34;:\u0026#34;amazon-ebs\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;{{user `aws_region`}}\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;{{user `aws_access_key`}}\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;{{user `aws_secret_key`}}\u0026#34;, \u0026#34;subnet_id\u0026#34;: \u0026#34;{{user `aws_subnet_id`}}\u0026#34;, \u0026#34;security_group_id\u0026#34;: \u0026#34;{{user `aws_security_group`}}\u0026#34;, \u0026#34;source_ami_filter\u0026#34;: { \u0026#34;filters\u0026#34;: { \u0026#34;virtualization-type\u0026#34;: \u0026#34;hvm\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ubuntu/images/*ubuntu-focal-20.04-amd64-server-*\u0026#34;, \u0026#34;root-device-type\u0026#34;: \u0026#34;ebs\u0026#34; }, \u0026#34;owners\u0026#34;: [\u0026#34;099720109477\u0026#34;], \u0026#34;most_recent\u0026#34;: true }, \u0026#34;instance_type\u0026#34;: \u0026#34;{{user `instance_type`}}\u0026#34;, \u0026#34;ssh_username\u0026#34;:\u0026#34;ubuntu\u0026#34;, \u0026#34;ami_name\u0026#34;: \u0026#34;ubuntu-ami-custom_{{timestamp}}\u0026#34; } ] In this configuration, all keys are important to consider, however there are some which are crucial and required to run it. More information about the keys can be found here: Amazon AMI Builder\nWe would like to create a custom Ubuntu-20.04 image on cloud and save it as AMI to run it later, we are searching its pattern from available AMIs on AWS Management Console or it can be found through out this website : https://cloud-images.ubuntu.com/locator/ec2/\nOnce you have declared which AMI to customize, it needs to be located under source_ami_filter with wildcards and owners. Setting most_recent to true means that when this Packer JSON file is executed it will fetch and customize last updated AMI.\nAccess Key, Secret Key are required and should not be exposed to public in any moment, if exposed, they need to be updated immediately. They will be used to communicate with AWS to fire up instances to create custom image according to given settings defined in builders and provisioners.\nThe values of keys are defined in variables and parsed from out of it.\n\u0026#34;variables\u0026#34;: { \u0026#34;aws_access_key\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;aws_secret_key\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;aws_region\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;aws_vpc\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;aws_subnet\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ami_name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ami_description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;builder_name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;username\u0026#34;:\u0026#34;ubuntu\u0026#34;, \u0026#34;instance_type\u0026#34;:\u0026#34;t2.medium\u0026#34;, \u0026#34;tarball\u0026#34;: \u0026#34;\u0026#34; }, In variables section, username, instance_type, aws_access_key, aws_secret_key variables should be set correctly to create the image on cloud. Other variables are optional and variables section can be populated more.\nCustomize settins on Cloud On cloud builds, cloud configuration file should be used instead of preseed.cfg to customize settings. The defaults.cfg file where it contains custom settings such as default username, password, changing visudo file and more. Example defaults.cfg can be as follow:\n#cloud-config system_info: default_user: name: ubuntu sudo: [\u0026#34;ALL=(ALL) NOPASSWD:ALL\u0026#34;] lock_passwd: false plain_text_passwd: \u0026#39;ubuntu\u0026#39; More information regarding to defaults.cfg file can be found here and customized more: https://cloudinit.readthedocs.io/en/latest/topics/examples.html\nHow to run Once variables are set, it can be run in same way with the local one.\n$ packer build aws_packer.json Complete packer JSON file : aws_packer.json\nAs a summary, Packer is really cool tool to use to automate the process of creating custom images and it can be used for Dockers as well. For local example in this post, it will produce OVA file to import, on cloud it will generate custom AMI under your AWS account.\nAll scripts and config files can be found in this repository: https://github.com/mrtrkmnhub/ubuntu-packer\n","permalink":"/posts/build-with-packer/","summary":"THE REPOSITORY: https://github.com/mrtrkmnhub/ubuntu-packer\nIn this blog post, provisioning and customizing images using packer will be shown with a template repository.\nIf you are asking or wondering what is Packer, the official definition is :\n Packer is a free and open source tool for creating golden images for multiple platforms from a single source configuration. (From Official Website).\n This post includes provisioning of ubuntu image on AWS and local.","title":"packer: build custom images on cloud and local "},{"content":"fail2ban A while ago, I was checking servers' logs to see any suspicious activities going on from outside. I noticed that the servers both staging/testing and production servers are receiving a lot of brute force SSH attacks from variety of countries which are shown in table below.\n List of IP Addresses ( who are doing SSH Brute Forcing ) ** Information on the table gathered from: [ https://www.maxmind.com/en/geoip-demo ]\n Ban failed attempts Although servers have no password login, they are kept brute forcing on SSH port. Well, fail2ban was one of obvious solution to block those IP addresses permanently or temporarily. I prefered to block them all permanently until manual unblocking has been done by me.\nThe steps for installing fail2ban is pretty obvious, you are doing same things like, apt-get update \u0026amp;\u0026amp; apt-get install fail2ban. After installation completed, configuration is much more important.\nFollowing steps will guide you to block any ip address who are brute forcing on SSH.\n  Copy template file  $ cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local   Set Ban time\nIt is possible to set ban time permanent or temporarily. I preffered to setup permanent, so for this reason I have changed bantime = -1. Save and exit from the file when you are done.\n  $ vim /etc/fail2ban/jail.conf # Permanent ban  bantime = -1  Create custom rules for SSH  $ vim /etc/fail2ban/jail.d/sshd.local [sshd] enabled = true port = ssh filter = sshd logpath = /var/log/auth.log # place of ssh logs  maxretry = 4 # maximum number of attempts that user can do  (*Maxretry value and log file can be changed according to your setup.)\n  Make the rules persistent\nIn order to make the rules persistent which means, the blocked IPs will not be deleted after restart of fail2ban service or restart of server. It requires to have some tricks to be done inside iptables rules under fail2ban. Add following cat and echo commands at the end of actionstart and actionban respectively .\n  $ vim /etc/fail2ban/action.d/iptables-multiport.conf . . . actionstart = iptables -N fail2ban-\u0026lt;name\u0026gt; iptables -A fail2ban-\u0026lt;name\u0026gt; -j RETURN iptables -I \u0026lt;chain\u0026gt; -p \u0026lt;protocol\u0026gt; -m multiport --dports \u0026lt;port\u0026gt; -j fail2ban-\u0026lt;name\u0026gt; cat /etc/fail2ban/persistent.bans | awk \u0026#39;/^fail2ban-\u0026lt;name\u0026gt;/ {print $2}\u0026#39; \\  | while read IP; do iptables -I fail2ban-\u0026lt;name\u0026gt; 1 -s $IP -j \u0026lt;blocktype\u0026gt;; done . . . actionban = iptables -I fail2ban-\u0026lt;name\u0026gt; 1 -s \u0026lt;ip\u0026gt; -j \u0026lt;blocktype\u0026gt; echo \u0026#34;fail2ban-\u0026lt;name\u0026gt; \u0026lt;ip\u0026gt;\u0026#34; \u0026gt;\u0026gt; /etc/fail2ban/persistent.bans  Save and restart service  $ systemctl restart fail2ban These are most basic steps to block IP addresses who are actively brute forcing to servers. After some time, I am able to see them with following command :)\n$ sudo fail2ban-client status sshd Status for the jail: sshd |- Filter | |- Currently failed:\t12 | |- Total failed:\t107 | `- File list:\t/var/log/auth.log `- Actions |- Currently banned:\t16 |- Total banned:\t16 `- Banned IP list:\t171.239.254.84 184.102.70.222 180.251.85.85 103.249.240.208 159.65.194.150 117.217.35.114 113.164.79.129 61.14.228.170 116.110.30.245 43.239.80.181 77.222.130.223 14.255.137.219 184.22.195.230 125.25.82.12 116.110.109.90 115.76.168.231 It is growing in time however at least they are not able to brute force the server with same IP addresses. There are plenty of other ways to make SSH port much more secure and effective however I think having updated ssh daemon/client, passwordless login and fail2ban will be enough in most of the cases. Therefore, while I was doing this stuff, although there are plenty of guides over there, I wanted to note down how I did it to come back and check if something happens.\nTake care !\n","permalink":"/posts/fail2ban/","summary":"fail2ban A while ago, I was checking servers' logs to see any suspicious activities going on from outside. I noticed that the servers both staging/testing and production servers are receiving a lot of brute force SSH attacks from variety of countries which are shown in table below.\n List of IP Addresses ( who are doing SSH Brute Forcing ) ** Information on the table gathered from: [ https://www.maxmind.com/en/geoip-demo ]","title":"fail2ban: block ssh bruteforce attacks "},{"content":"In this post, deployment process of an application with Ansible will be explained. Traditionally applications can be deployed in different ways, quite similar approach to deploy applications like in Ansible is executing bash script which has ssh commands. To give an example, Travis continuous integration has a feature where a bash script can be defined to deploy application and through given instructions within bash script, application can be successfully deployed.\nDetails regarding to deployment using Travis bash scripting can be found here\nTravis Script Deployment I would like to give an real case example from one of the project which I work on. We were using Travis script deployment for a while and it works pretty well. The bash script which I use in our deployment process is given below:\n#!/usr/bin/env bash f=dist/hknd_linux_amd64/hknd amigo=./svcs/amigo user=ntpd hostname=sec02.lab.es.aau.dk keyfile=./travis_deploy_key deploy_path=/data/home/ntpd/daemon/hknd amigo_path=/data/home/ntpd/daemon/svcs/amigo if [ -f $f ]; then echo \u0026#34;Deploying \u0026#39;$f\u0026#39; to \u0026#39;$hostname\u0026#39;\u0026#34; chmod 600 $keyfile ssh -i $keyfile -o StrictHostKeyChecking=no $user@$hostname sudo /bin/systemctl stop hknd.service scp -i $keyfile -o StrictHostKeyChecking=no $f $user@$hostname:$deploy_path scp -i $keyfile -r -o StrictHostKeyChecking=no $amigo $user@$hostname:$amigo_path ssh -i $keyfile -o StrictHostKeyChecking=no $user@$hostname sudo /bin/systemctl start hknd.service else echo \u0026#34;Error: $fdoes not exist\u0026#34; exit 1 fi As you can observe from the bash script, every step of the deployment is given as ssh/scp commands. There is no harm regarding to it as long as it contains few steps. However, as time pass more configurations, applications will required to be deployed, updated, modified and checked, then it might turn into headache. Therefore, having well structured deployment steps using Ansible will put us to safe side.\nBefore jumping into deployment with Ansible, I would like to point out some factors which can be counted as disadvantages of not integrating Ansible to deployment process.\n Not common way of utilizing resources Not well structured deployment scripts which has high potential of being not working very well. Having plain ssh commands increase likelihood of having issues regarding to settings, deployments and more.  There are many more drawbacks of using pure bash scripts in deployment process, however, these issues may not be applicable for all them.\nFor our case, I would like to convert our bash script given above to Ansible which has more elegant structure and easy to manage.\nMove to Ansible Since the bash script does not contain complex instructions, it would be very easy to convert it into Ansible playbooks. Before starting to convert it into Ansible playbook, necessary ssh connection should be set correctly for development and production environments. (- test environment as well if required -).\nSetting ssh connection between server and ansible user is pretty straitforward, it contains following steps;\n Generate SSH Key pair Copy public key to authorized_keys on server side Encrypt private key Have decrypt script to use private key on CI without compromising it.  Overall simplified flow for deployment is given below :\nAs it is declared from overall picture above, we need to provide encrypted ssh key and script for decryption together, in order to use plain private key to access the server.\nIn this setup, Github CI will be control node which will have access to server where we would like to deploy the application.\nLet\u0026rsquo;s start to complete steps,\n  Generate SSH Key pair\n$ ssh-keygen You can keep everything default or provide some information about the questions when you run it. Once, execution of command finished, there will be public and private key, you need to append the public key to user' authorized keys file on server. Afterwards, connection should be established, you may want to test it using traditional ssh command.\n  Encrypt Private Key\nIn order to use the ssh key which is generated before, we need to encrypt the key, I preferred to use gpg tool, there are many examples about it on internet, you can check it if you wish.\n$ gpg --symmetric --cipher-algo AES256 \u0026lt;private-key-file\u0026gt; The command will prompt you to provide passphrase to encrpyt and decrypt the private key when required. Choose strong and long passphrase. Once it is done, include encrpyted file into git. (- which means commit it as well-)\n  Once they have completed, the rest is structing Ansible playbook to deploy the file to server.\nExample Repo I am going to create a repository on Github to demonstrate what I have described earlier in action.\nFor the demo purposes, I will upload a service file to server and start it, simplified version of given bash commands above.\nAnsible playbook will contain following;\n Stopping already running service Changing binary file of the service Starting it again  The tasks can be extended according to user needs however to keep it short and show how Ansible could be used on continuous integration, I will continue to have minimal playbook.\nLink to example repository: https://github.com/mrturkmenhub/ansible-deploy\nThe structure of the repository as following:\nAs it can be observed from the figurre above, I have only three tasks which are combined under main.yml.\nSome configuration regarding to Ansible, such as private key, inventory file location declaration is saved to file ansible.cfg among ssh connection configuration.\nInventory file contains server(s) to deploy the application.\nThis post is not about how to write ansible playbooks, hence, I am going to skip to explain it. If you would like to check and understand it you can check following repositories for examples;\n DevOps Learning Journey Handwritten notes about Ansible  Decrypt script is crucial file which is decrypting encrypted private key to access the server.\nDO NOT FORGET TO SET YOUR SECRET_PASSPHRASE TO SECRETS OF THE REPOSITORY\nThe workflow file The workflow file for this repository is pretty straitforward to create as well, what needs to be done is that ansible should be installed into environment. Afterwards, running ansible playbook command after decrpyting the encrypted private key will complete the tasks.\nThe generated workflow is for giving demonstration, in normal production case, the pipeline should NOT be broken, each step from testing to production deployment should be as much as automated.\nThe completed workflow file:\n# This is a basic workflow to help you get started with Actions name: CI # Controls when the action will run.  on: # Triggers the workflow on tagged commits  push: tags: - \u0026#39;*.*.*\u0026#39; # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called \u0026#34;build\u0026#34; build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 - name: Install Ansible run: |sudo apt update -y sudo apt install software-properties-common -y sudo apt-add-repository --yes --update ppa:ansible/ansible sudo apt install ansible -y - name: Set Execute command to bash script run: chmod +x ./.github/scripts/decrypt.sh # Runs a single command using the runners shell - name: Decrypt large secret run: ./.github/scripts/decrypt.sh env: SECRET_PASSPHRASE: ${{ secrets.SECRET_PASSPHRASE }} - name: Escalate Private Key Permissions run: chmod 400 ~/.privkey - name: Run ansible command run: | ansible-playbook -i ./inventory main.yml env: ANSIBLE_CONFIG: ./ansible.cfg - name: Clean Key run: rm -rf ~/.privkey The final result from Github actions:\nKeep in mind that this is just a minor portion of a long pipeline which has all unit tests, checks, linting and integration tests. Without proper pipeline in place, having Ansible might not be logical or required. Consider your cases when you would like to move to deployment with Ansible.\nCheers !\n","permalink":"/posts/deploy-with-ansible/","summary":"In this post, deployment process of an application with Ansible will be explained. Traditionally applications can be deployed in different ways, quite similar approach to deploy applications like in Ansible is executing bash script which has ssh commands. To give an example, Travis continuous integration has a feature where a bash script can be defined to deploy application and through given instructions within bash script, application can be successfully deployed.","title":"Deploy with Ansible on CI/CD"},{"content":"While watching video tutorial about Ansible, I took some notes and created following PDF file.\nINTRODUCTION TO ANSIBLE HANDWRITTEN NOTES\n","permalink":"/posts/introduction-to-ansible-notes/","summary":"While watching video tutorial about Ansible, I took some notes and created following PDF file.\nINTRODUCTION TO ANSIBLE HANDWRITTEN NOTES","title":"Introduction to Ansible - Handwritten Notes"},{"content":"In some moments, Youtube algorithm is working perfect, but sometimes it shows a video from ten years ago from nowhere. For the moments where it shows and suggests videos/playlists to us, we might want to save the list of playlist and watch in some other time. It could be on a plane, train, bus, whenever you are planning to spent some time. However, taking the URL of a playlist and saving it to your cute note program might not be sufficient enough. There is a high chance that it will be forgotten or missed, therefore, I thought that it would be nice to have an automated way of saving playlists on somewhere and download them when I need. (- in particular, when there is no or limited internet connection -).\nIn this blog post, I will go through a simple project which downloads all the videos in a playlist and generates seperate tar.gz files for each playlist to release on Github using Github actions.\nRequirements Whenever starting a project, it is always nice to imply divide and conquer approach if you know what you would like to achieve. Divide and conquer approach will hugely assist you during the development and planning no matter what is the size of the project. As first step, let\u0026rsquo;s define what we need for accomblishing such a thing. ]\n A library/program which downloads Youtube videos from given URL. A library/program which compress the downloaded videos to minimize the size. A workflow on Github Actions to trigger releases.  When the given components are clarified, only one more step left to have. There should be main component which combines the requirements given above. For this purpose, I will use Go programming language.\nAvailable Tools When existing libraries, tools and open source projects checked for the first requirement, there are some on Github, namely;\n annie: ðŸ‘¾ Fast, simple and clean video downloader youtube-dl: Command-line program to download videos from YouTube.com and other video sites you-get: Dumb downloader that scrapes the web ytdl: YouTube download library and CLI written in Go  These are the tools which enable users to download Youtube videos by providing the URL or the ID.(- some of them supports different social media platforms too, e.g vimeo -).\nTo make things simpler, I chose to use youtube-dl, since it is more promising than others and formats the output very well according to user output pattern.\nAnother point is to clarify which tool/library should I use to compress the downloaded videos from Youtube. With help of a little bit googling, I found out that pigz is quite nice tool which compress given folder/file in paralel by using all available cores of the machine. The second requirement is cleared as well, now it is time to combine both of them in one and add Github workflow on top it.\nI will mention about the Github workflow file, after structure of the program which automates the process.\nCode Structure To make things faster (- in terms of development time -), I decided to go with using pre-existing binary file to execute commands, what I mean by that is basically having a pre-installed tool (youtube-dl \u0026amp; pigz) on the system before using this application.\nIf the readme file of youtube-dl is checked, youtube-dl can be installed as command line tool into your environment. It means that we can call the tool whenever we need from our application. There are other ways to accomblish it as well, such as instead of using pre-existing binary file, we can implement the functions in our application. However, the main idea of this post is NOT about how to create or use the library, the main idea is to present how it easy to have automated way of retrieving Youtube playlist videos and saving to Github Releases. The other requirement regarding to compress can be used in similar way. (- using a pre-existin command from system -)\nTo make things simple and extendable (- which means in case of more integration of tools we should be able to accomblish it without changing, many lines of code -), I will generate a main Client struct which will have exec function and it will be overridden according to command we pass.\nThe main client struct :\ntype Client struct { //youtube-dl client  YoutubeDL *YoutubeDL // Tar client \tTar *Tar // Used to enable root command \tsudo bool // flags to service \tflags []string // enable debug or not \tdebug bool // Implementation of ExecFunc. \texecFunc ExecFunc // Implementation of PipeFunc. \tpipeFunc PipeFunc } Client struct has some fields which enables us to override whenever we want, the struct contains exec(cmd string, args ...string) ([]byte, error), shellPipe(stdin io.Reader, cmd string, args ...string) ([]byte, error), and shellExec(cmd string, args ...string) ([]byte, error) functions. It can be extended according to our requirements in the future. The explanations of the functions are given on top of functions inside the source code.\nFor the youtube-dl client, I implemented only a function (-the client functionalities are really easy to extend-), which downloads all videos on given playlist by using pre-existing command line tool youtube-dl.\npackage client type YoutubeDL struct { c *Client } // exec executes an ExecFunc using \u0026#39;youtube-dl\u0026#39;. func (ytdl *YoutubeDL) exec(args ...string) ([]byte, error) { return ytdl.c.exec(\u0026#34;youtube-dl\u0026#34;, args...) } // DownloadWithOutputName generates Folder named with Playlist name // downloads videos under given playlist url to Folder func (ytdl *YoutubeDL) DownloadWithOutputName(folderName, url string) error { cmds := []string{\u0026#34;-o\u0026#34;, folderName + \u0026#34;/%(playlist_index)s - %(title)s.%(ext)s\u0026#34;, url} _, err := ytdl.exec(cmds...) return err } For any other additinal tool to use, it is extremely practical to add, for tar tool I have implemented following for specific purpose ( -which is compressing downloaded videos in paralel- ).\npackage client type Tar struct { c *Client } // exec executes an ExecFunc using \u0026#39;tar\u0026#39; command. func (tr *Tar) exec(args ...string) ([]byte, error) { return tr.c.exec(\u0026#34;tar\u0026#34;, args...) } // CompressWithPIGZ using tar with pigz compress program to compress given data func (tr *Tar) CompressWithPIGZ(fileName, folderToCompress string) error { cmds := []string{\u0026#34;--use-compress-program=pigz\u0026#34;, \u0026#34;-cf\u0026#34;, fileName, folderToCompress} _, err := tr.exec(cmds...) if err != nil { return err } return nil } Now, it is clear that for the both statements in the requirements section has been done. However, I wanted to keep track of what I have downloaded and release, for this reason, I have created two different csv files. They are called playlist-list.csv and old-playlist-list.csv under resources/ directory in the repository, playlist-list.csv will include all list of playlist URLs with preferred folder name to download. Futhermore, as you can guess, old-playlist-list.csv will include all the playlists which are downloaded and released. Once the playlist is downloaded and released with Github actions playlist-list.csv will be wiped and all content will be appended into old-playlist-list.csv file.\nIt will give easy way of checking what has been downloaded and released.\nThe code for reading and writing to csv files are pretty easy, and can be checked under main.go in the repository.\nWorkflow File The workflow file will include some steps, which are;\n Install pigz : required to compress data in parallel. Install youtube-dl : required to download playlist from given URL. Build Binary : required to have combined binary which handles both download and compress using pre-existing tools on the system. Create Release : the step which initializes releases. Run Binary : executes the program Upload videos to Github releases : uploads downloaded content to releases. Remove playlist and append downloaded playlists to old list : updates the list inside the playlist file and commits on master branch.  The steps given above are clickable to see inside the workflow on repository.\nThis small project is created for exclusive purpose, and it is very suitable to extend functionalities. However, there are many gaps regarding to the project such as;\n it does NOT check the given playlists whether they have been already released or not. it does NOT split created tar.gz files into 2 GB splits ( since it is required to have a file on Github releases under 2GB, but there is NO limitation for overall size of files on Github releases.) Does NOT have error handling mechanism and more.  These are the points which appears when the project is checked at first glance, however there are more missing points which could be done. However, the main aim was to give idea how to accomblish automated way of downloading youtube videos and releasing with Github actions.\nI am personally using it for personal needs whenever I find useful playlist, I include it into playlist-list.csv file and pushing the changes by tagging the commit in semantic versioning format.\nThere are tons of other services which could be integrated such as Slack, Discord, Mail or any another notification systems and more, however, to keep the post short and do not bother you, it is enough for now as it is.\nThe rule for the workflow could be easily changed, like instead of running it in tagged commits, it can run in scheduled way by changing run condition only, as shown below.\nname: Download \u0026amp; Release Youtube Playlists on: schedule: - cron: \u0026#39;0 0 * * *\u0026#39; # it means every day at midnight the workflow will run If you require or would like to have more features, or fixes, suggestions and etc, you are more welcome to open issues.\nGithub Limitations Since we are using Github actions, we have some limitations regarding to usage of it.\nThe limitations regarding to file sizes in releases, according to Github Statement here:Distributing large binaries\nBasically, a file size which will be uploaded to releases should NOT exceeds 2 GB. However, keep in mind that it is per file, there is NO limitation for overall size of the release :). It means that repository will be updated to split files into chunks if size of the file exceeds 2 GB. So, in case of 15 GB of playlist, it should be uploaded in 2GB chunks to releases. (- a feature which is NOT exists on youtubeto yet -)\nThere are some more limitations:\nJob execution time - Each job in a workflow can run for up to 6 hours of execution time. If a job reaches this limit, the job is terminated and fails to complete.\nWorkflow run time - Each workflow run is limited to 72 hours. If a workflow run reaches this limit, the workflow run is cancelled.\nMore details about limmitations on Github Actions: Usage Limits\nIt is good to keep in mind the given limitations above.\nJob execution time and Workflow run time can be easily fixed if you have your own server.\nIf you would like to run Github Actions in your server, there is no limitation regarding to Job execution time and Workflow run time.\nCheck out how to setup Github Actions for your server from here:\nSetup self hosted runners\nRepository youtubeto: Automated Youtube PlayList Releaser\nDemo \n","permalink":"/posts/download-release-youtube-playlists/","summary":"In some moments, Youtube algorithm is working perfect, but sometimes it shows a video from ten years ago from nowhere. For the moments where it shows and suggests videos/playlists to us, we might want to save the list of playlist and watch in some other time. It could be on a plane, train, bus, whenever you are planning to spent some time. However, taking the URL of a playlist and saving it to your cute note program might not be sufficient enough.","title":"Download Youtube Playlists and Release through Github Actions [ CI/CD ]"},{"content":" In this post, I will be describing to setup a workflow to build and release your Latex files through Github actions. First of all, keep in mind that this post is not about what is Latex and how to use it.\nIt is extremely nice to integrate daily development tools such as CI/CD to your preparation of paper, without any hassle. Why is that because it is cool to track of what has been changed on a paper over time. In fact, having a couple of people who are responsible in different parts of paper, sometimes blocks others. Therefore, having such a workflow will increase productivity for everyone in a group. Whenever pull request created to main branch, it will be easy to check typos, logic errors and missing points by others.\n Latex preparation Setup Github Actions Proof of Concept  Latex preparation I am assuming that you have agreed to work on Latex template to complete a paper. In this case, there is only small step left to do, create a Github repository (-it should be on Github, Github Actions will be used-) and push all files of your Latex template. (-in general, in following structure-)\n|-sections | introduction.tex | related_works.tex | problem.tex | solution.tex | conclusion.tex |- main.tex |- references.bib The given example structure can be changed according to your wishes, however important and logical part is that having main.tex on root directory of repository.\nOnce it is set, there is only one step to complete which is setting up Github Action workflows.\nSetup Github Actions There are a few different Github Actions to use for compiling Latex document to PDF on marketplace. Most preferred one is https://github.com/xu-cheng/latex-action and it is quite easy to integrate and use.\nIt basically creates generated PDF file from provided Latex file, it can be set in workflow file as given below: (- Note that this workflow runs on tagged commits which has a tag with *.*.* pattern -)\nname: Build LaTeX document on: tags: - \u0026#39;*.*.*\u0026#39; # semantic versioning  jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v2 - name: Compile LaTeX document uses: xu-cheng/latex-action@v2 with: root_file: main.tex However, setting up only this job is not sufficient enough to have completed workflow, we require to more jobs which are Create Release and Upload Release. As you may guess from their name, first one will create the release and second one will upload provided file to releases page. It can be setup as following\nname: Release Compiled PDF  on: push: tags: - \u0026#39;*.*.*\u0026#39; jobs: build_latex: runs-on: ubuntu-latest steps: - name: Set up Git repository uses: actions/checkout@v2 - name: Compile LaTeX document uses: xu-cheng/latex-action@v2 with: root_file: main.tex - name: Create Release id: create_release uses: actions/create-release@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: tag_name: ${{ github.ref }} release_name: Release ${{ github.ref }} draft: false prerelease: false - name: Upload Release Asset id: upload-release-asset  uses: actions/upload-release-asset@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: upload_url: ${{ steps.create_release.outputs.upload_url }}  asset_path: ./main.pdf asset_name: main.pdf asset_content_type: pdf The given workflow is completed version of what you might have at the end. In summary, it builds PDF from provided Latex file, creates release and upload file to release. For more details, you can check information on each action page.\nProof of Concept Here is example repository to check completed version.\nhttps://github.com/mrturkmenhub/latex-on-ci-cd\n","permalink":"/posts/build-release-latex/","summary":"In this post, I will be describing to setup a workflow to build and release your Latex files through Github actions. First of all, keep in mind that this post is not about what is Latex and how to use it.\nIt is extremely nice to integrate daily development tools such as CI/CD to your preparation of paper, without any hassle. Why is that because it is cool to track of what has been changed on a paper over time.","title":"Latex with Github Actions"},{"content":"In this post, I am going to write demo for a tool which I have just met, it is called Evans. It is basically universal gRPC client. What it means ? Basically when you have gRPC server and would like to test gRPC calls without creating client, you can test server side calls with Evans. It is known that gRPC is very common communication method between microservices, it can be used for internal and external communication. I do not have intention to explain what gRPC is in this post since it is not the purpose. If required documentation of gRPC can be investigated.\nCreate a simple gRPC server To demonstrate and see how evans works, a running gRPC should be exists, for this reason, I am going to provide a sample gRPC server. For this purpose, I will use Go programming language, however gRPC is supporting more programming languages which you may more familiar than Go.\ngRPC server is basically an API endpoint where clients can make requests, since it is an API, first thing could be to define which methods will be used for this service.\nFor simplicity and purpose of this post, I have created a basic microservice which will has four calls namely, Add,Delete,List and Find. Since the purpose is to understand how evans works, the gRPC server does not need to be complex or includes lots of calls.\nSample repository A sample microservice is created for demonstration purposes and all codes are available here : https://github.com/mrturkmenhub/BookShelf\nIf you have already a running gRPC server, you can direcly pass to demonstration of evans, if not, you can clone https://github.com/mrturkmenhub/BookShelf repository and test evans out.\nDefining calls In my opinion, it is always nice to prepare proto file before hand, because it is like a contract which creates your main service. Let\u0026rsquo;s imagine you would like to add, delete, list and find the books that you have read or wish to read. For this purpose, microservice should have at least four different calls which are Add, Delete, List and Find. There is no limit to have more calls however in order to do not get out of topic, I am keeping it small.\nFollowing proto file would be enough for BookShelf service.\n// it is important to declare syntax version syntax = \u0026#34;proto3\u0026#34;;service BookShelf { rpc AddBook(AddBookRequest) returns (AddBookResponse) {} rpc ListBook (ListBooksRequest) returns (ListBooksResponse) {} rpc DelBook (DelBookRequest) returns (DelBookResponse){} rpc FindBook (FindBookRequest) returns (FindBookResponse){}}message AddBookRequest { BookInfo book = 1; message BookInfo { string isbn =1; string name =2; string author=3; string addedBy=4; }}message AddBookResponse { string message = 1;}message ListBooksRequest {// no need to have anything // could be extended to list books based on category ... }message ListBooksResponse { repeated BookInfo books =1; message BookInfo { string isbn =1; string name =2; string author=3; string addedBy=4; }}message DelBookRequest { string isbn =1;}message DelBookResponse { string message =1;}message FindBookRequest { string isbn =1;}message FindBookResponse { Book book = 1; message Book { string isbn =1; string name =2; string author=3; string addedBy=4; }}Once proto file is declared, it becomes more easy to continue. For the demostration purposes, I will store information of books in memory. However, as you know, it is NOT acceptable for any production level application.\nCompile Proto file proto files are great since once you have defined what you need, you can directly generate codes in available languages which are represented in gRPC supported languages. The generation of codes in your desired language is pretty straitforward, I am going to generate the code for Go programming language.\n$ protoc -I proto/ proto/bs.proto --go_out=plugins=grpc:proto It will generate ready to use Go source code for your rpc calls which are defined in proto file.\nAfterwards, necessary piece of codes should be implemented, which are in memory store and book struct. For the aim of this post, I assumed that you have created all rest of the code as given in example gRPC server (-BookShelf-).\nNOTE: Generating source code through protoc requires to have protoc tool to be installed before hand. Installation of protoc is over here\nRun gRPC server The post is not covering all aspects of gRPC, proto buffers, Go language and those are not the intention of this post. Therefore, I am assuming that you had gRPC server and would like to test out and see whether your proto contract is running correctly without creating client side codes. Once it is confirmed that your gRPC calls are running without encouraging any unseen problems, then creating client side code will be much easy without any problem.\nYou can start gRPC server with:\n$ go run server/main.go BookShelf gRPC server is running .... Once gRPC server is up and running, you can use evans tool for inspecting gRPC server for available inquires.\nDemonstration of EVANS Evans is an open source project which is available at Github and I found it pretty useful, in particular, for people who have no idea what kind of calls are available in proto file, as it states its explanation, it is universal gRPC client. Installation of evans and more information is given its readme file.\nIt has plenty of features which are very handy to use for automating and testing some stuff on top of existing gRPC server or new one.\nLet\u0026rsquo;s make a demo, when you are using evans your gRPC server should be up and running, in order to make communication with universal gRPC client - evans. I assumed that you have followed readme file of evans and installed it correctly.\nNote that port 9000 is given because it is port of gRPC server.\nâ¯ evans -r -p 9000 ______ | ____| | |__ __ __ __ _ _ __ ___ | __| \\ \\ / / / _. | | \u0026#39;_ \\  / __| | |____ \\ V / | (_| | | | | | \\__ \\  |______| \\_/ \\__,_| |_| |_| |___/ more expressive universal gRPC client BookShelf@127.0.0.1:9000\u0026gt; show services +-----------+----------+------------------+-------------------+ | SERVICE | RPC | REQUEST TYPE | RESPONSE TYPE | +-----------+----------+------------------+-------------------+ | BookShelf | AddBook | AddBookRequest | AddBookResponse | | BookShelf | ListBook | ListBooksRequest | ListBooksResponse | | BookShelf | DelBook | DelBookRequest | DelBookResponse | | BookShelf | FindBook | FindBookRequest | FindBookResponse | +-----------+----------+------------------+-------------------+ BookShelf@127.0.0.1:9000\u0026gt; As you can observe all calls which are used in proto file can be used through evans, moreover its usage is pretty straitforward.\nYou can check demonstration video below.\nâš ï¸ You may wish to change the video quality to 1080p60\nIf you have any questions, fix, or something else, do not hesitate to contact with me.\n","permalink":"/posts/grpc-calls-with-evans/","summary":"In this post, I am going to write demo for a tool which I have just met, it is called Evans. It is basically universal gRPC client. What it means ? Basically when you have gRPC server and would like to test gRPC calls without creating client, you can test server side calls with Evans. It is known that gRPC is very common communication method between microservices, it can be used for internal and external communication.","title":"Universal gRPC client demonstration [Evans]"},{"content":"In recent post, which is Setup Highly Available Kubernetes Cluster with HAProxy , a highly available Kubernetes cluster is created. However, once I started to dig in and deploy some stuff to cluster, I realized that I am not able to connect any deployed application or services. For instance, when an web application is deployed using HAProxy load balancer (endpoint), and check from kubectl (on client side), its status is running. However, that application could not be reached from outside world although I re-patch an external IP address by following command\n$ kubectl patch svc \u0026lt;application-name\u0026gt; -n \u0026lt;name-of-namespace\u0026gt; -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;, \u0026#34;externalIPs\u0026#34;:[\u0026#34;\u0026lt;haproxy-ip-address\u0026gt;\u0026#34;]}}\u0026#39; After some searching and reading, I realized that worker nodes require their own ingress controllers in order to forward traffic between them in case of load. I will be giving more information of how I fix the issue, however let\u0026rsquo;s learn some basic terms and general information about ingress controller.\nWhat is ingress controller ? The best and simple explanation to this question is coming from Kubernetes official documentation over here, as they are expressing that ;\n Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.\n  An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontend to help handle the traffic.\n Whenever you have services which are running inside a cluster and would like to access them, you need to setup ingress controller for that cluster. The missing part was having no ingress controller on worker nodes in my k8s cluster. Everything was working however there was no access to them from outside world, that\u0026rsquo;s why ingress controller should take place in cluster architecture.\nIn this post, I will go for NGINX ingress controller with its default setup, however there are plenty of different ingress controllers which you may go for. I might change NGINX to Traefik in future but it depends on requirements yet for now, I will go with nginx ingress controller. The reason is that, it is super easy to setup, super rich with different features, included Kubernetes official documentation and fulfill what I am expecting for now.\nUpdates to cluster Let\u0026rsquo;s briefly what I have explained in previous post;\n Create VMs Setup SSH connection Use KubeSpray to deploy cluster Create HAProxy and establish SSH connection with all nodes.  I have noticed that when deploying cluster, some add-ons should be enabled in order to use ingress controller from cluster with external HAProxy load balancer. Now, since cluster deployment was established with Ansible playbooks, it is not needed to setup everything from scratch. All modified configuration can be re-deployed without effecting any resource which is exists on cluster setup. It means that, I can enable required parts in configuration file and re-deploy cluster as I did on previous post.\n  Enable ingress controller from inventory file inside KubeSpray\n$ vim inventory/mycluster/group_vars/k8s-cluster/addons.yml # Nginx ingress controller deployment ingress_nginx_enabled: false -\u0026gt; true Once this configuration part is updated from existing KubeSpray configuration files, k8s cluster should be redeployed with same command in previous post\nAssumption : previous configured KubeSpray settings are used.\n$ ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml It will take a while and update all necessary parts which are required.\n  Include Ingress API object to route traffic from external HAProxy server to internal services\nTo include Ingress API object, HAProxy configuration file should be modified, following lines should be added to /etc/haproxy/haproxy.cfg file.\n$ vim /etc/haproxy/haproxy.cfg frontend kubernetes-ingress-http bind *:80 default_backend kubernetes-worker-nodes-http backend kubernetes-worker-nodes-http balance leastconn option tcp-check server worker1 10.0.128.81:80 check fall 3 rise 2 server worker2 10.0.128.137:80 check fall 3 rise 2 server worker3 10.0.128.156:80 check fall 3 rise 2 In given configuration balancing algorithm is leastconn which can be changed into any load balancer algorithm which is supported by HAProxy, however leastconn algorithm is fitting more to what I would like to achieve that\u0026rsquo;s why it is declared as leastconn. Note that this configuration addition is on top of added part on previous post.\nOnce HAProxy configuration is updated, HAProxy should be restarted systemctl restart haproxy. It is all for HAProxy configuration, now let\u0026rsquo;s dive into setting up NGINX Ingress Controller.\n  Setup NGINX Ingress Controller It is super simple to deploy and setting up NGINX ingress controller since it is well documented and explains required parts in detail. To setup NGINX Ingress Controller, I will follow official guideline which is exists on NGINX Ingress Controller Installation.\nImage is taken from (https://www.nginx.com/products/nginx/kubernetes-ingress-controller/#resources)\nIn normal cases, the situation is as given figure above, however, since in existing k8s cluster, I am using HAProxy for communicating with clients, I need NGINX ingress controller inside worker nodes which will manage running applications/services by communicating with HAProxy and eventually, the services will be accessible from outside world.\nIf I summarize how overview diagram will look like in my case is like in given figure below.\nIt can be observed that, in given k8s cluster overview, HAProxy is in front, it communicates with clients, afterwards transmitting request based on defined rule on HAProxy configuration. Each worker node has NGINX ingress controller, what exactly it means, whenever a request appear to cluster, worker nodes will agree between each other and response back to user without having any problem. Since NGINX ingress controller is capable of load balancing inside worker nodes as well.\nThere is also Ingress Resource Rules part inside cluster, what it does, is that all routing rules based on path forwarded given service, an example on this is given below.\nSteps to create NGINX Ingress controller All steps shown below for installation of NGINX Ingress Controller taken from https://docs.nginx.com/nginx-ingress-controller/installation/\nMake sure that you are a client with administrator privilege, all steps related to NGINX ingress controller should be done through kubectl (on client computer/server)\n Clone Ingress Controller Repo  $ git clone https://github.com/nginxinc/kubernetes-ingress/ $ cd kubernetes-ingress  Create a namespace and a service account for the Ingress controller  $ kubectl apply -f common/ns-and-sa.yaml  Create a cluster role and cluster role binding for the service account  $ kubectl apply -f rbac/rbac.yaml  Create a secret with a TLS certificate and a key for the default server in NGINX  $ kubectl apply -f common/default-server-secret.yaml  Create a config map for customizing NGINX configuration:  $ kubectl apply -f common/nginx-config.yaml Afterwards, there are two different ways to run NGINX ingress controller deployment, which are as daemonset or as deployment. Main difference between those are summarized on official installation page as;\n Use a Deployment. When you run the Ingress Controller by using a Deployment, by default, Kubernetes will create one Ingress controller pod.\n  Use a DaemonSet: When you run the Ingress Controller by using a DaemonSet, Kubernetes will create an Ingress controller pod on every node of the cluster.\n I will go with DaemonSet approach, the reason is that generally when you have background-ish tasks which will run non-stateless then DaemonSet is more preferred way of running it.\n$ kubectl apply -f daemon-set/nginx-ingress.yaml Once it is applied as daemon set, the result could be checked with following command and result will be similar to given result below.\n$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ingress-47z8r 1/1 Running 0 24h pod/nginx-ingress-cmkfq 1/1 Running 0 24h pod/nginx-ingress-ft5pv 1/1 Running 0 24h pod/nginx-ingress-q554l 1/1 Running 0 24h pod/nginx-ingress-ssdrj 1/1 Running 0 24h pod/nginx-ingress-t9jml 1/1 Running 0 24h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/nginx-ingress 6 6 6 6 6 \u0026lt;none\u0026gt; 24h Deploy Example Application To test how an application will be exposed to externally from k8s cluster, an example applicaton could be deployed as given below. Note that the following example is simplest example for this context, hence, keep in mind that it might require more configuration and detailed approach then described here when you would like to deploy more complex applications.\n Create a sample NGINX Web Server (Using provided example)  nginx-deploy-main.yml\napiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 Taken from https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/\nIn given yaml deployment file above, two replicas of NGINX:1.14.2 will be deployed to cluster and it has name of nginx-deployment. The yaml explains itself very well.\nIt can be deployed either through directly from official link or from your local depends on your preferences.\n$ kubectl apply -f https://k8s.io/examples/application/deployment.yaml ## or you can do same thing with local file as given below $ kubectl apply -f nginx-deploy-main.yml Expose deployment:\n$ kubectl expose deploy nginx-deployment --port 80 Once it is deployed to cluster and exposed, there is one step left for this simple counter example is that, exposing the service and creating ingress rule (resource) in yaml file, by specifiying kind as Ingress.\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: \u0026lt;dns-record\u0026gt; (a domain like test.mydomain.com) http: paths: - path: / backend: serviceName: nginx-deployment servicePort: 80 The crucial part is serviceName and servicePort which are defining specifications of the services within cluster. The yaml specifications can be expanded as shown below, assume that you have wildcard record in your domain name server and have multiple services which are running in same port in a cluster, yaml file can be re-defined as given below.\nnginx-ingress-resource.yml\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-controller spec: rules: - host: \u0026lt;dns-record\u0026gt; (a domain like test.mydomain.com) http: paths: - path: / backend: serviceName: nginx-deployment servicePort: 80 - path: /apache backend: serviceName: apache-deployment servicePort: 80 - path: /native-web-server  backend: serviceName: native-web-server-deployment servicePort: 80 Keep in mind that all given services should be deployed before hand otherwise when a request made to any path which is not deployed, it may return either 404 or 500. There are plenty of different options to define and update the components in a k8s cluster. Therefore, all yaml files should be changed according to requirements.\nCreate ingress controller rules from provided yaml file\n$ kubectl create -f nginx-ingress-resource.yml Now, the NGINX web server deployment is ready on given DNS record in yaml file and according to request paths different services can be called which are also running inside kubernetes cluster.\nNote that, provided yaml files are just simple example of deploying NGINX web server without any certification, when certificates (HTTPS) enabled or any other type of deployment happened different configurations should be applied.\nWhen everything goes without any problem, you will have a cluster which uses NGINX Ingress controller for internal cluster routing and HAProxy as communication endpoint for clients. Keep in mind that whenever a new service or deployment take place, required configuration should be enabled in HAProxy configuration as it is enabled for port 80 applications above. Different services will have different requirements therefore it is important to catch main logic in a setup. It is all done for this post.\nCheers !\n","permalink":"/posts/setup-ingress-controller/","summary":"In recent post, which is Setup Highly Available Kubernetes Cluster with HAProxy , a highly available Kubernetes cluster is created. However, once I started to dig in and deploy some stuff to cluster, I realized that I am not able to connect any deployed application or services. For instance, when an web application is deployed using HAProxy load balancer (endpoint), and check from kubectl (on client side), its status is running.","title":"NGINX Ingress Controller with HAProxy for k8s cluster"},{"content":"The main purpose of this blog post a simple walkthrough of setting up Kubernetes cluster with external HAProxy which will be the endpoint where our kubectl client communicates over. Node specifications for this setup is given as shown in the table below. Keep in mind that all of them has access to each other with password and without password. The environment which Kubernetes cluster will stay is running on OpenStack. It means that once a configuration (ssh keys, hosts, and etc) is done for example master 1 then all other nodes could be initialized through snapshot of master 1. To be able to setup such a Kubernetes cluster easily, I will be using KubeSpray which is a repository where it has all required configuration and playbooks for setting up necessary cluster.\n Node Specification Prerequisites General Overview KubeSpray Configuration External Load Balancer Setup (HAProxy) Setup KubeSpray Configuration  The intention of this walkthrough is that setting up your own Kubernetes cluster in your own servers, this post is not very useful for people who are already using cloud provider solutions.(Kubernetes cluster as a service). You can checkout following resources listed below : (few of them :) )\nCloud Providers Solutions:\n Azure Kubernetes Service - AKS Google Kubernetes Engine - GKE Managed Kubernetes on DigitalOcean Kubernetes on AWS  Node Specification Kubernetes cluster will be setup on following nodes in the table below, note that HAProxy will run on another node and all ansible playbooks and setting up Kubernetes cluster will be managed through HAProxy. Keep in mind that all nodes + HAProxy is under same subnet internally which means that we will only one external IP address where HAProxy use and kubectl clients communicate. All instances are running on ubuntu_18.04, it means that the instructions and steps may not work with another system.\nPrerequisites  Nodes Requirements of KubeSpray Setting up SSH Key Across Nodes Getting snapshot ( -it is optional -) Setting up login with password  General Overview The following sketch is general overview of how Kubernetes cluster will look like at the end of this walkthrough, the figure is super overviewed version of cluster.\nIn given figure above, nodes do not have any external IP adress however, including HAProxy, all of them in same subnet, only HAProxy has external IP address which will be reachable by kubectl clients.\nBefore moving installation step of Kubernetes cluster, we need to setup a sample master node (instance) with predefined configuration. Since we will have only one server which is open to outside world, we need to make sure that there is a connection between HAProxy and sample master node. I am currently calling it sample master node, it is because, preliminary configurations such as authentication with password, disabled swap area and ssh keys will be all configured. This sample master node should be started and accesible over HAProxy, which means that in order to access to sample master node, I should do following;\n SSH to HAProxy using SSH key (Password Login disabled) like ssh -i ~/.ssh/id_rsa \u0026lt;username\u0026gt;@\u0026lt;ha-proxy-external-ip\u0026gt; Copy SSH Key to HAProxy, which let you in to sample master node Then SSH to sample master node with same approach. (ssh ~/.ssh/masternode.pem \u0026lt;username\u0026gt;@\u0026lt;master-node-ip\u0026gt;  After you are inside sample master node, now, some configurations and setting should be done. Afterwards, we can initialize other five nodes from snapshot of configured sample master node.\nSteps:\n Enable Password Login if not enabled already.  $ echo \u0026#34;PermitRootLogin yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config $ sed -i -E \u0026#39;s/PasswordAuthentication no/PasswordAuthentication yes/g\u0026#39; /etc/ssh/sshd_config  Specify Password for ROOT  $ sudo su $ passwd Given commands will ask new unix password for root user. Define the password and do not forget or lose it. Since we will gonna use snapshot of this configured machine, all settings will be same, I did like that to shortcut the process.\n Disable swap area (RUN ALL COMMANDS AS ROOT)  $ swapoff -a Afterwards, exit from sample master node, create snapshot of that node (it is called volume snapshot in OpenStack), once you have successfully created snaphot, all five other nodes should be initialized from snapshot of this sample master node. This way, there is no need to repeat same steps described above.\nIn case of not having possibility to create snapshot follow given steps (if and if only, you could NOT create snapshot and initialize other five nodes from the snapshot)\n  Create all nodes (workers and masters)\n  Enable SSH connection to all nodes from HAPRoxy server.\n  From HAProxy server, execute following steps. (-Make sure that you have configured SSH connection with ROOT priviledges and have access to all nodes from HAProxy node -)\nOnce you are sure that you have SSH access to all nodes from HAProxy through SSH, implement following steps.\n Install parallel-ssh (-to run a command in parallel on nodes-) (run with ROOT priviledges)  $ apt-get update \u0026amp;\u0026amp; apt-get install -y pssh  Install HAProxy (as ROOT priviledges)  $ apt-get install -y haproxy  Modify /etc/hosts (-For easy communication through nodes-)  Append worker and master node IPs to /etc/hosts file\n$ vim /etc/hosts 10.0.128.156 worker3 10.0.128.137 worker2 10.0.128.81 worker1 10.0.128.184 master3 10.0.128.171 master2 10.0.128.149 master1  Create nodes text file on home directory  $ cat nodes worker3 worker2 worker1 master3 master2 master1 Since IP addresses of them defined in /etc/hosts file, system can now recognize and connect IPs of them through just by name\n Generate and Copy SSH Key to all nodes (Required for easy communication)  If there is already a SSH key (like in ~/.ssh/id_rsa), you can use it as well.If not, you can do following step\n$ ssh-keygen # will prompt passphrase, you can leave empty , NOTE THAT IF YOU DO NOT HAVE SSH KEY, GENERATE IT. $ for i in $(cat nodes); ssh-copy-id $i; done The for loop given as second command will copy ssh key to all nodes, then accesing any node without password will be flawless.Like given command below;\n$ ssh master1 # in defualt uses same username with terminal session  Disable swap area on all nodes (Note that if you are using snapshot method, no need to do this step)  $ parallel-ssh -h nodes -i \u0026#34;swapoff -a\u0026#34; Parallel SSH tool is handy to complete tasks in parallel for multiple hosts.\nKubeSpray Configuration KubeSpray is a repository to setup Kubernetes clusters with predefined configuration settings using Ansible playbooks. The usage of KubeSpray is pretty straightforward, as default settings, KubeSpray is using internal load balancers in each worker node, which means that when you setup a Kubernetes cluster using default values of KubeSpray, you will have following arch overview.\nHowever, in this guide, external load balancer approach will be used to setup cluster, if you wish to leave everything as default with KubeSpray, you can skip this External Load Balancer Setup part.\nExternal Load Balancer Setup (HAProxy) Modify configuration file of HAProxy to enable external LoadBalancer, copy this following configuration and append to /etc/haproxy/haproxy.cfg. (end of file)\nlisten kubernetes-apiserver-https bind \u0026lt;your-haproxy-internal-ip\u0026gt;:8383 mode tcp option log-health-checks timeout client 3h timeout server 3h server master1 \u0026lt;your-master1-ip\u0026gt;:6443 check check-ssl verify none inter 10000 server master2 \u0026lt;your-master2-ip\u0026gt;:6443 check check-ssl verify none inter 10000 server master3 \u0026lt;your-master3-ip\u0026gt;:6443 check check-ssl verify none inter 10000 balance roundrobin Balance algorithm is roundrobin however you can change it from list of available balance algorithms provided by HAProxy.\nOnce it is done, save and restart HAProxy service.\n$ systemctl restart haproxy Setup KubeSpray Configuration Since external load balancer will be used, there is few things to be done to change default values in KubeSpray. Following steps will be done on HAProxy node.\n Clone the project and prepare environment  $ git clone https://github.com/kubernetes-sigs/kubespray $ apt-get install -y python3-pip # install pip3 if not installed $ cd kubespray  Follow the guide on KubeSpray README.md file  Following instructions taken from KubeSpray README.md\n# Install dependencies from ``requirements.txt`` sudo pip3 install -r requirements.txt # Copy ``inventory/sample`` as ``inventory/mycluster`` cp -rfp inventory/sample inventory/mycluster # Update Ansible inventory file with inventory builder declare -a IPS=(10.0.128.149 10.0.128.171 10.0.128.184 10.0.128.81 10.0.128.137 10.0.128.156) CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}  Modify generate hosts YAML file  When you check inventory/mycluster/hosts.yaml file, you will notice that it created two master nodes, which we require three, add missing one properly to that list as shown below.\nall: hosts: master1: ansible_host: 10.0.128.149 ip: 10.0.128.149 access_ip: 10.0.128.149 master2: ansible_host: 10.0.128.171 ip: 10.0.128.171 access_ip: 10.0.128.171 master3: ansible_host: 10.0.128.184 ip: 10.0.128.184 access_ip: 10.0.128.184 worker1: ansible_host: 10.0.128.81 ip: 10.0.128.81 access_ip: 10.0.128.81 worker2: ansible_host: 10.0.128.137 ip: 10.0.128.137 access_ip: 10.0.128.137 worker3: ansible_host: 10.0.128.156 ip: 10.0.128.156 access_ip: 10.0.128.156 children: kube-master: hosts: master1: master2: master3: kube-node: hosts: master1: master2: master3: worker1: worker2: worker3: etcd: hosts: master1: master2: master3: k8s-cluster: children: kube-master: kube-node: calico-rr: hosts: {} Once it is done, the other thing which should be modified to use external load balancer HAProxy, is all.yaml file located under inventory/mycluster/group_vars/all/.\nall.yml is general configuration file which specifies main configurations of your cluster, it uses Nginx load balancer by default which means that each worker node has its own local nginx load balancer as given second figure above. If not specified anything else.\n Disable default load balancer  $ vim inventory/mycluster/group_vars/all/all.yml loadbalancer_apiserver_localhost: false  Add external load balancer HAProxy.  $ vim inventory/mycluster/group_vars/all/all.yml ## External LB example config apiserver_loadbalancer_domain_name: \u0026#34;\u0026lt;domain-name-of-lb\u0026gt;\u0026#34; loadbalancer_apiserver: address: 10.0.128.193 port: 8383  Initialize cluster deployment  # under kubespray/ directoy  $ ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml It will take around 10-15 minutes which depens on your cluster and if everything goes well, at the end of deployment through Ansible you will not face with any problem. If so, you can test it by SSH to master node and try kubectl cluster-info.\n$ kubectl cluster-info Kubernetes master is running at ..... To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. It means that Kubernetes cluster with three master and three worker nodes available to use.\nNote that the default configuration of cluster could be changed more however before attempting to change default configuration, make sure that you did correct research on what to change on KubeSpray default settings. Otherwise, there might be problems regarding to customized configuration settings.\nFor more information stay updated and watch KubeSpray regarding to issues, pitfalls and more.\nLast step for this post is creating kubectl configuration for your personal/work computer to access the cluster. Install kubectl on your environment. Afterwards copy configuration from master node to your ~/.kube/ as config.\nSince we have only one endpoint, configuration file should be copied to HAProxy Server then your computer, through rsync or scp\n On HAProxy Server  $ scp root@master1:/etc/kubernetes/admin.conf config # will copy admin.conf as config  $ cp config /home/ubuntu/ # copy to a user home dir $ chown ubuntu:ubuntu /home/ubuntu/config # change owner of the file   On your personal/work computer  $ scp -i ~/.ssh/haproxy.pem ubuntu@\u0026lt;ha-proxy-ip\u0026gt;:/home/ubuntu/config ~/.kube/ Now, you should be able to get and dump your cluster information as in master nodes.\n$ kubectl cluster-info Kubernetes master is running at ..... To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. There are lots of configurations and different settings regarding to Kubernetes cluster environment and generally using Cloud Provider solutions are less painful or painless. However, sometimes it is less costly to setup your own environment and having full access to anything could be better for learning under the hood things or creating highly customized environments. It really depends on your situation therefore it is up to you to go and setup your own Kubernetes cluster or use it as service from cloud providers.\nBy the way, thanks for giving time to checkout the post ðŸ˜‰\n","permalink":"/posts/install-ha-kubernetes-cluster/","summary":"The main purpose of this blog post a simple walkthrough of setting up Kubernetes cluster with external HAProxy which will be the endpoint where our kubectl client communicates over. Node specifications for this setup is given as shown in the table below. Keep in mind that all of them has access to each other with password and without password. The environment which Kubernetes cluster will stay is running on OpenStack. It means that once a configuration (ssh keys, hosts, and etc) is done for example master 1 then all other nodes could be initialized through snapshot of master 1.","title":"Setup Highly Available Kubernetes Cluster with HAProxy"},{"content":"VPN KuralÄ±m BugÃ¼n sizlere kendinize ait VPN sistemi nasÄ±l kurulur, onu anlatmak istiyorum, daha Ã¶nce Ä°ngilizce olarak, yayÄ±nladÄ±m fakat TÃ¼rkÃ§e bir kaynaÄŸÄ±n da faydalÄ± olabileceÄŸini dÃ¼ÅŸÃ¼ndÃ¼m. Burada anlatÄ±lanlar, ubuntu ailesine (16.04,18.04) ait sunucular Ã¼zerinde test edilmiÅŸtir.\nÄ°lk olarak bulut hizmeti saÄŸlayan bir ÅŸirketten bu DigitalOcean, Google Cloud, Microsoft Azure veya Amazon olabilir, sunucu kiralÄ±yorsunuz, en ucuzu ve makul olanÄ± DigitalOcean tarafÄ±ndan sunulan aylÄ±k 5 dolar olan sunucu diyebilirim. Sunucuyu kiraladÄ±ktan ve ssh baÄŸlantÄ±sÄ±nÄ± saÄŸladÄ±ktan sonra VPN kurulumuna geÃ§ebiliriz.\nVPN hakkÄ±nda tam bilgisi olmayan arkadaÅŸlar iÃ§in ÅŸu ÅŸekilde Ã¶zetlenebilir, sizin iÃ§in oluÅŸturulmuÅŸ sanal bir baÄŸlantÄ± noktasÄ± gibi dÃ¼ÅŸÃ¼nebilirsiniz. Yani VPN\u0026rsquo;e baÄŸlandÄ±ktan sonra bilgisayarÄ±nÄ±zdan Ã§Ä±kan ve bilgisayarÄ±nÄ±za gelen aÄŸ trafik ÅŸifrelenmiÅŸ olarak iÅŸlenir. ÃœÃ§Ã¼ncÃ¼ parti yazÄ±lÄ±mlarÄ±n veya MITM gibi saldÄ±rÄ±larÄ±n Ã¶nÃ¼ne geÃ§miÅŸ olursunuz.\nNeden kendi VPN sistemi kurmalÄ±yÄ±z ? Ã‡Ã¼nkÃ¼ ÅŸu anda var olan bÃ¼tÃ¼n VPN sistemleri, Ã¼cretsiz olarak hizmet saÄŸlasa dahi, sizin bilgilerinizin satÄ±lmasÄ±, arÅŸivlenmesi ve gerektiÄŸinde ilgili birimlere aktarÄ±lmasÄ± amacÄ±yla kaydedilmektedir. Bunun ne gibi zararlarÄ± olabilir gelin birlikte ÅŸÃ¶yle bir sÄ±ralayalÄ±m:\n Oltalama saldÄ±rÄ±larÄ±na sadece sizin bilebileceÄŸiniz bilgiler ile maruz kalma. Ziyaret ettiÄŸiniz siteler tarafÄ±ndan reklam bombardÄ±manÄ±na maruz kalma. KiÅŸisel bilgilerinizin reklam veren ajanslara satÄ±lmasÄ±, bu durum birÃ§ok kiÅŸi tarafÄ±ndan tam olarak anlaÅŸÄ±lamÄ±yor, yani ÅŸu ÅŸekilde anlaÅŸÄ±lamÄ±yor, internet Ã¼zerinden alÄ±ÅŸveriÅŸ yapan A kiÅŸisi, kendine ait bilgilerin, onun bilgilerini satacak kiÅŸiler tarafÄ±ndan deÄŸersiz olduÄŸuna inanÄ±yor ve hiÃ§bir gizlilik saÄŸlamadan internet kullanÄ±mÄ±na devam ediyor. Bu sonunda o kiÅŸiye zarar vermese bile o kiÅŸinin konuÅŸtuÄŸu, gÃ¶rÃ¼ÅŸtÃ¼ÄŸÃ¼ veya birlikte Ã§alÄ±ÅŸtÄ±ÄŸÄ± arkadaÅŸlara zarar verebiliyor.  Burada sÄ±ralananlar sadece buzdaÄŸÄ±nÄ±n gÃ¶rÃ¼nen ucu bile diyemeyiz, gÃ¼nÃ¼mÃ¼zde veri iÅŸleme teknikleri ve yaklaÅŸÄ±mlarÄ± Ã¶yle geliÅŸmiÅŸtir ki siz bile kendinize ait olan bir ÅŸeyin varlÄ±ÄŸÄ±na farkÄ±nda olmadan onlar iÅŸlemleri tamamlamÄ±ÅŸ oluyor :).\nBu ve bunlardan Ã§ok daha fazla nedenden dolayÄ± VPN kullanÄ±mÄ± ÅŸart diyebilirim. Peki bunu nasÄ±l yapacaÄŸÄ±z, bu kÄ±sÄ±mdan sonra sizin bir bulut saÄŸlayÄ±cÄ±sÄ± tarafÄ±ndan sunucunu kiraladÄ±ÄŸÄ±nÄ±zÄ± ve ssh baÄŸlantÄ±sÄ±nÄ± saÄŸladÄ±ÄŸÄ±nÄ±zÄ± varsayÄ±yorum.\nBu gÃ¶nderide WireGuard VPN uygulamasÄ± kullanÄ±lacaktÄ±r. WireGuard VPN uygulamasÄ± aÃ§Ä±k kaynaklÄ± bir uygulama olup, saÄŸladÄ±ÄŸÄ± imkanlar sayesinde diÄŸer VPN uygulamalarÄ±na (OpenVPN ve diÄŸerleri) kÄ±yasla Ã§ok daha hÄ±zlÄ± ve gÃ¼venilirdir.\nSunucu AyarlarÄ± VPN uygulamasÄ±nÄ± kiraladÄ±ÄŸÄ±mÄ±z sunucu Ã¼zerine kuralÄ±m.\n$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y $ sudo add-apt-repository ppa:wireguard/wireguard $ sudo apt-get update $ sudo apt-get install wireguard UygulamayÄ± Ã§ekirdek gÃ¼ncellemeleri ile birlikte gÃ¼ncellemek iÃ§in gerekli komutu girelim.\n$ sudo modprobe wireguard AÅŸaÄŸÄ±da verilen komut girildiÄŸinde beklenen sonuÃ§.\n$ lsmod | grep wireguard wireguard 217088 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard AnahtarlarÄ± Ã¼retelim\n$ cd /etc/wireguard $ umask 077 $ wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey VPN Konfigurasyon dosyasÄ±nÄ± /etc/wireguard/wg0.conf ayarlayalÄ±m.\n[Interface] PrivateKey = \u0026lt;daha-Ã¶ncesinde-Ã¼retilen-gizli-anahtar\u0026gt; Address = 10.120.120.2/24 Address = fd86:ea04:1111::1/64 SaveConfig = true PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o ens3 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o ens3 -j MASQUERADE; ip6tables -D FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -D POSTROUTING -o ens3 -j MASQUERADE ListenPort = 51820 Burada Ã¶nemli nokta ens3, ip tables komutu iÃ§erisinde yer alan en3, sunucudan sunucuya farklÄ±lÄ±k gÃ¶sterebilir, bundan dolayÄ± sizin sunucunuzda ne ise aÄŸ kartÄ±nÄ±n ismi onu girmelisiniz. ifconfig  komutu sayesinde Ã¶ÄŸrenilebilir.\nBir diÄŸer Ã¶nemli nokta ise daha Ã¶ncesinde 4. adÄ±mda Ã¼retilen privatekey, iÃ§eriÄŸinin PrivateKey alanÄ±na girilmesidir.\nAÄŸ trafiÄŸini yÃ¶nlendirme\n/etc/sysctl.conf dosyasÄ± iÃ§erisine aÅŸaÄŸÄ±da verilen bilgileri girerek kaydediniz.\nnet.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 Bilgiler gerekli dosyaya kaydedildikten sonra aÅŸaÄŸÄ±daki komutlar sÄ±rasÄ± ile girilmelidir.\n$ sysctl -p $ wg-quick up wg0 KomutlarÄ±n girilmesi ve herhangi bir sorun gÃ¶rÃ¼lmemesi durumda ve wg komutu terminale girildikten sonra aÅŸaÄŸÄ±da verilen Ã§Ä±ktÄ±ya benzer bir Ã§Ä±ktÄ± gÃ¶receksiniz.\n$ wg interface: wg0 public key: loZviZQpT5Sy4gFKEbk6Vc/rcJ3bH84L7TUj4qMB918= private key: (hidden) listening port: 51820 EÄŸer herhangi bir sorun ile karÅŸÄ±laÅŸmazsanÄ±z bu adÄ±ma kadar, bu demek oluyor ki, sunucu tarafÄ±nda iÅŸiniz ÅŸimdilik tamamlandÄ± geriye sadece kendi bilgisayarÄ±mÄ±zÄ±, telefonumuzu vs VPN sunucusuna baÄŸlamak kaldÄ±.\nKullanÄ±cÄ± AyarlarÄ± KullanÄ±cÄ±larÄ±n kendi bilgisayar ortamlarÄ±nda, telefonlarÄ±nda, tabletlerinde veya diÄŸer sunucularÄ±nda kullanabileceÄŸi uygulamalarÄ± buradan indirebilirsiniz.\nGerekli uygulamayÄ± kendi ortamÄ±nÄ±za indirdikten sonra tek yapmanÄ±z gereken, VPN sunucusu tarafÄ±nda ayarladÄ±ÄŸÄ±mÄ±z VPN\u0026rsquo;e baÄŸlanmak, bunun iÃ§in gerekli olan sadece konfigurasyonlarÄ± doÄŸru girmek olacaktÄ±r.\nKullanÄ±cÄ± tarafÄ±nda, uygulama Ã¼zerinden aÅŸaÄŸÄ±da verilen konfigurasyona benzer bir ayarÄ± (kendi kurduÄŸunuz VPN ayarlarÄ±na gÃ¶re privatekey ve ip adressi deÄŸiÅŸiklik gÃ¶sterecektir.) ayarlamanÄ±z gerekmektedir.\n[Interface] Address = 10.120.120.2/32 Address = fd86:ea04:1111::2/128 # note that privatekey value is just a place holder PrivateKey = KIaLGPDJo6C1g891+swzfy4LkwQofR2q82pFR6BW9VM= DNS = 1.1.1.1 [Peer] PublicKey = \u0026lt;sunucunuza-ait-public-anahtar\u0026gt; Endpoint = \u0026lt;sunucunuzun-dÄ±ÅŸ-ip-adresi\u0026gt;:51820 AllowedIPs = 0.0.0.0/0, ::/0 Gerekli iÅŸlemler kullanÄ±cÄ± tarafÄ±nda da saÄŸlandÄ±ktan sonra, sunucu tarafÄ±nda bu kullanÄ±cÄ±ya baÄŸlantÄ± izni vermek kalÄ±yor, onuda aÅŸaÄŸÄ±da verilen komut ile saÄŸlayabilirsiniz.\n$ wg set wg0 peer \u0026lt;kullanici-public-anahtari\u0026gt; allowed-ips 10.120.120.2/32,fd86:ea04:1111::2/128 Sunucu tarafindan kullanicinin VPN baglantisi saÄŸladÄ±ÄŸÄ±nÄ± aÅŸaÄŸÄ±da verilen komut ile teyit edebilirsiniz.\n$ wg interface: wg0 public key: loZviZQpT5Sy4gFKEbk6Vc/rcJ3bH84L7TUj4qMB918= private key: (hidden) listening port: 51820 peer: Ta9esbl7yvQJA/rMt5NqS25I/oeuTKbFHJu7oV5dbA4= allowed ips: 10.120.120.2/32, fd86:ea04:1111::2/128 Daha sonrasinda, wireguard tarafÄ±ndan oluÅŸturulan aÄŸ kartÄ±nÄ± aktivate edelim.\n$ wg-quick up wg0 GÃ¼venlik DuvarÄ± ayarlarÄ± Bazen sunucu tarafÄ±nda yapmanÄ±z gereken bazÄ± gÃ¼venlik duvarÄ± ayarlarÄ± bulunmakta, bunlar VPN baÄŸlantÄ±sÄ±nÄ± baÅŸarÄ±lÄ± bir ÅŸekilde saÄŸlamanÄ±z iÃ§in kritik Ã¶neme sahiptir.\n$ ufw enable VPN uygulamasÄ±na baÄŸlanmamÄ±zÄ± saÄŸlayacak portu aÃ§Ä±yoruz.\n$ ufw allow 51820/udp IP tablolarÄ± ile 51820 portu iÃ§in bazÄ± ayarlamalar yapÄ±yoruz.\n$ iptables -A INPUT -p udp -m udp --dport 51820 -j ACCEPT $ iptables -A OUTPUT -p udp -m udp --sport 51820 -j ACCEPT Burada Ã¶nemli olan kÄ±sÄ±mlardan biriside bÃ¼tÃ¼n komutlar ROOT, yani yÃ¶netici yetkisi ile yapÄ±lmalÄ±, aksi takdirde hata verecektir.\nBu noktadan sonra, bilgisayarÄ±nÄ±za, tabletinize veya telefonunuza kurduÄŸunuz WireGuard uygulamasÄ± sayesinde sorunsuz ve gÃ¼venlikli bir ÅŸekilde internetinizi kullanabilirsiniz.\n","permalink":"/posts/vpn-kuralim/","summary":"VPN KuralÄ±m BugÃ¼n sizlere kendinize ait VPN sistemi nasÄ±l kurulur, onu anlatmak istiyorum, daha Ã¶nce Ä°ngilizce olarak, yayÄ±nladÄ±m fakat TÃ¼rkÃ§e bir kaynaÄŸÄ±n da faydalÄ± olabileceÄŸini dÃ¼ÅŸÃ¼ndÃ¼m. Burada anlatÄ±lanlar, ubuntu ailesine (16.04,18.04) ait sunucular Ã¼zerinde test edilmiÅŸtir.\nÄ°lk olarak bulut hizmeti saÄŸlayan bir ÅŸirketten bu DigitalOcean, Google Cloud, Microsoft Azure veya Amazon olabilir, sunucu kiralÄ±yorsunuz, en ucuzu ve makul olanÄ± DigitalOcean tarafÄ±ndan sunulan aylÄ±k 5 dolar olan sunucu diyebilirim. Sunucuyu kiraladÄ±ktan ve ssh baÄŸlantÄ±sÄ±nÄ± saÄŸladÄ±ktan sonra VPN kurulumuna geÃ§ebiliriz.","title":"Kendimize Ã¶zel VPN kurulumu "},{"content":"Concurrency in Go Concurrency in Go, makes Go programming language very unique and attractive compared to other languages, in this section I am going to share the notes which I took when I was watching coursera video series.\nIf you did not already check previous post on Go, it could be helpful to check it out first.\n Go Notes (OOP)  Specialization serie is Programming with Google Go Keep in mind that the notes are taken from several resources mainly from the course, however post may include other resources as well, I have referenced them when required, if nothing is referenced then it means, notes are taken from the course.\nParalel Execution  Two programs execute in paralel if they execute at exactly the same time At time t, an instruction is being performed for both P1 and P2.  Why use parallel execution  Tasks may complete more quickly Example: Two Piles of dishes to wash  Two dishwashers can complete twice as fast as one   Some tasks must be performed sequentially  Example: Wash dish, dry dish  Must wash before you can dry     Some tasks are parallelizable and some are not  Von Neumann Bottleneck Speedup without Parallelism  Can we achieve speedup without Parallelism ? Design faster processors  Get speedup without changing software   Design processor with more memory  Reduces the Von Heumann bottleneck Cache access time=1 clock cycle Main memory access time = ~100 clock cycles Increasing on-chip cache improves performance    Moore\u0026rsquo;s Law  Predicted that transistor density would double every two years Smaller transistors switch faster Not a physical law, just an observation Exponential increase in density would lead to exponential increase in speed  Power Wall Power / Temperature Problem  Transistors consume power when they switch Increasing transistor density leads to increased power consumption  Smaller transistors use less power, but density scaling is much faster   High power leads to high temperature Air cooling (fans) can only remove so much heat  Dynamic Power  P = a * CFV^2 a is percent of time switching C is capacitance (related to size) F is the clock frequency V is voltage swing (from low to high) Voltage is important 0 to 5V uses much more power then 0 to 1.3V  Dennard Scaling  Voltage should scale with transistor size Keeps power consumption and temperature, low Problem: Voltage can\u0026rsquo;t go too low  Must stay above threshold voltage Noise problems occur   Problem: Does not consider leakage power Dennard scaling must stop  Multi-Core Systems  P = a * CFV^2 Cannot increase frequency Can still add processor cores, without increasing frequency  Trend is apparent today   Parallel execution is needed to exploit multi-core systems Code made to execute on multiple cores Different programs on different cores  Concurrent vs Parallel Concurrent Execution  Concurrent execution is not necessarily the some as parallel execution Concurrent: start and end times overlap Parallel: execute at exactly the same time   Parallel tasks must be executed on different hardware Concurrent tasks may be executed on the same hardware  Only one task actually executed at a time   Mapping from tasks to hardware is not directly controlled by programmer  At least not in go    Concurrent Programming  Programmer determines which tasks can be executed in parallel Mapping tasks to hardware  Operating system Go runtime scheduler    Hiding Latency *(Crucial)   Concurrency improves performance even without parallelism\n  Tasks must periodically wait for something\n i.e. wait for memory X = Y+Z read Y,Z from memory May wait 100+ clock cycles    Other concurrent tasks can operate while one task is waiting\n  Concurrent programming is useful even no paralelism\n  Hardware Mapping  Programmer does not determine the hardware mapping Programmer makes parallelism possible Hardware mapping depends on many factors  Where is the data ? What are the communication costs ?    Concurrency Basics Processes  An instance of a running program Things unique to a process   Memory   a. Virtual address space b. Code, stack, heap, shared libraries  Registers   a. Program counter  Operating System  Allows many processes to execute concurrently Processes are switched quickly  20ms   User has the impression of parallelism Operating system must give processes fair access to resources  Scheduling Processes  Operating system schedules processes for execution Gives the illusion of parallel execution   OS gives fair access to CPU, memory , etc.  Context Switch  Control flow changes from one process to another   Process \u0026ldquo;context\u0026rdquo; must be swapped Air cooling (fans) can only remove so much heat  Threads and Goroutines Threads vs. Processes  Threads share some context Many threads can exist in one process   OS schedules threads rather than processes  Goroutines  Like a thread in Go Many Goroutines execute within a single OS thread  We can call Goroutines as lightweight threads in Go.\nGo Runtime Scheduler  Schedules goroutines inside an OS thread Like a little OS inside a single OS thread   Logical processor is mapped to a thread  Interleavings  Order of execution within a task is known   Order of execution between concurrent tasks is unknown Interleaving of instructions between tasks is unknown  Possible Interleavings  Many interleavings are possible Must consider all possibilities   Ordering is non-deterministic Interleavings occurs on machine code level.  Race Conditions It is happenning when multiple goroutines try to write/read from a source at the same time, could be prevented using mutex\n Outcome depends on non-deterministic ordering   Races occur due to communication  Communication Between Tasks  Threads are largely independent but not completely independent Web server, one thread per client.   Make threads for each client who are connecting to web server Image processing, 1 thread per pixel block   Some level of communication can occur between threads, let\u0026rsquo;s say we want to blur the image accordingly, then the threads which are working on neighborhood pixels should communicate with each other.  Goroutines Goroutines are new way of threading in lightweight approach.\nCreating a Goroutine  One goroutine is created automatically to execute the main() Other goroutines are created using the go keyword  package main func main () { a=1\tfoo()\ta=2 } func foo() { // does something ...  }  Main goroutine blocks on call to foo()  package main func main () { a=1\tgo foo()\ta=2 } func foo() { // does something ...  }  New goroutine created for foo() Main goroutine does not block  Exiting a Goroutine  A goroutine exits when its code is complete When the main goroutine is complete, all other goroutines exit A goroutine may not complete its execution because main completes early  Early Exit func main() { go fmt.Printf(\u0026#34;New routine\u0026#34;) fmt.Printf(\u0026#34;Main routine\u0026#34;) }  Only \u0026ldquo;Main routine\u0026rdquo; is printed Main finished before the new goroutine started  Delayed Exit func main() { go fmt.Printf(\u0026#34;New routine\u0026#34;) time.Sleep(100 * time.Milisecond) fmt.Printf(\u0026#34;Main routine\u0026#34;) }  Add a delay in the main routine to give the new routine a chance to complete \u0026ldquo;New RouteMainRoutine \u0026quot; is now printed  Timing with Goroutines  Adding a delay to wait for a goroutine is bad !! Timing assumptions may be wrong  Assumption: delay of 100 ms will ensure that goroutine has time to execute Maybe the OS schedules another thread Maybe the Go runtime schedules another goroutine   Timing is nondeterministic Need formal synchronization constructs  Synchronization  Using global events whose execution is viewed by all threads, simultaneously Want print to occur after update of x  Example\n   Task 1 Task 2     x=1    x=x+1    GB if GB print x     GLOBAL EVENT (GB) is viewed by all tasks at the same time Print must occur after update of x Synchronization is used to restrict bad interleavings  Threads in Go Threads in Go are generally handled using wait groups\nSync WaitGroup  Sync package contains functions to synchronize between goroutines sync.WaitGroup forces a goroutine to wait for other goroutines Contains an internal counter  Increment counter for each goroutine to wait for Decrement counter when each goroutine Waiting goroutine cannot continue until counter is 0    Using WaitGroup  Add() increments the counter Done() decrements the counter Wait() blocks until counter == 0  Example func foo(wg *sync.WaitGroup) { fmt.Printf(\u0026#34;New routine\u0026#34;) wg.Done() } func main() { var wg sync.WaitGroup wg.Add(1) go foo(\u0026amp;wg) wg.Wait() fmt.Printf(\u0026#34;Main Routine\u0026#34;) } Goroutine Communication  Goroutines usually work together to perform a bigger task Often need to send data to collaborate Example : Find the product of 4 integers  Make 2 goroutines, each multiplines a pair Main goroutine multiplies the 2 results   Need to send ints from main routine to the two sub-routines Need to send results from sub-routines back to main routine  Channels  Channels are used to make communication between goroutines Channels are typed Use make() to create a channel  c:=make(chan int)   Send and receive data using the \u0026lt;- operator Send data on a channel  C \u0026lt; - 3    Receive data from a channel x:= \u0026lt;- c  Example func prod(v1 int, v2 int, c chan int) { c \u0026lt;- v1*v2 } func main() { c:=make(chan int) go prod(1,2,c) go prod(3,4,c) a:= \u0026lt;-c b:= \u0026lt;-c fmt.Println(a*b) } Unbuffered Channel  Unbuffered channels cannot hold data in transit  Default is unbuffered   Sending blocks until data is received Receiving blocks until data is sent  Blocking and Synchronization  Channel communication is synchronous Blocking is the same as waiting for communication Receiving and ignoring the result is same as a Wait()  Buffered Channel Channel Capacity  Channel can contain a limited number of objects Default size 0 (unbuffered) Capacity is the number of objects, it can hold in transit Optional argument to make() defines channel capacity  c:=make(chan int, 3)    Sending only blocks if buffer is full Receiving only blocks if buffer is empty  Channel Blocking, Receive  Channel with capacity 1   First receive blocks until send occurs Second receive blocks forever  Channel Blocking, Send  Second send blocks until receive is done Receive can block until first send is done  Use of Buffering  Sender and receiver do not need to operate at exactly the same speed  Adding a channel to our goroutine Note that in this section notes are taken from: https://www.sohamkamani.com/blog/2017/08/24/golang-channels-explained\n A channel gives us a way to \u0026ldquo;connect \u0026quot; the different concurrent parts of our program. Channels can be thought of as \u0026ldquo;pipes\u0026rdquo; or \u0026ldquo;arteries\u0026rdquo; that connect the different concurrent parts of our code  Directionality out chan \u0026lt;- int `  The chan\u0026lt;- declaration tells us that you can only put stuff into the channel, but not receive anything from it. The int declaration tells us that the \u0026ldquo;stuff\u0026rdquo; you put into the channel can only be of the int datatype Although they look like seperate parts, chan\u0026lt;-int can be thought of as one datatype, that describes a \u0026ldquo;send-only\u0026rdquo; channel of integers. Similarly, an example of a \u0026ldquo;receive-only\u0026rdquo; channel declaration would look like:  out \u0026lt;- chan int   A channel can be declared without giving any directionatility, which means it can send or receive data.\n  Bi-directional channel can be created using following \u0026ldquo;make\u0026rdquo; statement\n  out :=make(chan int) After this section, notes are taken from: https://go101.org/article/channel.html if you prefer to have deep dive into channels, you may visit there.\n  Concurrent computations may share resources, generally memory resource. There are some circumstances may happen in a concurrent computing.\n  In the same period of one computation is writing data to a memory segment, another computation is reading data from the same memory segment. Then the integrity of the data read by the other computation might be not preserved.\n  In the same period of one computation is writing data to a memory segment, another computation is also writing data to the same memory segment. Then the integrity of the data stored at the memory segment might be not preserved.\n  These circumstances are called data races. One of the duties in concurrent programming is to control resource sharing among concurrent applications, so that data races will NOT happen. The ways to achieve this duty are called concurrency synchronization, or data synchronization. GO supports several data synchronization techniques. The following section will introduce one of them, channel.\nOther duties in concurrent programming include:\n Determine how many computations are needed Determine when to start, block, unblock and end a computation. Determine how to distribute workload among concurrent computations.  Most operations in Go are not synchronized. In other words, they are not concurrency-safe. These operations include value assignments, argument passing and container element manipulations. There are only a few operations which are synchronized, including the several to be introduced channel operations below.\nDO NOT COMMUNICATE BY SHARING MEMORY, SHARE MEMORY BY COMMUNICATING *(THROUGH CHANNELS)\nChannel Types and Values Like array, slice and map, each channel type has an element type. A channel can only transfer values of the element type of (the type of) the channel.\nChannel types can be bidirectional or single-directional. Assume T is an arbitrary type,\n chan T denotes a bidirectional channel type. Compilers allow both receiving values from and sending values to bidirectional channels. chan \u0026lt;-T denotes a send-only channel type. Compilers do not allow receiving values from send-only channels. \u0026lt;- chan T denotes a receive-only channel type. Compilers do not allow sending values to receive-only channels.  T is called element types of these channel types.\nValues of bidirectional channel type chan T can be implicitly converted to both send-only type chan \u0026lt;-T and receive-only type \u0026lt;-chan T , but not vice versa (even if explicitly). Values of send only type chan\u0026lt;-T cannot be converted to receive only type \u0026lt;-chan T.\nEach channel has capacity. A channel value with a zero capacity is called unbuffered channel and a channel value with a non-zero capacity is called buffered channel.\nFor more detailed explanation on channels you can visit https://go101.org/article/channel.html\nTake care ! ðŸ‘‹ðŸ»\nMaybe next time, topics could be revisited by examples ðŸ˜‰\n","permalink":"/posts/go-concur/","summary":"Concurrency in Go Concurrency in Go, makes Go programming language very unique and attractive compared to other languages, in this section I am going to share the notes which I took when I was watching coursera video series.\nIf you did not already check previous post on Go, it could be helpful to check it out first.\n Go Notes (OOP)  Specialization serie is Programming with Google Go Keep in mind that the notes are taken from several resources mainly from the course, however post may include other resources as well, I have referenced them when required, if nothing is referenced then it means, notes are taken from the course.","title":"Go notes (Concurrency) "},{"content":"In this post, I would like to share the notes that I took when I was completing following Go series education on Coursera. I can recommend it for anyone who would like to get rapid introduction to Go programming.\nProgramming with Google Go\nI should admit that although it looks fancy, nothing can be compared to actual development and contribution to open source projects. The course itself is quite interesting and contains very handy exercises regarding to Go development mentality. I would recommend it for anyone who has some interest in Go development and do not know where to start.\nThere will be following headers and subheaders which contains some notes on Go Programming language as bulletpoints.\nOOP in Go In Go, there is no such a concept of object oriented programming as in Java, however, it is possible to imply similar approach (object oriented approach) in Go using interfaces and structs.\nClasses There is no \u0026ldquo;Class\u0026rdquo; keyword in Go.\n Collection of data fields and functions that share a well-defined responsibility  Example: Point class Used in a geometry program Data: x coordinate, y coordinate Funtions:  DistToOrigin(), Quadrant() AddXOffSet(), AddYOffset()   Classes are template Contain data fields, not data    Classes are supported with structs in Go.\ntype Point struct { X float64 Y float64 }  Structs with methods, structs and methods together allow arbitrary data and functions to be composed.  func (p Point) DistToOrig() { t:= math.Pow(p.x,2) + math.Pow(p.y,2) return math.Sqrt(t) } func main() { p1 := Point(3,4) fmt.Println(p1.DistToOrig()) } Objects  Instance of a class Contains real data Example: Point Class  Encapsulation  Data can be protected from the programmer Data can be accessed only using methods Maybe we do not trust the programmer to keep data consistent Example: Double distance to origin  Option 1: Make method DoubleDist Option 2: Trust programmer to double X and Y directly    Encapsulation is supported as following ;\nCreate a package called data which has exported function which is PrintX, since it starts with capital letter, in Go, when something starts with capital letter, it means that it is exported\npackage data var x int=1 func PrintX() { fmt.Println(x) } Main package which is starting point of any application in Go.\npackage main func main() { data.PrintX() } Controlling access to Structs\n Hide fields of structs by starting field name with a lower-case letter. Define public methods which access hidden data  package data type Point struct{ x float64 y float64 } func (p *Point) InitMe(xn,xy float64) { p.x =xn p.y =xy } func (p *Point) Scale(v float64) { p.x=p.x*v p.y=p.y*v } func (p *Point) PrintMe() { fmt.Println(p.x,p.y) } Note that all methods are public ! Since their initial character is capital letter.\npackage main func main() { var p data.Point p.InitMe(3,4) p.Scale(2) p.PrintMe() }  Access to hidden fields can only be possible through public access.  Limitation of Method   Receiver is passed implicitly as an argument to the method\n  Method cannot modify the data inside the receiver\n  Example: OffsetX() should increase x coordinate\n  package main func main(){ p1:=Point(3,4) p1.OffsetX(5) } Large Receivers\n If receiver is large, lots of copying is required  type Image [100] [100]int func main() { i1 := GrabImage() i1.BlurImage() } 10.000 ints copied to BlurImage() (Pitfalls)\nPointer Receivers\nfunc (p *Point) OffsetX (v float64) { p.x=p.x+v }  Receiver can be a pointer to a type Call by reference, pointer is passed to the method  Point Receivers, Referenceing, Dereferencing  No need to dereference  func (p *Point) OffsetX (v int) { p.x=p.x+v }   Point is referenced as p, not *p\n  No need to reference\n  package main func main() { p:=Point{3,4} p.OffsetX(5) fmt.Println(p.x) }  Do not need to reference when calling the method  Good Programming Practices\n All methods for type have pointer receivers or All methods for a type have non-pointer receivers Mixing pointer/non-pointer receivers for a type will get confusing !  Pointer receiver allows modification    Polymorphism  Ability for an object to have different forms depending on the context Example Area() function  Rectangle area is base * height Triangle area is 0.5*base*height   Identical at a high level of abstraction Different at a low level of abstraction  Inheritance (No Inheritance in GoLang)   Sublcass inherits the methods/data of the superclass\n  Example: Speaker superclass\n Speak() method, pring \u0026quot;\u0026lt;noise\u0026gt; \u0026quot;     Subclassses Cat and Dog\n Also have the Speak() method    Cat and Dog are different forms of speaker\n  Remember: Go does not have inheritance\n  Overriding   Subclass redefines a method inherited from the superclass\n  Example: Speaker, Cat, Dog\n Spekaer Speak() prints \u0026ldquo;\u0026rdquo; Cat Speak() prints \u0026ldquo;meow\u0026rdquo; Dog Speak() prints \u0026ldquo;woof\u0026rdquo;    Speak() is polymorphic\n Different implementations for each class Same signature (name, params and return)    Interface   Set of method signatures\n Name, parameters, return values Implementation is NOT defined    Used to express conceptial similarity between types.\n  Example : Shape2D interface\n  All 2D shapes must have Area() and Perimeter()\n  Satisfying an Interface  Type satisfies an interface if type defines all methods specified in the interface.  Same method signatures   Rectangle and Triangle types satisfy the Shape2D interface  Must have Area() and Perimeter() methods Additional methods are OK.   Similar to inheritance with overriding.  Example type Shape2D interface { Area() float64 Perimeter() float64 } type Triangle {...} func (t Triangle) Area() float64 {....} func (t Triangle) Perimeter() float64 {....}  Triangle type satisfies the Shape2D interface No need to state it explicitly  Interface vs Concrete Types Concrete Types  Specify the exact representation of the data and methods Complete method implementation is included  Interface Types  Specifies some method signatures Implementations are abstracted  Interface Values  Can be treated like other values  Assigned to variables Passed, returned   Interface values have two components   Dynamic Type : Concrete type which it is assigned to Dynamic Value: Value of the dynamic type  Defining an interface type type Speaker interface { Speak()} type Dog struct {name string } func (d Dog) Speak() { fmt.Println(d.name) } func main() { var s1 Speaker var d1 Dog{\u0026#34;Brian\u0026#34;} s1=d1 s1.Speak()   Dynamic type is Dog and dynamic value is d1.\n  An interface can have a nil dynamic value\n  var s1 Speaker var d1 *Dog s1=d1  d1 has no concrete value yet s1 has a dynamic type but no dynamic value  Nil Dynamic Value  Can still call the Speak() method of s1 Does not need a dynamic value to call Need to check inside the method  func (d *Dog)Speak() { if d==nil{ fmt.Println(\u0026#34;\u0026lt;noise\u0026gt;\u0026#34;) }else{ fmt.Println(d.name) }\t} var s1 Speaker var d1 *Dog s1=d1 s1.Speak() // it works, since s1 is mapped to d1 Nil Interface Value  Interface with nil dynamic type Very different from an interface with a nill dynamic value  Nil dynamic value and valid dynamic type\nvar s1 Speaker var d1 *Dog s1=d1  Can call a method since type is known, Nil dynamic type  var s1 Speaker *(there is no actually method to call)  Cannot call a method, runtime error  No dynamic type and no dynamic value then you cannot call the interfaceâ€¦\nUsing Interfaces  Need a function which takes multiple types of parameter Function foo() parameter  Type X or Type Y   Define interface Z foo() parameter is interface Z Types X and Y satisfy Z Interface methods must be those needed by foo()  Example Interface for Shapes Pool in a Yard\n I need to put a pool in my yard Pool need to fit in my yard  Total area must be limited   Pool needs to be fenced  Total perimeters must be limited   Need to determine if a pool shape satisfies criteria FitInYard()  Takes a shape as argument Returns true if the shape satisfies criteria   FitInYard()  Many Possible shape types  Rectangle, triangle, circle     FitInYards() should take many shape types Valid shape types must have  Area() Perimeter()   Any shape with these methods is OK.  type Shape2D interface { Area() float64 Perimeter() float64 } type Triangle {...} func (t Triangle) Area() float64 {...} func (t Triangle) Perimeter() float64 {...} type Rectangle {...} func(t Rectangle) Area() float64 {...} func (t Rectangle) Perimeter() float64 {....}   Rectangle and Triangle satisfy Shape2D interface.\n  FitInYard() Implementation\n  func FitInYard(s Shape2D) bool { if (s.Area() \u0026gt; 100 \u0026amp;\u0026amp; s.Perimeter() \u0026gt; 100) { return true } return false } Empty Interface  Empty interface specifies no methods All types satisfy the empty interface Use it to have a function accept any type as a parameter  func PrintMe(val interface{} ) { fmt.Println(val) } Type Assertions Concealing Type Differences__  Interfaces hide the differences between types  func fitInYard(s Shape2D)bool { if (s.Area() \u0026gt;100 \u0026amp;\u0026amp; s.Perimeter()\u0026gt;100){ return true } return false }  Sometimes you need to treat different types in different ways  Exposing Type Differences   Example: Graphics program\n  DrawShape() will draw any shape\nfunc DrawShape (s Shape2D) {..... }   Underlying API has different drawing functions for each shape\nfunc DrawRect (r Rectangle) {.... func DrawTriangle(t Triangle) {...   Concrete type of shape s must be determined\n  Type Assertions for Disambiguation  Type assertions can be used to determine and extract the underlying concrete type  func DrawShape(s Shape2D) bool { rect,ok :=s.(Rectangle) if ok { DrawRect(rect) } tri,ok := s.(Triangle) if ok { DrawRect(tri) } }  Type assertion extracts Rectangle from Shape2D  Concrete type in parentheses   If interface contains concrete type  rect == concrete type, ok == true   If interface does not contain concrete type  rect==zero, ok==false    Type Switch  Switch statement used with a type assertion  func DrawShape(s Shape2D) bool { switch:= sh:=s.(type) { case Rectangle: DrawRect(sh) case Triangle: DrawTri(sh) } } Error Interface  Many Go programs return error interface objects to indicate errors  type error interface { Error() string }  Correct operation : error==nil  Incorrect operation: Error() print error message  Handling Errors f,err := os.Open(\u0026#34;/harris/text.txt\u0026#34;) if err!=nil { fmt.Println(err) return }  Check whether the error is nil If it is not nil, handle it fmt package calls the error() method to generate string to print  Keep in mind that the topics which are mentioned on this post is just brief summary, it means that all subheaders and headers can be extended to any size, however these are just bulletpoints and overall information in Functions, Methods, and Interfaces in Go module of specialization serie.\nIn next post, notes which are taken from concurrency module of the specialization serie will be posted.\nTake care ! ðŸ‘‹ðŸ»\n","permalink":"/posts/go-notes/","summary":"In this post, I would like to share the notes that I took when I was completing following Go series education on Coursera. I can recommend it for anyone who would like to get rapid introduction to Go programming.\nProgramming with Google Go\nI should admit that although it looks fancy, nothing can be compared to actual development and contribution to open source projects. The course itself is quite interesting and contains very handy exercises regarding to Go development mentality.","title":"Go notes (OOP topics) "},{"content":"Why VPN? It is crucial to do not expose your personal details or not being attacked by someone when you are connected to public endpoints such as coffee, airport, hotel and guest WIFI points. Furthermore, sometimes, people require to have organizatinal VPN access if organization itself does not provide one. For instance when students in universities have taken into consideration, it is quite important to reach resources that university is providing, it could be IEEE library access or enclosed resource which is only exclusive to internal network of the organization.\nNo matter what is your intention to use VPN, it is quite useful to have in any case.\nThere are lots of tutorials and explanations regarding to setting up WireGuard however I could not manage to find proper instructions for MacOSX users that\u0026rsquo;s why I thought it could be useful to pack up in once in a post. I hope, this would be handy for anyone who are seeking for brand new tutorial for using WireGuard on MacOSx.\nPreliminary information; there is no such a concept of server and client in WireGuard, all devices are called as peers. However, in sake of understanding better, I will refer virtual private server as WireGuard server however, it is actually a PEER !\nPrerequirement Wireguard is a decent VPN solution with all recent crypto features compared to other open source VPN solutions furthermore, it is lightweight.\nWireguard works based on key-pair relation between server and client just like ssh connection. The setup for personal usage is quite simple to do. Moreover, it is super fast, simple and more performant than any other open source VPN solution. Let\u0026rsquo;s start build our own VPN solution using WireGuard.\n Prepare the environment to setup WireGuard  Following scenario is made on ubuntu 18.04, however almost for any linux distribution the steps are more or less same.\n  Virtual private server (VPS), cheapest VPS with ubuntu 18.04 could be enough to get your own personal VPN. It can be rented over ;\n Google Cloud: Google provides $300 for 12 months for new comers to Google Cloud. Amazon AWS: Amazon has some free tiers which could be suitable for running your own VPN solution. Check it out from their website. Digital Ocean: Provides cheapest VPS on demand like $5 per month, which could be enough for personal VPN. Furthermore, you can claim $50 for Digital Ocean from Github Students Package, if you have student account on Github. Microsoft Azure: If you have Github student account, you can take some advantages of Microsoft cloud services as well. Check it out from Github Student Pack    Setup Peer 1  [-Peer 1 is the Server, which will route all or some of your network traffic- ]  I assumed that you have bought your VPS and ready to go for installation steps for WireGuard. Make sure that you have done with your SSH connection.\n$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y $ sudo add-apt-repository ppa:wireguard/wireguard $ sudo apt-get update $ sudo apt-get install wireguard Syncing Wireguard with kernel updates, which means that whenever there is an update for your linux kernel, wireguard module will be updated too.\n$ sudo modprobe wireguard Afterwards, you can ensure that the module is loaded as following;\n$ lsmod | grep wireguard wireguard 217088 0 ip6_udp_tunnel 16384 1 wireguard udp_tunnel 16384 1 wireguard Generate Keys Keys are backbone of WireGuard, extremely important step to take into.\n$ cd /etc/wireguard $ umask 077 $ wg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey After given commands above, you will have publickey and privatekey in /etc/wireguard/ directory.\nSetup Configuration In this step, an interface should be defined in order to route all traffic from clients over rented VPS. Common interface for WireGuard is wg0.\nCreate a configuration file on VPS as /etc/wireguard/wg0.conf through your favorite text editor (vim, nano, vi).\n[Interface] PrivateKey = \u0026lt;generated-private-key-here\u0026gt; Address = 10.120.120.2/24 Address = fd86:ea04:1111::1/64 SaveConfig = true PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o ens3 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o ens3 -j MASQUERADE; ip6tables -D FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -D POSTROUTING -o ens3 -j MASQUERADE ListenPort = 51820   ens3 is main network interface fo your server, if it is different change it with correct one, you can observe your by typing ifconfig.\n  PrivateKey is the key which was generated in generate keys step.\n  Forward Traffic Traffic which comes into VPS should be forwarded properly, in order to set it up, change /etc/sysctl.conf file as follows.\nUsing your text editor, add following two lines of configuration into /etc/sysctl.conf\nnet.ipv4.ip_forward=1 net.ipv6.conf.all.forwarding=1 After that, to activate them immeditaly, type;\n$ sysctl -p It is time to bring VPN interface up;\n$ wg-quick up wg0 If there is no problem or complain when the command above called, you should be able to get following results from given command below.\n$ wg interface: wg0 public key: loZviZQpT5Sy4gFKEbk6Vc/rcJ3bH84L7TUj4qMB918= private key: (hidden) listening port: 51820 Setup Peer 2 (-client-) In my case, I will gonna go over for setup for MacOS client, however, the logic remains same for any other client that you will use.\nAll required appliations are downloadable from given adress below.\n Applications for all distribution  After installation of your application from given official website (https://www.wireguard.com/install/), it is time to add peers to server, and configure Peer 2 (client).\nSetup Configuration (client) If you are using MacOsX, you need to add following configuration under /usr/local/etc/wireguard/wg0.conf, however if you are using Linux, /etc/wireguard/wg0.conf would be correct place to put the configuration given below.\n[Interface] Address = 10.120.120.2/32 Address = fd86:ea04:1111::2/128 # note that privatekey value is just a place holder PrivateKey = KIaLGPDJo6C1g891+swzfy4LkwQofR2q82pFR6BW9VM= DNS = 1.1.1.1 [Peer] PublicKey = \u0026lt;your server public key\u0026gt; Endpoint = \u0026lt;your server public ip\u0026gt;:51820 AllowedIPs = 0.0.0.0/0, ::/0 If you have already installed GUI application from official installation website of WireGuard (https://www.wireguard.com/install/), it could be nice to arrange that configuration file as shown below.\nYou will be able to create your configuration file through WireGuard GUI application as follows.\nNote that the PrivateKey value is just a placeholder, it is not valid value to put into, you need to take care of your own private key which is generated by WireGuard.\n PublicKey : This should be same with the public key when you have generated the keys on server in this step PrivateKey: This is the key which is generated by WireGuard when you start to create new ocnfiguration tunnel from the applicaiton that you have installed from official site. EndPoint: IP address or A record of your VPS.  Add Peer When you are done with setting up configuration in client side, you need to let your other peer (server) know your device. It can be achieved by following command on server.\n$ wg set wg0 peer \u0026lt;client-public-key\u0026gt; allowed-ips 10.120.120.2/32,fd86:ea04:1111::2/128  client-public-key: It is a placeholder, change it with your own public key which can be seen in your WireGuard application, e.g; it is shown in given figure above.  When everything goes fine, you will be able to list peers by executing following command on server side.\n$ wg interface: wg0 public key: loZviZQpT5Sy4gFKEbk6Vc/rcJ3bH84L7TUj4qMB918= private key: (hidden) listening port: 51820 peer: Ta9esbl7yvQJA/rMt5NqS25I/oeuTKbFHJu7oV5dbA4= allowed ips: 10.120.120.2/32, fd86:ea04:1111::2/128 Last but not least step is to up wg0 interface on client side.(peer 2)\n$ wg-quick up wg0 Now you can check your IP address location through https://whatismyipaddress.com/\nFirewall configuration It can be handy to enable ufw and allow port 51820/udp for traffic routing.\n Check ufw status  $ ufw status verbose If not enabled, enable it by ; (before enabling ufw make sure that you have already allowed port 22 otherwise you may face some problems in return)  $ ufw enable Allow port 51820/udp  $ ufw allow 51820/udp Sometimes it is required to setup iptables, if that is the case use following rule.\n$ iptables -A INPUT -p udp -m udp --dport 51820 -j ACCEPT $ iptables -A OUTPUT -p udp -m udp --sport 51820 -j ACCEPT Note: Run commands with a user which has root privileges\nHow Traffic Looks Like Following figures are just snippet of some network traffic which is captured by WireShark. In the first figure, WireGuard is disabled, however in second image, WireGuard is enabled, you can easily distinguish the difference.\nIn ordinary usage (without VPN), your network traffic is seen as follows;\nWhen you have configured your WireGuard to route all your traffic, here is how it is look like;\nIt can be observed that all the traffic which goes out from client side has been encrypted with WireGuard.\nAll in all, it is quite handy to have your own private VPN access which decreases the likelihood of being attacked. Apart from VPN capabilities of WireGuard, it stabilize WIFI connections and actually improves quality of WiFi connection on clients. Keep in mind that you need to create exclusive key pairs for each client who would like to route all or some of his/her traffic over VPN.\nReferences  Quick Start Guide Installation Guide Unofficial WireGuard Docs Performance Limitation  ","permalink":"/posts/setup-free-vpn/","summary":"Why VPN? It is crucial to do not expose your personal details or not being attacked by someone when you are connected to public endpoints such as coffee, airport, hotel and guest WIFI points. Furthermore, sometimes, people require to have organizatinal VPN access if organization itself does not provide one. For instance when students in universities have taken into consideration, it is quite important to reach resources that university is providing, it could be IEEE library access or enclosed resource which is only exclusive to internal network of the organization.","title":"Free VPN [WireGuard] Setup  "},{"content":"Summary In this post, one of the well known (-open to discuss :)-) error of \u0026ldquo;No space left on device\u0026rdquo; which is caused due to Docker will be solved with different approaches.\nNote: \u0026ldquo;No space left on device\u0026rdquo; error can be caused due to any other reason than docker itself. Hence, it would be nice to make sure that the error is caused due to docker volumes.\nYou can check whether it is caused due to docker volumes or not by following steps over here\nNote that the explanations and examples may differ according to your environment, hence instead of taking these information as rules, applying them as a reference would be much better approach.\nThe system information for particular scenario described on post is given below:\nSystem information  PRETTY_NAME: Debian GNU/Linux 9 (stretch) NAME: Debian GNU/Linux VERSION_ID: 9 VERSION: 9 (stretch) KERNEL_RELEASE: 4.9.0-6-amd64  Docker daemon version Engine:\n Version: 19.03.8 API version: 1.40 (minimum version 1.12) Go version: go1.12.17 Git commit: afacb8b7f0 Built: Wed Mar 11 01:24:36 2020 OS/Arch: linux/amd64 Experimental: false  Although, it is NOT fully required to have same or somehow closer version of OS and Docker daemon, it could be nice to know exactly which conditions the following fix can work.\nMost of the cases, the following information could be valid for all Debian based systems however it is not for sure.\nThe Problem Docker can be used for most of the cases although now Kubernetes or any other orchestration systems preffered to used. That kind of orchestration systems may not cause this problem, hence they mostly provide auto scaling and runs on cloud. However, when you have a server with all responsibility, you may face this exact problem.\nThe problem is installation of docker into any system with default settings may create head ache in the future. If you are planning to use docker, intensively, the main reason of that, docker is generally using\n /var/lib/docker  place for docker volumes. In most scenarios, servers do not use root path for storing information, instead the appropriate approach for storing information on server is creating data volumes which has huge capacity and ability to extend or shrink time to time. The problem starts to show off itself when docker containers have been used for long period of time.\nEnsure about the problem It is quite handy to check whether the error of \u0026ldquo;No space left on device\u0026rdquo; caused by docker volumes. To do that following simple bash commands can be used.\n$ df -h /var/lib This will display free and used disk space for /var/lib path. Afterwards, you can see how is the difference between free and used spaces among usage percentage. An example is given below, it is taken from the system that I mentioned at the beginning in system information section.\n$ df -h /var/lib Filesystem Size Used Avail Use% Mounted on /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/mapper/data-data 9.1T 402G 8.2T 5% /data udev 252G 0 252G 0% /dev /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / proc 0 0 0 - /proc /dev/sda3 173G 162G 2.4G 99% / tmpfs 51G 275M 51G 1% /run /dev/sda3 173G 162G 2.4G 99% / /dev/nvme0n1 1.5T 775G 618G 56% /scratch /dev/sda3 173G 162G 2.4G 99% / sysfs 0 0 0 - /sys /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / /dev/sda3 173G 162G 2.4G 99% / As you can observe from the output above, I have plenty of spaces for volume /dev/nvme0n1 and /dev/mapper/data-data, however I was getting \u0026ldquo;No space left on the device\u0026rdquo; error, because root path became full due to docker volumes.\nIt can be quickly checked by the command below.\n$ df /var/lib/docker Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda3 180572828 166898524 4432040 98% / Temporary solution A temporary solution would be pruning docker volumes by running ;\n$ docker volume prune it will prune all volumes which are NOT in-use, if the volumes which are creating this bunch of data are in-use, prune command will not have any effect on the error. Although it works, it is a temporary solution, it may re-trigger the error in future. To resolve this issue permanently following approach could be used: Permanent Solution\nThe approach of having temporary solution can be extended with cronjobs, however, it is definitely not nice way of handling issues.\nHowever, it could be nice to have cron jobs which prune docker system time to time, because old docker images, containers and some other leftovers can create bunch of reclaimable storage. Docker prune commands do not have effect on resources (containers, volumes and networks) which are under use.\nYou can setup a cronjob which weekly prunes system, as provided below.\n$ crontab -e 0 0 * * 0 docker system prune -f # this will run at 00:00 on Sunday everyweek 0 0 * * 0 docker volume prune -f # will remove all (NOT in use) volumes weekly 0 0 * * 0 docker network prune -f # will remove all (NOT in use) networks  You can write a script according to your needs then place it to cron job as well. Before adding cron jobs, make sure $USER has valid permissions for docker group.\nNow, we are sure that the error caused due to docker volumes which means we can proceed with a permanent solution.\nIf it the error is not caused due to docker volumes, it could be caused from running short of inodes or basically not enough storage area.\nPermanent Solution Permanent solution is using one of storage volume for storing docker volumes, instead of using root path. These are the steps that you may take;\nStop docker service !\n$ systemctl stop docker Rsync all data under /var/lib/docker to other directory under storage volume\n$ mkdir -p /data/mnt # creates plave to use for docker volumes $ rsync -a /var/lib/docker /data/mnt/ # will sync everything  Rename default docker volumes path, this for taking a backup until we have success at the end.\n$ mv /var/lib/docker /var/lib/dockerbckp Create symbolic link to actual place\n$ ln -s /data/mnt/docker /var/lib/docker Enable DOCKER_OPTS\n$ vim /etc/default/docker DOCKER_OPTS=\u0026#34;--dns 8.8.8.8 --dns 8.8.4.4 -g /data/mnt/docker\u0026#34; Uncomment DOCEKR_OPTS add -g /data/mnt/docker as shown above in /etc/default/docker\nStart Docker daemon\n$ systemctl start docker If there was no error during implementation of steps, you can now test your setup.\nNote that thee should NOT be space between curly brackets when listing mount point only.\n$ docker volume create 2f2bd462b89c39bb641e7daf01048c5d811dd7796d7f89d250ea82c4532d2707 $ docker volume inspect --format { {.Mountpoint} } 2f2bd462b89c39b641e7daf01048c5d811dd7796d7f89d250ea82c4532d2707 /data/mnt/docker/volumes/2f2bd462b89c39bb641e7daf01048c5d811dd7796d7f89d250ea82c4532d2707/_data As you can observe above docker is using /data/mnt/docker for docker volumes, now we can safely remove old backup data. rm -rf /var/lib/dockerbckp\nI would like to mention that there is an information about changing docker volume instead implementing all steps mentioned above, just specifiying path under /etc/docker/daemon.js file would work (-example daemon.js is given-). However, when I tried that approach, it did not worked.\nExample\n$ vim /etc/docker/daemon.js { \u0026#34;data-root\u0026#34;: \u0026#34;/mnt/docker-data\u0026#34;, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } Check this documentation: https://docs.docker.com/config/daemon/systemd/I hope, this post can help others to find required information quickly and implement necessary steps to get to work.\nReferences:  https://github.com/moby/moby/issues/10613 https://github.com/moby/moby/issues/10613 https://success.docker.com/article/error-message-no-space-left-on-device-in-default-machine https://forums.docker.com/t/how-do-i-change-the-docker-image-installation-directory/1169  ","permalink":"/posts/no-space-left-on-device/","summary":"Summary In this post, one of the well known (-open to discuss :)-) error of \u0026ldquo;No space left on device\u0026rdquo; which is caused due to Docker will be solved with different approaches.\nNote: \u0026ldquo;No space left on device\u0026rdquo; error can be caused due to any other reason than docker itself. Hence, it would be nice to make sure that the error is caused due to docker volumes.\nYou can check whether it is caused due to docker volumes or not by following steps over here","title":"no space left on this device "},{"content":"GiriÅŸ Elasticsearch Ã¼zerinde bÃ¼yÃ¼k boyuttaki verileri hÄ±zlÄ± bir ÅŸekilde iÅŸlemek Ã§aba gerektiren iÅŸlerden biridir. Bu yazÄ±da bir Ã§alÄ±ÅŸma esnasÄ±nda yapÄ±lan elasticsearch performans iyileÅŸtirmelerini ve nasÄ±l yapÄ±ldÄ±ÄŸÄ±nÄ± anlatmaya Ã§alÄ±ÅŸacaÄŸÄ±m.\nBu iyileÅŸtirme iÅŸlemlerinin nasÄ±l yapÄ±ldÄ±ÄŸÄ±na geÃ§meden once elasticsearch mimarisinde bulunan bazÄ± bileÅŸenlerden bahsetmekte yarar var.\n Cluster : Elasticsearch bir veya birden fazla bilgisayarda entegre ÅŸekilde Ã§alÄ±ÅŸabilir ve bu elasticsearch Ã¼n Ã§alÄ±ÅŸtÄ±ÄŸÄ± makinelere NODE denir. Cluster (KÃ¼me) ise bu nodeâ€™larÄ±n oluÅŸturduÄŸu gruba verilen yapÄ±ya denir. Index : Elasticsearch Ã¼zerinde veriler indexlerde tutulur, index basit olarak dÃ¶kÃ¼manlarÄ±n toplandÄ±ÄŸÄ± ve tutulduÄŸu yapÄ±dÄ±r. Shard: Elasticsearch Ã¼ birden fazla makine Ã¼zerinde (sanal veya fiziksel makine) tutabildiÄŸimizden dolayÄ±, indekslerde tutulan veriler bu cluster adÄ± verdiÄŸimiz ortamlarda daÄŸÄ±tÄ±k (distributed) ÅŸekilde tutulur. Bu iÅŸlemin yÃ¶netim kÄ±smÄ±nÄ± elasticsearch otomatik olarak halleder. Replica: Elasticsearch normalde (default) her indeks iÃ§in 5 ana shard ve 1 replica oluÅŸturur, yani her bir indeks 5 adet shardâ€™a sahip ve her shard bir replica iÃ§ermektedir. AÅŸaÄŸÄ±da bu durumu gÃ¶steren bir ekran gÃ¶rÃ¼ntÃ¼sÃ¼ verilmiÅŸtir.  http://\u0026lt;elk-ip\u0026gt;:9200/_cat/shards Bu performans iyileÅŸtirme adÄ±mlarÄ±, tek sunucu (node) Ã¼zerinde Ã§alÄ±ÅŸan Elasticsearch Ã¼zerinde yapÄ±lmÄ±ÅŸtÄ±r, yani daÄŸÄ±tÄ±k bir sistem Ã¼zerinde iyileÅŸtirme yapmak buradaki anlatÄ±lacaklardan farklÄ± olacaktÄ±r. (BazÄ± kÄ±sÄ±mlarÄ± benzerlik gÃ¶sterse dahi)\nNot : Bu iyileÅŸtirme iÅŸlemleri, Ã§alÄ±ÅŸtÄ±rÄ±lan sunucu sayÄ±sÄ±na (daÄŸÄ±tÄ±k sistem ise), internet hÄ±zÄ±na (daÄŸÄ±tÄ±k sistem ise), sunucuda Ã§alÄ±ÅŸan iÅŸletim sisteminden, kullandÄ±ÄŸÄ± disk, CPU ve RAM kapasitesine gÃ¶re deÄŸiÅŸiklik gÃ¶sterebilir.Paralel Bulk indeksleme Elasticsearch Ã¼zerinde indeksleme iÅŸlemi birkaÃ§ farklÄ± yÃ¶ntem ile yapÄ±labilmektedir bunlardan bazÄ±larÄ±, tek tek indeksleme, bulk indeksleme ve parallel indekslemedir.\nTek tek indeksleme yÃ¶ntemi, tahmin edeceÄŸiniz Ã¼zere veri bÃ¼yÃ¼k olduÄŸunda tercih edilecek bir yÃ¶ntem deÄŸildir, nedeni ise her kayÄ±t iÃ§in elasticsearche istekte bulunmasÄ±ndan dolayÄ±dÄ±r. Yani 10000 adet satÄ±r iÃ§in 10000 istek gÃ¶nderilecek demektir, bunun yerine bulk indeksleme tercih edilir 10000 adet kayÄ±t iÃ§in tek istek gÃ¶nderimi yapar bÃ¶ylece hem istek sayÄ±sÄ± minimuma indirilmiÅŸ olur, hem de indeksleme sÃ¼resi azaltÄ±lmÄ±ÅŸ olur. Bunun bir adÄ±m daha geliÅŸmiÅŸi ise paralel bulk tÄ±r, bu indeksleme yÃ¶nteminde ise birden fazla thread ile veri elasticsearche gÃ¶nderilecektir saÄŸlar. Bizim Ã§alÄ±ÅŸmamÄ±zda paralel bulk iÅŸlemi kullanÄ±lmÄ±ÅŸtÄ±r.\nBu Ã§alÄ±ÅŸmada, Elasticsearch Ã¼n Python modÃ¼lleri kullanÄ±lmÄ±ÅŸtÄ±r, bu modÃ¼lde paralel bulk kullanÄ±mÄ± aÅŸaÄŸÄ±daki ÅŸekildedir.\nParalel bulk kullanabilmek iÃ§in Python generator kullanmak tercih edilen yÃ¶ntemlerden biridir, nedeni hem ram kullanÄ±mÄ± az olur, hemde tekrarlÄ± (iterate) bir yapÄ±ya sahiptir.\nÃ–rnek generator yapÄ±sÄ± :\ndef gendata(docs_list): for json in docs_list: yield { \u0026#34;_index\u0026#34;: \u0026#34;herhangibirsey\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;isim\u0026#34;:json[\u0026#39;isim\u0026#39;], \u0026#34;soyisim\u0026#34;:json[\u0026#39;soyisim\u0026#39;], \u0026#34;sehir\u0026#34;: json[\u0026#39;sehir\u0026#39;], \u0026#34;yas\u0026#34;:json[\u0026#39;yas\u0026#39;], \u0026#34;meslek\u0026#34;:json[\u0026#39;meslek\u0026#39;] } Bu generator yapÄ±sÄ±nda, gendata fonksiyonu docs_list adÄ±nda bir liste alÄ±yor ve bu listenin iÃ§eriÄŸi ÅŸu ÅŸekilde olduÄŸunu varsayÄ±yoruz:\ndocs_list= [{\u0026#34;isim\u0026#34;: \u0026#34;Mehmet\u0026#34;,\u0026#34;soyisim\u0026#34;: \u0026#34;Ataklar\u0026#34;,\u0026#34;sehir\u0026#34;: \u0026#34;Kocaeli\u0026#34;,\u0026#34;yas\u0026#34;: 45,\u0026#34;meslek\u0026#34;: \u0026#34;Ogretmen\u0026#34;}] gendata fonksiyonu docs_list listesi icerisindeki her bir dokumandan gereken alanlari alarak indeksleme fonksiyonuna vermektedir. Parallel bulk, Python script Ã¼zerinden ÅŸu ÅŸekilde Ã§aÄŸrÄ±labilir.\nfor response in parallel_bulk(elasticDeamon, gendata(doc_records), thread_count=7): pass Indeks yenileme aralÄ±ÄŸÄ±nÄ± kaldÄ±rma (refresh_interval) Node Ã¼zerinde bulunan indeks e, bulk indexleme iÅŸlemi yapÄ±lÄ±rken, indeks yenileme aralÄ±ÄŸÄ± bulk indeksleme sÃ¼resi boyunca ortadan kaldÄ±rÄ±lmalÄ±dÄ±r. Ã‡Ã¼nkÃ¼ elasticsearch Ã¼n her yenileme yapmasÄ± sunucu Ã¼zerinde segment oluÅŸturmasÄ±nÄ± saÄŸlamaktadÄ±r, bu hem makinen kaynaklarÄ±na dezavantaj olarak yansÄ±maktadÄ±r, ram ve cpu kullanÄ±mÄ±nÄ± artÄ±ran pahalÄ± bir iÅŸlemdir.\nKibana Ã¼zerinde bulunan â€œDev Toolsâ€ kÄ±smÄ±ndan aÅŸaÄŸÄ±daki verilen komut ile kaldÄ±rÄ±labilir.\nPUT /\u0026lt;indeks-ismi\u0026gt;/_settings { \u0026#34;index\u0026#34;: { \u0026#34;refresh_interval\u0026#34;: -1 } } Terminal Ã¼zerinden:\ncurl -X PUT \u0026#34;\u0026lt;elk-ip\u0026gt;:9200/\u0026lt;index-ismi\u0026gt;/_settings\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;index\u0026#34; : { \u0026#34;refresh_interval\u0026#34; : -1 } } \u0026#39; Bulk indeksleme iÅŸlemi sona erdiÄŸinde ise, aynÄ± komutlar kullanÄ±larak, yenileme aralÄ±ÄŸÄ± â€œnullâ€ a eÅŸitlenebilir. BÃ¶ylece kullanÄ±cÄ± kibana Ã¼zerinden, yenileme aralÄ±ÄŸÄ±nÄ± kendisi ayarlayabilir.\nPUT /\u0026lt;index-ismi\u0026gt;/_settings { \u0026#34;index\u0026#34;: { \u0026#34;refresh_interval\u0026#34;: null } } Indeks kopyalarÄ±nÄ± devre dÄ±ÅŸÄ± bÄ±rakmak (Replica) KulaÄŸa hoÅŸ gelmesede indeks kopyalarÄ±nÄ± (replicas) devre dÄ±ÅŸÄ± bÄ±rakmak indeksleme hÄ±zÄ±nÄ± artÄ±rÄ±r, en bÃ¼yÃ¼k dezavantajÄ± indeksi herhangi bir hata durumunda veri kaybÄ±na karÅŸÄ± savunmasÄ±z bÄ±rakÄ±r.\nKibana â€œDevToolsâ€ kÄ±smÄ±ndan kopyalarÄ±n devre dÄ±ÅŸÄ± bÄ±rakÄ±lmasÄ±.\nPUT /\u0026lt;indeks-ismi\u0026gt;/_settings { \u0026#34;index\u0026#34; : { \u0026#34;number_of_replicas\u0026#34; : 0 } } Terminal Ã¼zerinden:\ncurl -X PUT \u0026#34;\u0026lt;elk-ip\u0026gt;:9200/\u0026lt;index-ismi\u0026gt;/_settings\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;index\u0026#34; : { \u0026#34;number_of_replicas\u0026#34; : 0 } } \u0026#39; Swap alanÄ±nÄ± kaldÄ±rmak. (Sunucu Ã¼zerindeki) ElasticsearchÃ¼ hÄ±zlÄ± yapan faktÃ¶rlerden en Ã¶nemlisi ram Ã¼zerinden iÅŸlem yapmasÄ±dÄ±r. Linux sunucularÄ±nda bulunan swap alanÄ±, ram de yeterli alan kalmadÄ±ÄŸÄ±nda veya ram Ã¼zerinde uzun sÃ¼re iÅŸlem yapÄ±lmayan (aktif olmayan) dosyalarÄ±n disk Ã¼zerinde kÄ±sa sÃ¼reliÄŸine saklanmasÄ±ndan oluÅŸan alandÄ±r. Bu elasticsearh iÃ§in dezavantaj olabilmektedir, elasticsearchÃ¼n tamamen ram Ã¼zerinden iÅŸlem yapmasÄ±nÄ± saÄŸlamak adÄ±na swap alanÄ±nÄ± kaldÄ±rmak indeksleme ve arama yapma hÄ±zÄ±nÄ± artÄ±racaktÄ±r.\nSwap alanÄ±nÄ± geÃ§ici olarak ÅŸu ÅŸekilde kaldÄ±rabilirsiniz, terminal Ã¼zerinden bu komutu yazmanÄ±z yeterlidir.\n$ swapoff -a Swap alanÄ±nÄ± tamamen kaldÄ±rabilmek iÃ§in â€œrootâ€ yetkisi ile /etc/fstab dosyasÄ± iÃ§erisinde swap kelimesi geÃ§en kÄ±smÄ± yorum satÄ±rÄ± yapmanÄ±z yeterli olacaktÄ±r.\nSwap alanÄ±nÄ± ortadan kaldÄ±rdÄ±ktan sonra sunucu Ã¼zerinde Ã§alÄ±ÅŸan elasticsearch ayarlarÄ±nda ufak bir deÄŸiÅŸiklik yapmak gerekecektir.\n/etc/elasticsearch/elasticsearch.yml\nelasticsearch.yml dosyasÄ± iÃ§erisine ÅŸu parametreyi eklemeniz gerekmektedir.\nbootstrap.mlockall: true Bu iÅŸlem elasticsearch Ã¼n tamamen RAM Ã¼zerinden iÅŸlem yapmasÄ±nÄ± saÄŸlayacaktÄ±r.\nJVM Heap AlanÄ±nÄ± ArtÄ±rmak Elasticsearch JVM heap, verileri hÄ±zlÄ± bir ÅŸekilde iÅŸlemek ve veriler Ã¼zerindeki iÅŸlemleri yapabilmek iÃ§in elasticsearche Ã¶zel olarak ayrÄ±lmÄ±ÅŸ bir alan. Bu alan normalde (default olarak) 1 GB alana sahiptir, eÄŸer sunucu Ã¼zerinde yeterli miktarda RAM mevcut ise bu alanÄ± artÄ±rmak indeksleme ve iÅŸlem yapma hÄ±zÄ±nÄ± artÄ±racaktÄ±r.\nBurada Ã¶nemli olan JVM Heap alanÄ± 64 Bit yapÄ±ya sahip bir sunucu iÃ§in maksimum 32 GB a kadar artÄ±rÄ±lmalÄ±dÄ±r, sunucu Ã¼zerinde Ã§ok daha fazla RAM olsa dahi 32 GB limiti geÃ§memek gerekmektedir. Bununla ilgili detaylÄ± aÃ§Ä±klamaya buradan eriÅŸebilirsiniz: https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html\nJVM Heap alanÄ± ayarlanÄ±rken genellikle fiziksel ram boyutunun yarÄ±sÄ± kadar heap alanÄ± vermek tercih edilir, 32 GB boyutunu geÃ§meyecek ÅŸekilde.\nJVM Heap ayarlarÄ± ÅŸu ÅŸekilde yapÄ±labilir, Debian tabanlÄ± bir iÅŸletim sisteminde elasticsearch Ã¼n bulunduÄŸu dizin altÄ±nda jvm.options adÄ±nda bir dosya bulunmaktadÄ±r.\nEÄŸer heap alanÄ±nÄ± 16 GB ayarlamak isterseniz(fiziksel RAM in en az 32 GB olduÄŸundan emin olunuz ), jvm.options dosyasÄ± iÃ§erisine ÅŸu ÅŸekilde kaydedebilirsiniz.\n/etc/elasticsearch/jvm.options ## bu jvm.options dosyasÄ± iÃ§erisine aÅŸaÄŸÄ±da verilen parametler girilir. -Xms16GB -Xmx16GB Bu parametreler, jvm.options dosyasÄ± iÃ§erisine kaydedildikten sonraki adÄ±mda ise elasticsearch servisini yeniden baÅŸlatmayÄ± unutmayÄ±nÄ±z.\nsudo service elasticsearch restart SSD veya RAID 0 disk kullanÄ±mÄ± HDD disklere gÃ¶re Ã§ok hÄ±zlÄ± olan SSD diskler, elasticsearch Ã¼n veriyi daha hÄ±zlÄ± iÅŸlemesine, verimliliÄŸi artÄ±rmasÄ±na direkt olarak etki edecektir. RAID diskleri kullanÄ±rken RAID 0 haricindeki tiplerini kullanmak tercih edilmez.\nBu kÄ±sÄ±mda elasticsearch performansÄ±nÄ± artÄ±rmak iÃ§in yapÄ±lmasÄ± gerekli olabilecek bazÄ± adÄ±mlardan bahsedildi bunlar Ã¶zet olarak.\n Paralel bulk indekslemek JVM heap alanÄ± artÄ±rmak Ä°ndeks kopyalarÄ± devre dÄ±ÅŸÄ± bÄ±rakmak Ä°ndeks yenileme aralÄ±ÄŸÄ±nÄ± devre dÄ±ÅŸÄ± bÄ±rakmak Sunucu Swap alanÄ±nÄ± kaldÄ±rmak SSD veya RAID 0 Disk Kullanmak  Bu, elasticsearch performans iyileÅŸtirme adÄ±mlarÄ±nÄ± gÃ¶steren birinci kÄ±sÄ±m, ikinci kÄ±sÄ±mda, elasticsearch Ã¼zerinde indeks oluÅŸtururken, mapping sisteminin verimize gÃ¶re nasÄ±l yapÄ±landÄ±rÄ±lmasÄ± gerektiÄŸinden, indeks Ã¼zerinde otomatik olarak oluÅŸturulan bazÄ± alanlarÄ±n kaldÄ±rÄ±lmasÄ±ndan, optimum shard sayÄ±sÄ±nÄ±n belirlenmesinden, indeks performans (benchmarking) Ã¶lÃ§Ã¼mlerinden ve Grafana Ã¼zerinden elasticsearch deÄŸerlerinin (CPU,I/O, RAM, DISK kullanÄ±mÄ±nÄ±n) izlenmesi anlatÄ±lacaktÄ±r.\nBu Ã§alÄ±ÅŸma esnasÄ±nda yararlanÄ±lan kaynaklar https://blog.codecentric.de/en/2014/05/elasticsearch-indexing-performance-cheatsheet\u0026gt;https://www.elastic.co/guide/en/elasticsearch/reference/master/tune-for-indexing-speed.htmlhttps://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.htmhttps://elasticsearch-py.readthedocs.io/en/master","permalink":"/posts/elasticsearch-performans-art%C4%B1r%C4%B1m%C4%B1/","summary":"GiriÅŸ Elasticsearch Ã¼zerinde bÃ¼yÃ¼k boyuttaki verileri hÄ±zlÄ± bir ÅŸekilde iÅŸlemek Ã§aba gerektiren iÅŸlerden biridir. Bu yazÄ±da bir Ã§alÄ±ÅŸma esnasÄ±nda yapÄ±lan elasticsearch performans iyileÅŸtirmelerini ve nasÄ±l yapÄ±ldÄ±ÄŸÄ±nÄ± anlatmaya Ã§alÄ±ÅŸacaÄŸÄ±m.\nBu iyileÅŸtirme iÅŸlemlerinin nasÄ±l yapÄ±ldÄ±ÄŸÄ±na geÃ§meden once elasticsearch mimarisinde bulunan bazÄ± bileÅŸenlerden bahsetmekte yarar var.\n Cluster : Elasticsearch bir veya birden fazla bilgisayarda entegre ÅŸekilde Ã§alÄ±ÅŸabilir ve bu elasticsearch Ã¼n Ã§alÄ±ÅŸtÄ±ÄŸÄ± makinelere NODE denir. Cluster (KÃ¼me) ise bu nodeâ€™larÄ±n oluÅŸturduÄŸu gruba verilen yapÄ±ya denir.","title":"elasticsearch/performans "},{"content":"Ã–zet Terminal Ã¼zerinden kullanÄ±lan en meÅŸhur yazÄ± dÃ¼zenleme programÄ± VIM hakkÄ±nda komutlar ve bazÄ± kullanÄ±ÅŸlÄ± linux komutlarÄ±\nScript nasÄ±l yazÄ±lÄ±r.   Dosya oluÅŸturulur ve dosyanÄ±n baÅŸÄ±na terminalin yolu eklenir #!/bin/bash\n  terminal komutlarÄ±nÄ± bu yolun altÄ±na yazÄ±nÄ±z.\n  daha sonra dosyanÄ±n iznini ayarlayÄ±nÄ±z. chmod +x dosya_ismi\n  terminal scriptini ÅŸu ÅŸekilde Ã§alÄ±ÅŸtÄ±rÄ±labilirsiniz. ./dosya_ismi\n  Ã¶rnek bash scripti\n  !/bin/bash echo \u0026#34;AdÄ±nzÄ± giriniz: \u0026#34; read isim echo \u0026#34;Sifrenizi giriniz\u0026#34; read sifre if [[ ( $isim == \u0026#34;admin\u0026#34; \u0026amp;\u0026amp; $sifre == \u0026#34;random\u0026#34; ) ]]; then echo \u0026#34;BaÅŸarÄ±lÄ±\u0026#34; else echo \u0026#34;BaÅŸarÄ±sÄ±z\u0026#34; fi VIM Birden fazla dosya Ã¼zerinde Ã§alÄ±ÅŸmak $ vim *.txt # birden fazla txt ile dosyasÄ±nÄ± aynÄ± anda aÃ§manÄ±zÄ± saÄŸlar (eÄŸer birden fazla dosya mevcut ise) $ :wall veya :qall # bÃ¼tÃ¼n aÃ§Ä±k dosyalardan yaz veya Ã§Ä±k komutudur. $ vim -o *.txt # birden fazle txt dosyasÄ±nÄ± aÃ§ar ve yatay dÃ¼zlemde gÃ¶sterir, dikey dÃ¼zlem iÃ§in -O parametresi kullanÄ±lara $ :args *.txt # txt ile biten bÃ¼tÃ¼n dosyalarÄ± argument listesine aktarÄ±r. $ :all # bÃ¼tÃ¼n dosyalarÄ± yatay dÃ¼zlemde ayÄ±rÄ±r $ CTRL-w # birden fazla pencere arasÄ±nda gezmenizi saÄŸlar $ :split # aynÄ± dosyayÄ± iki farklÄ± pencerede gÃ¶sterir. $ :split \u0026lt;acÄ±lacak_dosya\u0026gt; # dosyayÄ± yeni bir pencerede aÃ§ar $ :vsplit # brden fazla pencereyi dikey komunda ayÄ±rÄ±r, tablolar iÃ§in Ã§ok kullanÄ±ÅŸlÄ±dÄ±r. \u0026#34;:set scrollbind \u0026#34; komutu ile aÃ§Ä±k dosyalar da aynÄ± anda yukarÄ± aÅŸaÄŸÄ± yapabilirsiniz.  $ :close # bulunduÄŸunuz pencereyi kapatÄ±r $ :only # bulunduÄŸunuz pencere hariÃ§ diÄŸerlerinin tamamÄ±nÄ± kapatÄ±r.  Hece KontrolÃ¼ \u0026amp; SÃ¶zlÃ¼k $ aspell -c \u0026lt;dosya\u0026gt; # verilen dosyada heceleri kontrol eder, terminal komutudur $ aspell -l \u0026lt;dosya\u0026gt; # terminal komutu $ :! dict \u0026lt;cumle\u0026gt; # cÃ¼mlenin anlamÄ±nÄ± kontrol etmenizi saÄŸlar $ :! wn \u0026#39;cumle\u0026#39; -over # cÃ¼mlenin eÅŸ anlamlÄ±larÄ±nÄ± gÃ¶sterir DosyayÄ± yazdÄ±rma $ :ha # bÃ¼tÃ¼n dosyayÄ± yazdÄ±rÄ±r $ :#,#ha # (#,#) ile belirtilen alandaki metini yazdÄ±rÄ±r BirleÅŸtime / Ekleme Komutu $ :r \u0026lt;dosya_ismi\u0026gt; # aÃ§Ä±k olan dosya iÃ§erisine, # aynÄ± dizinde olan baÅŸka bir dosyayÄ± eklemek iÃ§in bu komut kullanÄ±labilir, # imlecin hizasÄ±ndan sonra ekleme yapar Geri Alma / Yeniden Alma $ u # en son yaptÄ±ÄŸÄ±nÄ±z deÄŸiÅŸikliÄŸi geri alÄ±r $ U # yaptÄ±ÄŸÄ±nÄ±z bÃ¼tÃ¼n deÄŸiÅŸiklikleri geri alÄ±r $ CTRL-R # geri alÄ±nmÄ±ÅŸ bir kÄ±smÄ± yeniden getirmenizi saÄŸlar. Kopyalama \u0026amp; YapÄ±ÅŸtÄ±rma $ yy # imlecin bulunduÄŸu satÄ±rÄ± kopyalar, 2 satÄ±r kopyalamak iÃ§in 2yy kullanÄ±labilir. $ p # kesilen/kopyalanan iÃ§eriÄŸi imleÃ§ten baÅŸlayacak ÅŸekilde yapÄ±ÅŸtÄ±rÄ±r  Silme/Kesme (NORMAL modda uygulanÄ±r. Yani Vim komut satÄ±rÄ±nda deÄŸil. EXE modunda deÄŸil. ) $ x # imlecin Ã¼zerinde bulunduÄŸu karakteri siler. $ dw # imlecin bulunduÄŸu kelimeyi sonuna kadar siler (BoÅŸluklar dahil ) $ de # imlecin bulunduÄŸu kelimeyi sonuna kadar siler (BoÅŸluklar hariÃ§ ) $ cw # kelimenin geriye kalan kÄ±smÄ±nÄ± siler ve sizi ekleme moduna alÄ±r, ekleme modundan ESC ile Ã§Ä±kabilirsiniz. $ c$ # bulunduÄŸu satÄ±rÄ± tamamen siler ve sizi ekleme moduna alÄ±r ekleme modundan ESC ile Ã§Ä±kabilirsiniz.  $ d$ # imlecten itibaren satÄ±rÄ± siler e $ dd # satÄ±rÄ± tamamen siler, imlecin nerede olduÄŸunun Ã¶nemi yoktur $ 2dd # ileriki 2 satÄ±rÄ± siler, benzer sekilde 3dd : uc satÄ±r siler, 4dd: dort satÄ±r siler, (imlecten bagÄ±msÄ±z) Koyma $ p # kesilen/kopyalanan iÃ§eriÄŸi imleÃ§ten baÅŸlayacak ÅŸekilde yapÄ±ÅŸtÄ±rÄ±r  Dosya iÃ§erisinde arama (Vim) (bu kÄ±sÄ±mda genelde dÃ¼zenli ifadeler kullanÄ±lÄ±r ) $ /aramak_istediÄŸiniz_dÃ¼zen # yazdÄ±ÄŸÄ±nÄ±z ifadeyi aÃ§Ä±k olan belge iÃ§erisinde arar ve hepsini iÅŸaretler $ ?aramak_istediÄŸiniz_dÃ¼zen # yazdÄ±ÄŸÄ±nÄ±z ifadeyi aÃ§Ä±k olan belge iÃ§erisinde arar ama iÅŸaretlemez, n ile ileriki kelimeyi gÃ¶rebilirsiniz.  $ :set ic # kelimelerin bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf ayrÄ±mÄ±nÄ± ortadan kaldÄ±rÄ±r $ :set hls # aranan ve bulunan kelimeleri vurgulu ÅŸekilde gÃ¶sterir. DÃ¼zenli ifadeler ile metin yÃ¶netimi $ :s/harf1/harf2/ # harf1, harf2 ile deÄŸiÅŸtirilir fakat sadece ilk karÅŸÄ±laÅŸmada yapÄ±lÄ±r $ :s/harf1/harf2/g # bÃ¼tÃ¼n dosya iÃ§erisindeki harf1, harf2 ile deÄŸiÅŸtirilir. $ :s/harf1/harf2/gc # yukarÄ±daki iÅŸlemin aynÄ±sÄ±nÄ± onay alarak yapmak iÃ§in \u0026#34;c\u0026#34; eklenir $ :#,#s/harf1/harf2/g # (#,#) arasÄ±ndaki satÄ±rlarda bulunan harf1, harf2 ile deÄŸiÅŸtirilir. $ :%s/harf1/harf2/g # tÃ¼m dosyadaki harf1 ifadesi harf2 ile deÄŸiÅŸtirilir. $ :%s/\\(harf1\\)\\(.*\\)/\\1/g # harf1 sonrakisindeki bÃ¼tÃ¼n satÄ±rlarÄ± siler. $ :%s/\\(SL\\dm\\d\\d\\d\\d\\d\\.\\d\\)\\(.*\\)/\\1\\t\\2/g # SL1m12345.1 ve tanÄ±mÄ± arasÄ±na TAB boÅŸluÄŸu ekler  $ :%s/\\n/ifade/g #SatÄ±r verilen ifade ile deÄŸiÅŸtirilir. $ :%s/\\(^SL\\dm\\d\\d\\d\\d\\d.\\d\\t.\\{-}\\t.\\{-}\\t.\\{-}\\t.\\{-}\\t\\).\\{-}\\t/\\1/g # 5 ve 6.ncÄ± TAB taki (5. Kolondaki), iÃ§eriÄŸi \u0026#34;{-}\u0026#34; ile degiÅŸtirir.  $ :#,#s/\\( \\{-} \\|\\.\\|\\n\\)/\\1/g # (#,#) verilen aralÄ±kta ne kadar cÃ¼mle olduÄŸunu hesaplar $ :%s/\\(E\\{6,\\}\\)/\u0026lt;font color=\u0026#34;green\u0026#34;\u0026gt;\\1\u0026lt;\\/font\u0026gt;/g # 6 dan fazla E geÃ§en kÄ±sÄ±mlarÄ±, HTML renkleri ile vurgular. $ :%s/\\([A-Z]\\)/\\l\\1/g # BÃ¼yÃ¼k harfleri, kÃ¼Ã§Ã¼k harfler ile deÄŸiÅŸtirir, \u0026#39;%s/\\([A-Z]\\)/\\u\\1/g\u0026#39; , bu ise kÃ¼Ã§Ã¼k harfleri bÃ¼yÃ¼k harfler ile deÄŸiÅŸtirir. $ :g/ifade/ s/\\([A-Z]\\)/\\l\\1/g | copy $ # ifade yeni oluÅŸturulan ifade ile deÄŸiÅŸtirilir eÅŸdeÄŸer olanlar copy $ ile yazdÄ±rÄ±lÄ±r.  HTML DÃ¼zenleme -metini HTML formatÄ±na cevirme $ :runtime! syntax/2html.vim # vim iÃ§erisinde bu komutu Ã§alÄ±ÅŸtÄ±rÄ±nÄ±z. Vim iÃ§erisinden terminal komutu Ã§alÄ±ÅŸtÄ±rma $ :!\u0026lt;terminal_komutu\u0026gt; \u0026lt;ENTER\u0026gt; # terminal komutunu vim iÃ§erisinden Ã§alÄ±ÅŸtÄ±rÄ±r $ :sh terminal ile vim arasÄ±nda gezmenizi saÄŸlar Tablo dÃ¼zenleyicisi olarak Vim' i kullanmak $ v # karakterleri seÃ§mek iÃ§in gÃ¶rsel mod baÅŸlatÄ±lÄ±r. $ V # satÄ±rlarÄ± seÃ§mek iÃ§in gÃ¶rsel mod baÅŸlatÄ±lÄ±r. $ CTRL-V # blok gÃ¶rsel seÃ§im yapmanÄ±zÄ± saÄŸlar. $ :set scrollbind # aynÄ± ayna ayrÄ±lan iki ayrÄ± dosyada gezinti yapmanÄ±zÄ± saÄŸlar.  Vim ayarlarÄ±nÄ± deÄŸiÅŸtirmek - .vimrc dosyasÄ± iÃ§erisindeki parametreler isteÄŸinize gÃ¶re deÄŸiÅŸtirilebilir.\nKullanÄ±ÅŸlÄ± terminal komutlarÄ± $ cat \u0026lt;dosya1\u0026gt; \u0026lt;dosya2\u0026gt; \u0026gt; \u0026lt;sonuc\u0026gt; # dosya1 ve dosya2 yi sonuc dosyasina kopyalar ve sonuc dosyasini olusturur. $ paste \u0026lt;dosya1\u0026gt; \u0026lt;dosya2\u0026gt; \u0026gt; \u0026lt;p_sonuc\u0026gt; # iki farklÄ± kaynaktan gelen girdiyi, aralarÄ±nda TAB boÅŸluÄŸu olacak ÅŸekilde aynÄ± dosya (p_sonuc) iÃ§erisine yapÄ±ÅŸtÄ±rÄ±r. $ cmp \u0026lt;dosya1\u0026gt; \u0026lt;dosya2\u0026gt; # iki dosyanÄ±n aynÄ± olup olmadÄ±gÄ±nÄ± size bildirir. $ diff \u0026lt;dosya1\u0026gt; \u0026lt;dosya2\u0026gt; # iki dosya arasÄ±ndaki farklÄ±lÄ±klarÄ± gÃ¶sterir $ head -\u0026lt;numara\u0026gt; \u0026lt;dosya\u0026gt; # verdiÄŸiniz numara kadar ilk X satÄ±rÄ± yazdÄ±rÄ±r. $ tail -\u0026lt;numara\u0026gt; \u0026lt;dosya\u0026gt; # verdiÄŸiniz numara kadar son X satÄ±rÄ± yazdÄ±rÄ±r.  $ split -l \u0026lt;numara\u0026gt; \u0026lt;dosya\u0026gt; # dosyanÄ±n satÄ±rÄ±larÄ±nÄ± ayÄ±rÄ±r. $ csplit -f out dosya_ismi \u0026#34;%^\u0026gt;%\u0026#34; \u0026#34;/^\u0026gt;/\u0026#34; \u0026#34;{*}\u0026#34; # dosya_ismini \u0026gt; den itibaren birÃ§ok farklÄ± kÃ¼Ã§Ã¼k dosyalar oluÅŸturur. $ sort \u0026lt;dosya_ismi\u0026gt; # dosya iÃ§erisindekileri sÄ±ralar -b argument kullanÄ±lÄ±rsa boÅŸluklarÄ± yok sayar. $ sort -k 2,2 -k 3,3n girdi_dosyasÄ± \u0026gt; cÄ±ktÄ± # -k argument i kolon iÃ§in, -n sayÄ±sal olarak sÄ±ralar ve tablo ÅŸeklinde kaydeder.  $ sort girdi_dosyasÄ± | uniq \u0026gt; cÄ±ktÄ± # uniq komutu aynÄ± olan verileri dahil etmez. $ join -1 1 -2 1 \u0026lt;tablo1\u0026gt; \u0026lt;tablo2\u0026gt; # tablo1 ve tablo2 yi birleÅŸtirir, -1 dosya1, 1:kolon1; -2dosya2, col2.  $ sort tablo1 \u0026gt; tablo1a; sort tablo2 \u0026gt; tablo2a; join -a 1 -t \u0026#34;`echo -e \u0026#39;\\t\u0026#39;`\u0026#34; tablo1a tablo2a \u0026gt; tablo3 # \u0026#39;-a \u0026lt;tablo\u0026gt;\u0026#39; : verilen tablonun bÃ¼tÃ¼n kayÄ±tlarÄ±nÄ± yazdÄ±rÄ±r. Normalde yazdÄ±rma iÅŸlemi iki tabloda ortak olan kÄ±sÄ±mlarÄ± yazdÄ±rÄ±r. \u0026#39;-t \u0026#34;`echo -e \u0026#39;\\t\u0026#39;`\u0026#34; -\u0026gt;\u0026#39;  : TAB boÅŸluÄŸu kullanarak tablolarÄ± Ã§Ä±ktÄ± dosyasÄ±na yazdÄ±rÄ±r. $ cat tablom | cut -d , -f1-3 # cut komutu : tablonun belirlenen kÄ±sÄ±mlarÄ± alÄ±r, -d alanlarÄ±n nasÄ±l ayrÄ±lacaÄŸÄ±nÄ± belirtilsiniz. -d : burada , olarak belirlenmiÅŸtir, normalde TAB boÅŸluk, -f tablonun kolonlarÄ±nÄ± belirtir, kolon 1 den 3 e.  KullanÄ±ÅŸlÄ± tek satÄ±r komutlar $ for i in *.input; do mv $i ${i/isim\\.eski/isim\\.yeni}; done # isim.eski adÄ±ndaki dosyanÄ±n ismini, isim.yeni olarak deÄŸiÅŸtirir. Komutu test etmek iÃ§in, do mv komutu Ã¶nÃ¼ne \u0026#34;echo\u0026#34; konulabilir.  $ for i in *.girdi; do ./uygulama $i; done # bir Ã§ok dosya iÃ§in verilen uygulamayÄ± Ã§alÄ±ÅŸtÄ±rÄ±r.  $ for i in *.girdi; do komut -d /veri/../veri_tabanÄ± -i $i \u0026gt; $i.out; done # komut for dÃ¶ngÃ¼sÃ¼ iÃ§erisinde *.girdi Ã¼zerinde Ã§alÄ±ÅŸÄ±r ve *.out dosyasÄ± oluÅŸturur.  $ for i girdi *.pep; do hedef -db /usr/../veri_tabanÄ± -seed $i -out $i; done # hedef in Ã¼zerinde for dÃ¶ngÃ¼sÃ¼ Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r ve Ã§Ä±ktÄ± dosyasÄ± yazdÄ±rÄ±lÄ±r. $ for j girdi 0 1 2 3 4 5 6 7 8 9; do grep -iH \u0026lt;ifade\u0026gt; *$j.seq; done #  verilen ifadeyi girdi \u0026gt; 10.000 dosyaya kadar arar ve ne kadar o ifade geÃ§tiÄŸini yazdÄ±rÄ±r. $ for i in *.pep; do echo -e \u0026#34;$i\\n\\n17\\n33\\n\\n\\n\u0026#34; | ./program $i \u0026gt; $i.out; done # etkileÅŸimli programÄ± Ã§alÄ±ÅŸtÄ±rÄ±r ve girdi/Ã§Ä±ktÄ± sorar.  Basit Perl KomutlarÄ± $ perl -p -i -w -e \u0026#39;s/ifade1/ifade2/g\u0026#39; girdi_dosyasÄ± # girdi dosyasÄ± iÃ§erisindekileri verilen ifadelere gÃ¶re deÄŸiÅŸimini yapar. \u0026#39;-p\u0026#39; bu komut yedek bir dosya oluÅŸturur  $ perl -ne \u0026#39;print if (/ifade1/ ? ($c=1) : (--$c \u0026gt; 0)) ; print if (/ifade2/ ? ($d = 1) : (--$d \u0026gt; 0))\u0026#39; girdi_dosyasÄ± \u0026gt; cÄ±ktÄ±_dosyasÄ± # ifade1 ve ifade2 iÃ§eren satÄ±rlarÄ± ayrÄ±ÅŸtÄ±rÄ±r (parse eder.)  WGET (terminal Ã¼zerinden linki verilen dosya indirimini gerÃ§ekleÅŸtirir.) $ wget ftp://ftp.itu.edu.tr.... # verilen linkteki dosya wget komutunun Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ± dizine iner.  SCP (Ä°ki makine arasÄ±nda gÃ¼venli kopyalama iÅŸlemi saÄŸlar. ) Genel KullanÄ±m. $ scp kopyalanacak_dosya kopyalanacak_yer # Ã–rnekler Sunucudan dosya kopyalamak iÃ§in (bilgisayarÄ±nÄ±zÄ±n terminalinden) $ scp kullanÄ±cÄ±@sunucu_ip:dosya_adÄ± . # \u0026#39;.\u0026#39; en sona nokta koyulmasÄ±, sunucu Ã¼zerindeki kopyalanacak dosyayÄ± bulunduÄŸunuz yere kopyalamasÄ±nÄ± saÄŸlar.  BilgisayarÄ±nÄ±zdan sunucuya kopyalama yapmak iÃ§in. (Bilgisayar terminalinden) $ scp resim.jpg kullanÄ±cÄ±@sunucu_ip:~/belgeler/resimler/ Sunucu Ã¼zerinde bulunan klasÃ¶rÃ¼ bilgisayarÄ±mÄ±za kopyalamak iÃ§in. (Bilgisayar terminalinden) $ scp -r kullanÄ±cÄ±@sunucu_ip:dizin/ ~/Masaustu Bilgisayar Ã¼zerinde bulunan klasÃ¶rÃ¼ sunucuya kopyalamak iÃ§in. (Bilgisayar terminalinden) $ scp -r klasÃ¶r/ kullanÄ±cÄ±@sunucu_ip:dizin/ NFTP : (Dosya transfer iÅŸlemlerinizi kolay ÅŸekilde terminal Ã¼zerinden yapmanÄ±zÄ± saÄŸlar ) $ open ncftp $ ncftp\u0026gt; open sunucu_url # sunucuya baglantÄ± saÄŸlanÄ±yor.. $ ncftp\u0026gt; cd /root/Masaustu. # masaustune gecildi $ ncftp\u0026gt; get resimler.gz # masaustunde bulunan resimler.gz indirildi. $ ncftp\u0026gt; bye # gule gule mesajÄ± alÄ±ndÄ± ","permalink":"/posts/vim/","summary":"Ã–zet Terminal Ã¼zerinden kullanÄ±lan en meÅŸhur yazÄ± dÃ¼zenleme programÄ± VIM hakkÄ±nda komutlar ve bazÄ± kullanÄ±ÅŸlÄ± linux komutlarÄ±\nScript nasÄ±l yazÄ±lÄ±r.   Dosya oluÅŸturulur ve dosyanÄ±n baÅŸÄ±na terminalin yolu eklenir #!/bin/bash\n  terminal komutlarÄ±nÄ± bu yolun altÄ±na yazÄ±nÄ±z.\n  daha sonra dosyanÄ±n iznini ayarlayÄ±nÄ±z. chmod +x dosya_ismi\n  terminal scriptini ÅŸu ÅŸekilde Ã§alÄ±ÅŸtÄ±rÄ±labilirsiniz. ./dosya_ismi\n  Ã¶rnek bash scripti\n  !/bin/bash echo \u0026#34;AdÄ±nzÄ± giriniz: \u0026#34; read isim echo \u0026#34;Sifrenizi giriniz\u0026#34; read sifre if [[ ( $isim == \u0026#34;admin\u0026#34; \u0026amp;\u0026amp; $sifre == \u0026#34;random\u0026#34; ) ]]; then echo \u0026#34;BaÅŸarÄ±lÄ±\u0026#34; else echo \u0026#34;BaÅŸarÄ±sÄ±z\u0026#34; fi VIM Birden fazla dosya Ã¼zerinde Ã§alÄ±ÅŸmak $ vim *.","title":"vim "},{"content":"Ã–zet: Bu yazÄ±da linux ortamÄ±na biraz daha giriÅŸ yaparak, linux ortamÄ±nda bulunan komutlar hakkÄ±nda kÄ±sa bilgilendirme yapÄ±lmasÄ± planlanmaktadÄ±r.\nGiriÅŸ Neden Linux ?  Birden fazla iÅŸlemi aynÄ± anda kolay ÅŸekilde yapmanÄ±zÄ± saÄŸlar Uzaktan iÅŸlemlerinizi halletmede bÃ¼yÃ¼k kolaylÄ±k saÄŸlar Birden fazla kullanÄ±cÄ± aynÄ± sunucuya eriÅŸebilir Terminale, bir sistem Ã¼zerinde olan kaynaklara birden fazla eriÅŸim mÃ¼mkÃ¼ndÃ¼r ArayÃ¼z olan sistemlere gÃ¶re daha performanslÄ±, Bedava , GÃ¼ncel  Temeller Bu bilgilendirme dosyasÄ± iÃ§in not   BÃ¼tÃ¼n komutlar bÃ¼yÃ¼k ve kÃ¼Ã§Ã¼k harfe duyarlÄ±dÄ±r.\n  \u0026quot;$\u0026quot;komutun baÅŸlangÄ±cÄ±nÄ± temsil eder.\n  \u0026quot;#\u0026quot; komutun sonunu temsil eder.\n  Windows bilgisayar Ã¼zerinden giriÅŸ iÃ§in  PuTTY : windows bilgisayar Ã¼zerinden SSH ile baglantÄ± saÄŸlamak iÃ§in gereklidir. PuTTY programÄ±nda SSH ile baÄŸlantÄ± iÃ§in sunucu IP adresi ve PORT numarasÄ±nÄ± bilmelisiniz.  Mac veya Linux Bilgisayarlardan EriÅŸim Bu tÃ¼r bilgisayarlar UNIX tabanlÄ± olduÄŸundan dolayÄ± terminal Ã¼zerinden aÅŸaÄŸÄ±da verilen komutlarÄ± yazmanÄ±z yeterli olacaktÄ±r ekstradan herhangi bir programa gerek duyulmamaktadÄ±r.\n$ ssh \u0026lt;kullanÄ±cÄ±_adÄ±\u0026gt;@\u0026lt;sunucu_adresi(IP)\u0026gt; $ kullanÄ±cÄ±_adÄ±: ... $ ÅŸifre: ... {% endhighlight %} #### GiriÅŸ yaptÄ±ktan sonra ÅŸifrenizi deÄŸiÅŸtirmek isterseniz ```bash $ passwd # bu komutu kullanabilirsiniz.  # bu komut sayesinde giriÅŸ yapÄ±lan  # kullanÄ±cÄ± iÃ§in yeni ÅŸifre  # belirleyebilirsiniz. {% endhighlight %} #### Listeleme ```bash $ pwd # ÅŸu anda bulunduÄŸunuz konumu Ã§Ä±ktÄ± olarak yazdÄ±rÄ±r $ ls # bulunduÄŸunuz konumdaki dosyalarÄ± ve klasÃ¶rleri listeler $ ll # ls komutunun eÅŸdeÅŸi olarak tanÄ±mlÄ± bir ifadedir genelde \u0026#34;ls -alF\u0026#34; olarak kayÄ±tlÄ±dÄ±r bash profilinde $ ll -R # dosyalar/klasÃ¶rler listelenir, klasÃ¶rler iÃ§erisindeki dosyalarda listelenir $ ll -t # listeleme iÅŸlemi kronolojik ÅŸekilde gerÃ§ekleÅŸir. $ stat \u0026lt;dosya_adÄ±\u0026gt; # dosyaya ait bilgileri meta bilgileri listeler $ whoami # sizin sistem tarafÄ±ndan kim olduÄŸunuzu sÃ¶yler, yani kullanÄ±cÄ± adÄ±nÄ±zÄ± listeler $ hostname # baÄŸlÄ± olduÄŸunu makinenin URL ni yada IP sini gÃ¶sterir Dosyalar ve KlasÃ¶rler $ mkdir \u0026lt;klasÃ¶r_adÄ±\u0026gt; # belirlenen isimde klasÃ¶r oluÅŸturur $ cd \u0026lt;klasÃ¶r_adÄ±\u0026gt; # klasÃ¶r adÄ± tanÄ±mlanan klasÃ¶re gidersini. $ cd .. # Ã¼st klasÃ¶re gitmenizi saÄŸlar $ cd ../../ # iki Ã¼st klasÃ¶re gitmenizi saÄŸlar $ cd # ana klasÃ¶rÃ¼ne gidersiniz $ rmdir \u0026lt;klasÃ¶r_adÄ±\u0026gt; # klasÃ¶rÃ¼ siler $ rm \u0026lt;dosya_adÄ±\u0026gt; # dosyayÄ± siler $ rm -r \u0026lt;klasÃ¶r_adÄ±\u0026gt; # klasÃ¶rÃ¼ ve iÃ§erisindeki bÃ¼tÃ¼n dosyalarÄ± siler $ mv \u0026lt;dosyaadÄ±1\u0026gt; \u0026lt;dosyaadÄ±2\u0026gt; # isim deÄŸiÅŸtirmenizi saÄŸlar, name $ mv \u0026lt;dosyaadÄ±\u0026gt; \u0026lt;taÅŸÄ±nacak_yol\u0026gt; # dosyayÄ± belirtilen yere taÅŸÄ±r $ cp \u0026lt;dosyaadÄ±\u0026gt; \u0026lt;kopyalanacak_yol\u0026gt; # dosyayÄ± belirtilen yere kopyalar, eÄŸer klasÃ¶r kopyalanacak ise -r parametresi eklenir.  KÄ±sayollar $ . # sadece nokta bulunduÄŸunuz dizini ifade eder $ ~/ # kullanÄ±cÄ±nÄ±n ana dizinini ifade eder $ history # yazmÄ±ÅŸ olduÄŸunuz komutlarÄ±n kaydÄ±nÄ± tutar ve bu komut ile eriÅŸebilirsiniz $ !\u0026lt;komut_sÄ±ralamasÄ±\u0026gt; # daha Ã¶nce yazmÄ±ÅŸ oldunuz komutu sÄ±ralamasÄ±nÄ±n numarasÄ±nÄ± vererek calÄ±ÅŸtÄ±rabilirsiniz. $ yukarÄ±(asagÄ±)_oklarÄ± # geÃ§miÅŸ komutlar arasÄ±nda gezmenizi saÄŸlar $ \u0026lt;tamamlanmamÄ±ÅŸ_yol_veya_dosyaadÄ±\u0026gt; TAB # Tab a bastÄ±ÄŸÄ±nÄ±zda sistem otomatik tamamlama iÅŸlemini gerÃ§ekleÅŸtirir. $ \u0026lt;tamamlanmamÄ±ÅŸ komut\u0026gt; SHIFT\u0026amp;TAB # komutu otomatik tamamlar $ Ctrl a # imlecin en baÅŸa gitmesini saÄŸlar $ Ctrl e # imlecin en sona gitmesini saÄŸlar $ Ctrl d # imlec altÄ±ndaki karakteri siler $ Ctrl k # imlecin bulunduÄŸu saÄŸlar $ Ctrl y # Ctrl k ile alÄ±nan iÃ§erik yapÄ±ÅŸtÄ±rÄ±lÄ±r. YardÄ±m Alma $ man # genel yardÄ±m $ man wc # wc komutu hakkÄ±nda yardÄ±m almanÄ±zÄ± saÄŸlar $ wc --help # wc komutu hakkÄ±nda yardÄ±m almanÄ±zÄ± saÄŸlar $ info wc # wc komutu hakkÄ±nda detaylÄ± bilgi almanÄ±z saÄŸlanÄ±r $ apropos wc # wc komutuna ait bÃ¼tÃ¼n yardÄ±m dosyalarÄ± gÃ¼ncellenir. AradÄ±ÄŸÄ±nÄ±z DosyayÄ± Bulma YÃ¶ntemleri Arama yapmak $ find -name \u0026#34;*aramakistediÄŸinizdesen*\u0026#34; # girdiÄŸiniz desene gÃ¶re bulunduÄŸunuz dizinde arama yapmanÄ±zÄ± saÄŸlar. $ find /usr/local -name \u0026#34;*klas*\u0026#34; # isminin iÃ§erisinde klas geÃ§en dosyalarÄ± ve klaksÃ¶rleri listeler. $ find /usr/local -iname \u0026#34;*klas*\u0026#34; # yukarÄ±daki komutuna benzer ÅŸekilde, isminin iÃ§erisinde klas geÃ§en dosyalarÄ± ve klasÃ¶rleri listeler fakat bu durumda bÃ¼yÃ¼k veya kÃ¼Ã§Ã¼k harfte olmasÄ± dikkate alÄ±nmaz $ find ~ -type f -mtime -2 # 2 gÃ¼n iÃ§erisinde deÄŸiÅŸtirilmiÅŸ bÃ¼tÃ¼n dosyalarÄ± listeler $ locate \u0026lt;aramakistediÄŸinizdesen\u0026gt; # aradÄ±ÄŸÄ±nÄ±z dosyayÄ± veya klasÃ¶rÃ¼ sistem genelinde arar. $ which \u0026lt;uygulama_adÄ±\u0026gt; # uygulamanÄ±n nerede bulunduÄŸunu gÃ¶sterir $ whereis \u0026lt;uygulama_adÄ±\u0026gt; # uygulamanÄ±n Ã§alÄ±ÅŸtÄ±rÄ±labilir dosyasÄ±nÄ±n yerini gÃ¶sterir $ dpkg -l | grep aramakistediÄŸinizpaketismi # Debian paketleri iÃ§erisinde arama yaparak verilen desende bulunan paketleri listel Dosya iÃ§erisinde arama yapmak $ grep aranan_kelime dosya # dosya iÃ§erisinde aranan kelimenin nerelerde geÃ§tiÄŸini size aktarÄ±r $ grep -H aranan_kelime # -H Ã§Ä±ktÄ± dosyasÄ±nÄ± aranan kelimenin Ã¶nÃ¼ne koyar $ grep \u0026#39;aranan_kelime\u0026#39; dosya | wc # burada iki komut birleÅŸtirilmiÅŸtir, yani grep komutunun Ã§Ä±ktÄ±sÄ± wc komutuna girdi olmaktadir yani aranan kelime verilen dosyada 5 kere geÃ§iyor ise bu durumda sonuc 5 olarak dÃ¶nmektedir.  $ find /home/kullanÄ±cÄ±_adÄ± -name \u0026#39;*.txt\u0026#39; | xargs grep -c ^.* # verilen dizinde txt dosyalarÄ±nÄ± bularak bu dosyalar iÃ§erisindeki satÄ±r sayÄ±sÄ±nÄ± hesaplayarak Ã§Ä±ktÄ± vermektedir.  Ä°zinler \u0026amp; Hak SahipliÄŸi $ ls -al # bu komut Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±gÄ±nda buna benzer bir Ã§Ä±ktÄ± gÃ¶rÃ¼nebilir : drwxrwxrwx Burada bahsi geÃ§en harflerin anlamlarÄ± aÅŸaÄŸÄ±daki gibi verilebilir.\n d: dizin rwx: oku, yaz, Ã§alÄ±ÅŸtÄ±r ilk Ã¼Ã§lÃ¼ (rwx) : kullanÄ±cÄ± izinleri(u) ikinci Ã¼Ã§lÃ¼: grup izinleri (g) Ã¼Ã§Ã¼ncÃ¼ Ã¼Ã§lÃ¼: diÄŸer izinleri (o) ifade etmektedir. En baÅŸta d olursa bu o dosyanÄ±n aslÄ±nda bir klasÃ¶r olduÄŸunu ifade etmektedir.  KullanÄ±cÄ± ve grupa, yazma ve Ã§alÄ±ÅŸtÄ±rma izni vermek:\n$ chmod ug+rx dosya_ismi KullanÄ±cÄ± haklarÄ±nÄ±n alÄ±nmasÄ± $ chmod ugo-rwx dosya_ismi \u0026#39;+\u0026#39; izin eklemeyi saÄŸlar \u0026#39;-\u0026#39; izin silmenizi saÄŸlar $ chmod +rx dosya_ismi/ VEYA $ chmod 755 dosya_ismi/ Hak sahipliÄŸinin deÄŸiÅŸtirilmesi $ chown \u0026lt;kullanÄ±cÄ±_adÄ±\u0026gt; \u0026lt;dosya veya klasÃ¶r\u0026gt; # kullanÄ±cÄ± hak sahibini deÄŸiÅŸtirir $ chgrp \u0026lt;grup\u0026gt; \u0026lt;dosya veya klasÃ¶r\u0026gt; # grup hak sahibini deÄŸiÅŸtirir $ chown \u0026lt;kullanÄ±cÄ±_adÄ±\u0026gt;:\u0026lt;grup\u0026gt; \u0026lt;dosya veya klasÃ¶r\u0026gt; # kullanÄ±cÄ± ve grup hak sahipliÄŸini deÄŸiÅŸtirir KullanÄ±ÅŸlÄ± Linux KomutlarÄ± $ df # sistem diskinin ne kadar dolu ve boÅŸ olduÄŸu bilgisini gÃ¶sterir $ free # ne kadar Ã¶nbellek (RAM) alanÄ±nÄ±n boÅŸ/dolu olduÄŸunu gÃ¶sterir $ uname -a # iÅŸletim sistemine ait temel bilgileri gÃ¶sterir $ bc # terminal Ã¼zerinden hesap makinesi kullanmanÄ±zÄ± saÄŸlar $ /sbin/ifconfig # sunucunun aÄŸ bilgilerini listeler. $ ln -s orjinal_dosyaismi yeni_dosyaismi # orjinal dosyaya link oluÅŸturur $ du -sh # bulunduÄŸunuz konumdaki disk kullanÄ±m bilgilerini listeler $ du -sh * # bulunduÄŸunuz konumdaki dosyalarÄ±n/klasÃ¶rlerin kullanÄ±m bilgilerini gÃ¶sterir $ du -s * | sort -nr # sÄ±ralanmÄ±ÅŸ ÅŸekilde dosyalarÄ±n/klasÃ¶rlerin kullanÄ±mlarÄ±nÄ± listeler Ä°ÅŸlem YÃ¶netimi $ who # sisteme kimin girdiÄŸini gÃ¶sterir $ w # sistemde kimlerin olduÄŸunu gÃ¶sterir $ ps # arka planda Ã§alÄ±ÅŸan iÅŸlemler hakkÄ±ndaki bilgileri listeler. $ ps -e # sistemdeki bÃ¼tÃ¼n iÅŸlemleri listeler $ ps aux | grep \u0026lt;kullanÄ±cÄ±_adÄ±\u0026gt; # kullanÄ±cÄ±ya ait Ã§alÄ±ÅŸtÄ±rÄ±lan iÅŸlemleri listeler $ top # CPU ve RAM deÄŸerlerinin kullanÄ±m bilgilerini gÃ¶sterir $ mtop # birden fazla CPU iÃ§in top komutunun yaptÄ±ÄŸÄ±nÄ± yapar $ Ctrl z \u0026lt;enter\u0026gt; bg or fg \u0026lt;enter\u0026gt; # Ã§alÄ±ÅŸan iÅŸlemleri durdurur, arka plana atar (bg) veya Ã¶n plana getirir (fg) $ Ctrl c # yeni baÅŸlamÄ±ÅŸ olan iÅŸlemi durdurur $ kill \u0026lt;iÅŸlem_no\u0026gt; # belirlenen iÅŸlemi sonlandÄ±rÄ±r, iÅŸlem ID sine gÃ¶re belirlenir $ renice -n \u0026lt;Ã¶nemlilik_deÄŸeri\u0026gt; # iÅŸlemin Ã¶nemlilik deÄŸerini deÄŸiÅŸtirmenizi saÄŸlar  Text dosyalarÄ±nÄ± okumak $ less \u0026lt;dosya_ismi\u0026gt; # belirtilen dosyayÄ± terminal Ã¼zerinden okumanÄ±zÄ± saÄŸlar G :dosyanÄ±n sonuna gider, g : dosyanÄ±n baÅŸÄ±na gider.  $ more \u0026lt;dosya_ismi\u0026gt; # dosya iÃ§eriÄŸini gÃ¶sterir Ã§Ä±kmak iÃ§in q ya basÄ±lmasÄ± gereklidir. $ cat \u0026lt;dosya_ismi\u0026gt; #dosya iÃ§eriÄŸini terminale yazdÄ±rÄ±r Metin DÃ¼zenleyicileri VI ve VIM Terminal tabanlÄ± gÃ¼Ã§lÃ¼ metin dÃ¼zenleyicidir. Vi genelde linux tabanlÄ± bÃ¼tÃ¼n sistemlerde mevcuttur, vim, vi nin geliÅŸmiÅŸidir.\nEMACS Grafik tabanlÄ± metin dÃ¼zenleyicidir. Bu dÃ¼zenleyiciye ait olan klavye dÃ¼zeni hakkÄ±nda bilginiz olmalÄ±dÄ±r. BÃ¼tÃ¼n linux ve unix tabanlÄ± sistemlerde mevcuttur.\nXEMACS EMACS in Ã§ok daha geliÅŸmiÅŸidir, yazÄ±m hatalarÄ±, web ve metini iyi bir ÅŸekilde kontrol etmek mÃ¼mkÃ¼ndÃ¼r fakat normalde yÃ¼klenmiÅŸ olmaz.\nPICO Terminal tabanlÄ± basit metin dÃ¼zenleyicidir, buna ait klavye dÃ¼zeni bilinmelidir.\nVIM Temelleri Temeller $ vim dosya_ismi # dosya_ismin de dosya oluÅŸturur veya yazma modunda aÃ§ar $ i # vim \u0026#39;in iÃ§erisine girdikten sonra i aÃ§Ä±k olan dosyaya bir ÅŸeyler yazmanÄ±za olanak saÄŸlar. $ ESC # dosya dÃ¼zenleme modundan Ã§Ä±kÄ±lÄ±r $ : # vim iÃ§erisinde kullanacaÄŸÄ±nÄ±z komutlar : ile baÅŸlar $ :w # vim iÃ§erisinde :w yaptÄ±ÄŸÄ±nÄ±zda yazdÄ±ÄŸÄ±nÄ±zÄ± kaydeder. $ :q # bu komut vim den Ã§Ä±kmanÄ±zÄ± saÄŸlar $ :q! # hiÃ§ birÅŸeyi kaydetmeden Ã§Ä±kmanÄ±zÄ± saÄŸlar. $ :wq # kaydederek Ã§Ä±kar $ R # vim iÃ§erisinde Ã¶zellik deÄŸiÅŸtirmenizi saÄŸlar $ r # imlecin bulunduÄŸu karakteri deÄŸiÅŸtirmenizi saÄŸlar $ q: # vim iÃ§erisinde yazdÄ±ÄŸÄ±nÄ±z komutlarÄ±n kaydÄ±nÄ± gÃ¶sterir $ :w yeni_dosyaadÄ± # yeni dosyaya kaydeder. $ :#,#w yeni_dosyaadÄ±# belirlenen (#,#) aralÄ±ktaki metini yeni dosyaya kaydeder. $ :# belirlenen (#) satÄ±ra gitmenizi saÄŸlar YardÄ±m $ vimtutor # vim iÃ§erisinde nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±na dair bilgileri iÃ§eren tur baÅŸlatÄ±lÄ±r. $ :help # vim iÃ§erisinde yardÄ±m aÃ§ar, Ã§Ä±kmak iÃ§in q komutu kullanÄ±lÄ±r. $ :help \u0026lt;konu\u0026gt; # belirlenen konu hakkÄ±nda yardÄ±m aÃ§ar $ :help \u0026lt;konu\u0026gt; CTRL-D # belirlenen konunun geÃ§tiÄŸi bÃ¼tÃ¼n yardÄ±m dÃ¶kÃ¼manÄ±nÄ± listeler. $ :\u0026lt;yukarÄ±-asagÄ± tuslarÄ±\u0026gt; # daha Ã¶nceki yaptÄ±ÄŸÄ±nÄ±z komutlar arasÄ±nda gezmenizi saÄŸlar. Dosya iÃ§erisinde gezme (vim iÃ§erisinde) $ $ # bulunduÄŸunuz satÄ±rÄ±n en sonuna gider $ A # bulunduÄŸunuz satÄ±rÄ±n en sonuna yazma modunu aÃ§arak gider $ 0 (sÄ±fÄ±r) # satÄ±rÄ±n baÅŸlangÄ±cÄ±na gider $ CTRL-g # imlecin nerede olduÄŸu ve o satÄ±r hakkÄ±nda bilgi verir $ SHIFT-G # imleci dosyanÄ±n en sonuna getirir GÃ¶rÃ¼ntÃ¼ (vim iÃ§erisinde) WRAPPING AND LINE NUMBERS $ :set nowrap # kelimelerin kaymamalarÄ±nÄ± saÄŸlar $ :set number # satÄ±r numaralarÄ±nÄ± gÃ¶sterir ArÅŸivleme ve SÄ±kÄ±ÅŸtÄ±rma $ tar -cvf dosya_ismi.tar klasÃ¶r/ # verilen klasÃ¶r iÃ§in arÅŸiv oluÅŸturur $ tar -czvf dosya_ismi.tgz klasÃ¶r/ # verilen klasÃ¶r iÃ§in arÅŸivlenmiÅŸ ve sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ dosya oluÅŸturur.  ArÅŸivleri gÃ¶rÃ¼ntÃ¼leme $ tar -tvf dosya_ismi.tar $ tar -tzvf dosya_ismi.tgz Ã‡Ä±kartma $ tar -xvf dosya_ismi.tar $ tar -xzvf dosya_ismi.tgz $ gunzip dosya_ismi.tar.gz $ tar zxf blast.linux.tar.Zs Basit yÃ¼kleme iÅŸlemleri RPM yÃ¼klemeleri $ rpm -i uygulama_ismi.rpm $ rpm --query \u0026lt;paket_ismi\u0026gt; ## RPM versiyonunu kontrol etme iÃ§in Debian paketlerinin yÃ¼klenmesi $ apt-cache search nmap # nmap adÄ±ndaki uygulamayÄ± debian deposundan arama yapar $ apt-cache show nmap # nmap hakkÄ±nda tanÄ±mlamayÄ±(bilgi) gÃ¶sterir $ apt-get install nmap # nmap \u0026#39;i sisteme kurar. $ apt-get update # sistemdeki uygulamalarÄ± ve servisleri gÃ¼nceller $ apt-get upgrade -u # uygulamalarÄ±n yeni versiyonu var ise yÃ¼kseltme yapar $ dpkg -i dosya.deb # indirilen debian dosyasÄ±nÄ±n yÃ¼klenmesini saÄŸlar $ aptitude # apt-get ile aynÄ± iÅŸlemi gÃ¶rÃ¼r $ aptitude search vim # vim programÄ±nÄ± debian deposunda arar  Cihazlar Takma /Ã‡Ä±karma usb/floppy/cdrom $ mount /media/usb $ umount /media/usb $ mount /media/cdrom $ eject /media/cdrom $ mount /media/floppy Ã‡evresel DeÄŸiÅŸkenler $ xhost user@host # kullanÄ±cÄ± iÃ§in Ã§alÄ±ÅŸtÄ±rma izini ekler $ echo DISPLAY # ekranÄ±n ayarlarÄ±nÄ± gÃ¶sterir $ export (setenv) DISPLAY=\u0026lt;lokal_IP\u0026gt;:0 # gÃ¶rÃ¼ntÃ¼ deÄŸiÅŸkeninin deÄŸerini deÄŸiÅŸtirir $ unsetenv DISPLAY # gÃ¶rÃ¼ntÃ¼ deÄŸiÅŸkenini siler $ printenv # kullanÄ±lan Ã§evresel deÄŸiÅŸkenleri listeler $ $PATH # terminal Ã¼zerinde programlarÄ±n Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlayan yollarÄ± gÃ¶sterir. ","permalink":"/posts/linux-temeller-1/","summary":"Ã–zet: Bu yazÄ±da linux ortamÄ±na biraz daha giriÅŸ yaparak, linux ortamÄ±nda bulunan komutlar hakkÄ±nda kÄ±sa bilgilendirme yapÄ±lmasÄ± planlanmaktadÄ±r.\nGiriÅŸ Neden Linux ?  Birden fazla iÅŸlemi aynÄ± anda kolay ÅŸekilde yapmanÄ±zÄ± saÄŸlar Uzaktan iÅŸlemlerinizi halletmede bÃ¼yÃ¼k kolaylÄ±k saÄŸlar Birden fazla kullanÄ±cÄ± aynÄ± sunucuya eriÅŸebilir Terminale, bir sistem Ã¼zerinde olan kaynaklara birden fazla eriÅŸim mÃ¼mkÃ¼ndÃ¼r ArayÃ¼z olan sistemlere gÃ¶re daha performanslÄ±, Bedava , GÃ¼ncel  Temeller Bu bilgilendirme dosyasÄ± iÃ§in not   BÃ¼tÃ¼n komutlar bÃ¼yÃ¼k ve kÃ¼Ã§Ã¼k harfe duyarlÄ±dÄ±r.","title":"terminal/komut "},{"content":"Ã–zet: Bu kÄ±sa yazÄ±mÄ±zda linux bilgisayarlarÄ±nÄ±n terminali Ã¼zerinden yapabileceÄŸiniz basit iÅŸlemlere dair bilgiler verilecektir.\nLinux tabanlÄ± sunucularda/bilgisayarda terminal Ã¼zerinden kopyalama Kopyalama iÅŸlemi \u0026ldquo;cp\u0026rdquo; komutu ile yapÄ±lmaktadÄ±r, bu komuta ait format aÅŸaÄŸÄ±daki gibi Ã¶zetlenebilir.\ncp [parametreler] [kopyalanacak-dosya] [kopyalanmasi-hedeflenen-yer] Bu komutun kullanÄ±mÄ±na Ã¶rnek verelim, kopyalanacak dizin ve kopyalanmasÄ± gereken dosya ;\nKopyalanacak dizin : /home/geek/Masaustu/\nKopyalanacak dosya : /home/geek/Dokumanlar/resim.png\nBu durumda komut : (* Dizinlere ve dosyalara eriÅŸim hakkÄ±na sahip olduÄŸunuzdan emin olunuz)\ncp /home/geek/Dokumanlar/resim.png /home/geek/Masaustu/ EÄŸer bir dizin iÃ§erisindeki bÃ¼tÃ¼n dosyalar kopyalanmasÄ± planlanÄ±yor ise -R parametresi kullanÄ±lmasÄ± gerekmektedir. Diyelim ki bir dizin iÃ§erisinde 1.png, 2.png, 3.png \u0026hellip; 12.png gibi dosyalar var ise ve bu dizinin adÄ± \u0026ldquo;resimler\u0026rdquo; ise, resimler dizisi istenilen diziye aÅŸaÄŸÄ±daki komut yardÄ±mÄ± ile kopyalanabilir.\ncp -R /home/geek/Dokumanlar/resimler /home/geek/Masaustu/ Bu kÄ±sÄ±mda bÃ¼tÃ¼n veri hedeflenen dizine aktarÄ±lmaktadÄ±r.\ncp komutuna ait bazÄ± parametreler ve onlarÄ±n kÄ±sa aÃ§Ä±klamalarÄ±:\n-R : verilen dizindeki bÃ¼tÃ¼n dosyalarÄ± hedeflenen dizine kopyalamak iÃ§in gereklidir.\n-p : kopyalama yaparken dosyaya ait olan, oluÅŸturulma, deÄŸiÅŸtirme, sahiplik bilgilerini deÄŸiÅŸtirmeden onlar ile birlikte kopyalamak iÃ§in gereklidir.\nTerminal Ã¼zerinden yeniden baÅŸlatma Linux terminali Ã¼zerinden bir sunucuyu yeniden baÅŸlatmak veya kapatmak iÃ§in gerekli olan komutlar ÅŸu ÅŸekilde Ã¶zetlenebilir. Bu komutlarÄ±n Ã§alÄ±ÅŸmasÄ± iÃ§in sunucu Ã¼zerinde yÃ¶netici (root) yetkilerine sahip olmalÄ±sÄ±nÄ±z.\nYeniden baÅŸlatmak iÃ§in\nreboot Bu kÄ±sÄ±mda \u0026quot;Permission denied\u0026quot; veya buna benzer bir izin reddedildi mesajÄ± aldÄ±ÄŸÄ±nÄ±zda aynÄ± komutu sudo eki koyarak denemelisiniz, yÃ¶netici yetkisi olmadÄ±ÄŸÄ± durumda bu tÃ¼r mesajlarÄ± alÄ±rsÄ±nÄ±z.\nsudo reboot Bu komut bazen her linux daÄŸÄ±lÄ±mÄ± iÃ§in geÃ§erli olmayabilir bu durumda aÅŸaÄŸÄ±da verilen komut reboot komutuna alternatif olarak verilebilir. (* Bu komut MacOS bilgisayarlar iÃ§inde kullanÄ±labilir, MacOS iÅŸletim sistemleri Hybrid bir yapÄ±ya sahip olduÄŸundan dolayÄ± Unix Ã§ekirdeÄŸi iÃ§ermektedir.)\nshutdown -r now \u0026quot;shutdown\u0026quot; komutunda isterseniz bekleme sÃ¼resi ekleyebilirsiniz bÃ¶ylece sunucu veya bilgisayar, belirlenen bekleme sÃ¼resi sonunda verdiÄŸiniz komutu uygulayacaktÄ±r.\nshutdown -r +30 30 dk sonrasÄ±nda sunucu yeniden baÅŸlatÄ±lacaktÄ±r, benzer ÅŸekilde dk belirlemektense, istediÄŸiniz saatte bu iÅŸlemi yapmak isterseniz, istediÄŸiniz saati aÅŸaÄŸÄ±daki formatta ayarlanabilir.\nshutdown -r 19:30 Sunucu, saat 19:30 da yeniden baÅŸlatÄ±lmaya ayarlanmÄ±ÅŸtÄ±r.\n\u0026quot;shutdown\u0026quot; komutuna ait bazÄ± parametreler ve aÃ§Ä±klamalarÄ± ÅŸu ÅŸekilde Ã¶zetlenebilir.\n-r : sunucuya yeniden baÅŸlatma sinyalini vermesi iÃ§in gereklidir.\n-k : sunucuya baÄŸlÄ± olan fakat yÃ¶netici yetkisinde eriÅŸmemiÅŸ kiÅŸileri sunucudan atar.\n","permalink":"/posts/linux-terminalinden-basit-komutlar/","summary":"Ã–zet: Bu kÄ±sa yazÄ±mÄ±zda linux bilgisayarlarÄ±nÄ±n terminali Ã¼zerinden yapabileceÄŸiniz basit iÅŸlemlere dair bilgiler verilecektir.\nLinux tabanlÄ± sunucularda/bilgisayarda terminal Ã¼zerinden kopyalama Kopyalama iÅŸlemi \u0026ldquo;cp\u0026rdquo; komutu ile yapÄ±lmaktadÄ±r, bu komuta ait format aÅŸaÄŸÄ±daki gibi Ã¶zetlenebilir.\ncp [parametreler] [kopyalanacak-dosya] [kopyalanmasi-hedeflenen-yer] Bu komutun kullanÄ±mÄ±na Ã¶rnek verelim, kopyalanacak dizin ve kopyalanmasÄ± gereken dosya ;\nKopyalanacak dizin : /home/geek/Masaustu/\nKopyalanacak dosya : /home/geek/Dokumanlar/resim.png\nBu durumda komut : (* Dizinlere ve dosyalara eriÅŸim hakkÄ±na sahip olduÄŸunuzdan emin olunuz)","title":"cp/reboot komularÄ± "}]